[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Empirical Research with AI",
    "section": "",
    "text": "0.1 关于这本书\n这本书是在准备 连享会系列公开课 中的「AI时代如何做实证研究？」专题时编写的讲义。\n书中介绍了 AI 工具如何提升实证研究的效率，如何帮助我们更好地思考和解决问题。书中还介绍了 ChatGPT, 豆包, Github Copilot 等工具的使用，以及 Markdown、Github 和 Jupyter Notebook 的协同工作方式。\n当然，此处介绍的内容只是 GenAI 的一些非常基础的应用，有关 GenAI 的更深入、更系统的介绍，将在 2025 年 9 月 13, 14, 20 日开设的 GenAI 专题课程中讲授，详情参见：",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>前言</span>"
    ]
  },
  {
    "objectID": "index.html#关于这本书",
    "href": "index.html#关于这本书",
    "title": "Empirical Research with AI",
    "section": "",
    "text": "生成式人工智能专题：社会科学研究的革新工具\n\n嘉宾：杨阳 (香港浸会大学)\n时间：2025 年 9 月 13, 14, 20 日 (共三天)\n方式：线上直播 + 30 天回放",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>前言</span>"
    ]
  },
  {
    "objectID": "index.html#如何使用本书",
    "href": "index.html#如何使用本书",
    "title": "Empirical Research with AI",
    "section": "0.2 如何使用本书？",
    "text": "0.2 如何使用本书？\n这本书是使用 VScode 编辑器 + Quarto 插件编写，进而通过 Github pages 发布的。详情参见 Quarto books。 \n\n你可以 Fork 本书的 github 仓库，然后你的电脑上演练书中的例子。\n你需要做一些基本的配置，参见 安装和环境配置\n书稿中还有不少错漏之处，还请不吝指出：arlionn@163.com。",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>前言</span>"
    ]
  },
  {
    "objectID": "index.html#如何找到我",
    "href": "index.html#如何找到我",
    "title": "Empirical Research with AI",
    "section": "0.3 如何找到我？",
    "text": "0.3 如何找到我？\n我最近也在小红书注册了账号 连玉君 (ID: 95085566173)，大家可以在那里留言和讨论。\n\n\n主页 | 课程 | 视频 | 推文 | 资料",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>前言</span>"
    ]
  },
  {
    "objectID": "body/00_intro.html",
    "href": "body/00_intro.html",
    "title": "2  简介：AI 带来了什么？",
    "section": "",
    "text": "2.1 理念",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>简介：AI 带来了什么？</span>"
    ]
  },
  {
    "objectID": "body/00_intro.html#理念",
    "href": "body/00_intro.html#理念",
    "title": "2  简介：AI 带来了什么？",
    "section": "",
    "text": "自然语言编程 vs. 传统编程\n\n\n「自然语言编程」与 Python、C++ 等传统编程本质上都是向计算机发出指令，要求其执行特定操作\n区别在于：\n\n传统编程语言（如 Python、C++）有严格的语法和结构\n自然语言编程则用人类语言（如中文、英文）描述操作\n\n\n\n思维方式与沟通能力\n\n\n初学时，自然语言编程似乎更简单\n真正发挥其潜力，关键在于思维方式和沟通方式（如何提问）\n学习曲线很陡峭：\n\n知识广度：你要知道很多东西以及他们的关联，才能提出好的问题\n知识深度：基本概念、核心理论、核心算法\n逻辑思维：界定问题、拆解问题、追问（横向 v.s. 纵向）\n语言表达：简洁、准确、清晰\n\n\n\n最核心的理念转变\n\n\n提示词 = 自然语言的”代码”\n写好提示词，就像写好 Python/C++ 代码一样重要\n许多高校已开设「提示词工程」课程，「Prompt 工程师」将成为热门职业\n\n\n推荐学习资料\n\n\nPrompt Engineering Guide\n吴恩达老师的 ChatGPT Prompt Engineering for Developers",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>简介：AI 带来了什么？</span>"
    ]
  },
  {
    "objectID": "body/00_intro.html#提示词",
    "href": "body/00_intro.html#提示词",
    "title": "2  简介：AI 带来了什么？",
    "section": "2.2 提示词",
    "text": "2.2 提示词\n\nTips\n\n\n先粗后细 e.g. 生成讲义 v.s 先细后粗\n顺藤摸瓜-迁移 e.g. 各种抽样方法\n虚构角色 e.g. 你是一个资深的英文经济学期刊的编辑 → 推文\n\n\n收集整理自己的提示词\n\n\nChatGPT Prompting Cheat Sheet\nThe Complete ChatGPT Cheat Sheet 2025!\nPrompt工作手册 - 方法篇",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>简介：AI 带来了什么？</span>"
    ]
  },
  {
    "objectID": "body/00_intro.html#我的使用经验",
    "href": "body/00_intro.html#我的使用经验",
    "title": "2  简介：AI 带来了什么？",
    "section": "2.3 我的使用经验",
    "text": "2.3 我的使用经验\n\n我常用的提示词：https://github.com/arlionn/UseChatGPT\n论文推介：\n\n使用 AI 撰写论文推介\n案例：AI 助我读论文之 Wing et al. (2024)\n\n借助 AI 找 IV\n\n帮我找 20 个 IV → 详情\n寻找 IV 的提示词如何写？\n\n学计量：使用 AI 解读 Tobit 模型",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>简介：AI 带来了什么？</span>"
    ]
  },
  {
    "objectID": "body/01_install_Python_Anaconda.html",
    "href": "body/01_install_Python_Anaconda.html",
    "title": "3  Python：安装和环境配置",
    "section": "",
    "text": "3.1 安装 Anaconda\n对于初学者来讲，建议安装 Anaconda 套装。它是一个开源的 Python 发行版，集成了 Python 解释器、包管理器 Conda 和许多常用的科学计算和数据分析库（如 NumPy、Pandas、Matplotlib 等）。\n虽然 Anaconda 自带的编辑器 Jupyter Notebook 很好用，但如果你平时经常用 VScode 写东西，建议安装 VScode 作为编辑器。VScode 支持多种编程语言，可以安装各种插件来增强功能。对于编写 Python 代码而言，仅需安装 python 和 jupyter 插件就可以满足基本需求。\n网上有不少关于 Anaconda 和 VScode 的安装和配置的教程，如：\n根据我的使用经验，基本步骤总结如下：\n下面，我详细说明每个步骤的操作。",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python：安装和环境配置</span>"
    ]
  },
  {
    "objectID": "body/01_install_Python_Anaconda.html#安装-anaconda",
    "href": "body/01_install_Python_Anaconda.html#安装-anaconda",
    "title": "3  Python：安装和环境配置",
    "section": "",
    "text": "提示\n\n\n\n\n请务必认真阅读安装步骤，不要跳过任何步骤，尤其是安装路径的选择和环境变量的配置。否则后续出错后，需要大量时间排查问题。\n安装过程中，可以借助 DeepSeek，ChatGPT, 豆包等 AI 助手来解决问题。\n\n\n\n\n下载 Anaconda（建议注册一个账号，若不注册，可以点击 Skip）。\n安装 Anaconda。安装 Anaconda 最重要的事情：\n\nSelect Installation Type 页面，建议选择 Just Me，然后点击 Next。\nChoose Install Location 页面，建议使用默认路径 C:\\Users\\用户名\\Anaconda3 作为 Anaconda 的安装路径，这样可以避免一些潜在的权限和路径问题。然而， 如果你的用户名中包含中文字符或空格，建议选择「自定义路径」，并选择一个英文路径，如 C:\\myProgram\\Anaconda3。\nAdvanced Installation Options 页面，确保同时勾选如下两个选项：\n\n[√] Add Anaconda to my PATH environment variable\n[√] Register Anaconda as my default Python 3.x\n\n\n详情参见：VSCode 与 Anaconda安装配置\n\n安装完成后，打开 Anaconda Prompt（命令行界面），输入以下命令检查安装是否成功：\nconda --version\n如果显示版本号，则表示安装成功。\n\n\n\n\n\n\n\n可选步骤\n\n\n\n\n安装完成后，打开 Anaconda Navigator (在开始菜单或应用程序中找到它)。在 Anaconda Prompt 中输入以下命令更新 Conda 到最新版本：\nconda update conda\n创建一个新的虚拟环境（可选）：如果你想在一个独立的环境中工作，比如，你要同时使用 Python 3.8 和 Python 3.12，以便完成不同的项目，你可以创建一个新的虚拟环境。输入以下命令创建一个名为 myenv38 的虚拟环境，并安装 Python 3.8：\nconda create --name myenv38 python=3.8\n同理，如果你想使用 Python 3.12，你可以创建一个名为 myenv312 的虚拟环境，并安装 Python 3.12：\nconda create --name myenv312 python=3.12\n接下来，你可以激活特定的虚拟环境，比如 myenv38，输入以下命令：\nconda activate myenv38\n此时，若执行 canda list 命令，你会看到当前环境中安装的所有包和版本信息；而执行 canda install Stargazer, v = 2.1.1，则会在当前环境中安装 Stargazer 包的 2.1.1 版本。",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python：安装和环境配置</span>"
    ]
  },
  {
    "objectID": "body/01_install_Python_Anaconda.html#安装-vscode-及插件",
    "href": "body/01_install_Python_Anaconda.html#安装-vscode-及插件",
    "title": "3  Python：安装和环境配置",
    "section": "3.2 安装 VScode 及插件",
    "text": "3.2 安装 VScode 及插件\n\n\n\n\n\n\nVScode 及插件的安装和使用\n\n\n\n\n王胜文, 2023, VScode编辑器：安装、使用与常用快捷键\n初虹, 2022, Markdown-LaTeX：经管人的VSCode配置大全.\n连玉君, 2024, VScode 插件：安装、配置和使用.\n连玉君, 2024, VScode：实用 Markdown 插件推荐.\n连玉君, 2024, 从基础到 AI 助手：Python 用户最爱的 VScode 插件清单\n\n\n\nVisual Studio Code (VS Code) 是微软推出的轻量但功能强大的源代码编辑器，基于 Electron 开发，支持 Windows、Linux 和 macOS 操作系统。VScode 免费开源且功能强大，VScode 写代码支持各种流行的编程语言，包括 C++、Java 等，使用熟练之后，基本不会再使用其他的编辑器。最重要的是，VScode 平台上有大量的插件，极大地扩展了其功能。\n\n下载 VScode (请酌情选择适合你操作系统的版本)。\n安装 VScode（双击下载的安装包，按照提示完成安装）。\n安装完成后，打开 VScode。\n在 VScode 中，安装插件以扩展 VScode 的性能，比如 Python、Jupyter 等。\n\n\n3.2.1 VScode 插件\n插件安装方法：点击图中的 四个小方块 图标，在搜索框中填入插件名称，点击 Install 即可。\n详情参见 VScode 插件：安装、配置和使用\n\n为了能在 VScode 中借助 Jupyter Notebook 中同时运行 Python 和 Stata 代码，我们需要安装 python, Jupyter, nbstata, Copilot 等插件，具体说明如下。\n\n3.2.1.1 Python 和 Jupyter 插件\n为了能够在 VScode 中使用 Jupyter Notebook 运行 Python 代码，并借助 AI 提升工作效率，你需要安装以下插件：\n\nPython (必装，作者：Microsoft)：运行 Python 代码\nJupyter (必装，作者：Microsoft)：用于编辑和运行 Jupyter Notebook 文件\nPylance (推荐，作者：Microsoft)：代码补全和智能提示等功能\nGitHub Copilot (建议安装，作者为：Github)：AI 编程助手\n\n收费：首月免费，后续每月 $10.0，支持国内发行的 Visa 或 Master 信用卡付款。\n建议同时安装 Github Copilot Chat 插件，提供更好的交互体验。\n\n\n\n\n\n\n\n\n有关编程助手的说明\n\n\n\n\n安装编程助手前，请确保能顺畅上网。\n建议预先申请一个 GitHub 账号，使用 Copilot 时，可以用 GitHub 账号进行关联和登录。\n建议选定一个编程助手，不同同时使用多个助手，以免造成冲突。如果已经安装了多个编程助手，可以在 VScode 的插件管理界面中，点击已安装的插件旁的齿轮按钮，选择 unable (暂停使用) 或 Uninstall (卸载)。\n我个人使用 Copilot，觉得效果还不错。也有人说 Codeium 也不错 (Copilot 的替代品，Free，目前已经更名为 Windsurf Plugin)，它在 VScode 中的安装量比 Copilot 还要多。Cline (作者：Cline，Free) 是最近新出的，我没有尝试，无法评判。\n\n\n\nVScode 插件市场提供了诸多 Python 相关的插件，可以酌情安装，详见：VScode：实用 Python 插件清单。\n比如，我装了如下插件 - Data Wrangler (可选)：表格呈现效果很美观\n\n\n3.2.1.2 Markdown 插件\nVScode 支持 Markdown 语法，可以用来编写文档、笔记等。安装 Markdown 插件可以增强 VScode 对 Markdown 的支持，比如预览、语法高亮、制作幻灯片等。多数情况下，安装如下三个插件就可以满足需求：\n\nMarkdown All in One\nMarkdown Preview Enhanced\nMarp (制作幻灯片)\n\n若想较为全面的了解 Markdown 类插件，可参阅 VScode：实用 Markdown 插件推荐。\n你也可以点击 VScode 左边栏中的 四个小方块 图标，在搜索框中输入 markdown，酌情安装其他插件。\n\n\n3.2.1.3 Stata 插件\n在 VScode 的扩展搜索框中搜索 stata，安装如下插件：\n\nStata Enhanced (作者：Kyle Barron)：语法高亮支持；\nstataRun (作者：Yeaoh Media Inc)：用于建立 VScode 和 Stata 的后台关联。\n\n要顺利运行 Stata 代码，还需要安装 nbstata 包，下文将详细介绍。",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python：安装和环境配置</span>"
    ]
  },
  {
    "objectID": "body/01_install_Python_Anaconda.html#配置-stata-环境nbstata",
    "href": "body/01_install_Python_Anaconda.html#配置-stata-环境nbstata",
    "title": "3  Python：安装和环境配置",
    "section": "3.3 配置 Stata 环境：nbstata",
    "text": "3.3 配置 Stata 环境：nbstata\n为了在 VS Code 中的 .ipynb 文档中直接运行 Stata 代码，我们可以借助 nbstata 扩展包 (注意：nbstata 是 Python 包，不是 VScode 插件)。下面介绍如何在已有 Python + Jupyter 环境基础上完成配置。\n\n\n\n\n\n\n前提条件：你已经完成以下配置\n\n\n\n\n已经安装 Anaconda 套装；\n已经安装 VS Code 编辑器；并在插件市场安装了 Python 和 Jupyter 插件；\n可以在 VS Code 中新建 .ipynb 文件并执行代码块中的 Python 代码。\n\n\n\n需要注意的是，nbstata 仅适用于 Stata 17+ 及以上版本 (配置了 pystata)，且 Stata 的安装路径中不包含中文字符和空格 (在 Stata 中输入 sysdir 可以查看你的 Stata 安装路径)。\n\n3.3.1 配置步骤\n\n\n\n\n\n\n注意\n\n\n\n\n下面的配置步骤需要在 VS Code 的终端中完成，而不是在 Jupyter Notebook 中。\n打开 VS Code 的终端的方式参见 5. Python 安装常见问题 → 5.5 小节。\n如果按下面的方法无法配置成功，可以参阅 nbstata User Guide 和 Jupyter + Stata 配置方法 (第 4 小节)。也可以打开 豆包，把你的错误代码截图和粘贴给它，多数情况下你都能在它的引导下完成配置。\n\n\n\n首先，打开 VS Code 的终端 (参见 5. Python 安装常见问题 → 5.5 小节)，依次执行以下命令：\npip install nbstata\n\npython -m nbstata.install --conf-file\n该指令将生成一个配置文件，便于我们后续修改配置。运行成功后，提示信息如下：\nC:\\Users\\Administrator\\.config\\nbstata\\nbstata.conf\n你可以到此文件夹下，用 VScode 打开 nbstata.conf 文件，查看或修改配置。我的配置如下：\n[nbstata]\nstata_dir = D:\\stata17\nedition = mp\nsplash = False\ngraph_format = png\ngraph_width = 7.5in\ngraph_height = 5.0in\necho = None\nmissing = .\nbrowse_auto_height = True\n完成上述配置后，nbstata 就可以在 VS Code 的 .ipynb 文件中运行 Stata 代码了 (参见下方的 测试 提示框)。如果受阻，可以尝试按如下方式修改 nbstata.conf 文件：\n\n在 Stata 命令窗口中输入 sysdir 命令，结果中第一行显示的就是 Stata 的安装路径 (因人而异)：\n. sysdir\n  STATA:  D:\\stata17\\\n复制 D:\\stata17\\，填入 nbstata.conf 文件的 stata_dir 字段中 (有时需要把反斜杠 \\ 改成斜杠 /)。\n进入 D:\\stata17\\ 目录可以查看 Stata 的版本类型，如 StataMP-64.exe、StataSE-64.exe 或 StataIC-64.exe。根据你的 Stata 版本，对照修改 nbstata.conf 文件中 edition 字段的值为 mp、se 或 ic。\n\n\n\n\n\n\n\n测试\n\n\n\n\n打开或新建一个 .ipynb 文件；\n点击 + Codes 按钮，创建一个新的代码单元格，指定 Kernel 为 Stata (nbstata)，输入 display 2 + 2。\n按快捷键 Shift + Enter 或点击单元格左上角的 Run 按钮，若输出 4，说明配置成功。\n\n\n\n\n\n3.3.2 补充说明\n\nNote：安装过程中遇到问题，可以将错误代码，以及你当前的 Python 和 Stata 安装信息发送给豆包或 ChatGPT，它会引导你排除问题。我的 ChatGPT 对话 - nbstata 安装讲义 供各位参考。\n\n如下步骤都是可选的，主要是为了方便后续的配置和使用。\n\n3.3.2.1 更新 nbstata\n如需更新 nbstata，只需在 VS Code 终端中执行以下命令：\npip install --upgrade nbstata\n\n\n3.3.2.2 python -m nbstata.install 的完整语法\n安装时，如果需要更改安装环境或安装路径，可以按如下语法规则编写命令 (方括号表示选项)：\npython -m nbstata.install [--sys-prefix] [--prefix PREFIX] [--conf-file]\n其中，\n\n--sys-prefix 指定安装的环境，例如 virtualenv (虚拟环境) 或 conda env (Conda 环境)。\n--prefix PREFIX 指定安装路径。\n--conf-file 将创建一个配置文件。\n\n详情参见 nbstata User Guide。\n\n\n\n3.3.3 nbstata 扩展信息\n有关 nbstata 的更多信息和使用方法，参见：\n\nnbstata 文档，以及 nbstata User Guide\nJupyter + Stata 配置方法，第 4 小节。\n黄晨晨, 2023, Jupyter Notebook 与 Stata 交互：nbstata\n韩少真, 展金永, 2020, 珠联璧合 I：Jupyter Notebook 和 Stata 关联 (windows系统)。这个推文介绍了如何在原生 Jupyter Notebook 中安装 nbstata 包。\n\n\n\n\n\n\n\n后续\n\n\n\n\n安装过程中可能会遇到各种小挫折，参见 Python 安装常见问题。\n完成上述环境配置后，就可以使用 Jupyter Notebook 编写代码了，参见 Jupyter Notebook 的使用。",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python：安装和环境配置</span>"
    ]
  },
  {
    "objectID": "body/01_use_Junpyter_Notebook.html",
    "href": "body/01_use_Junpyter_Notebook.html",
    "title": "4  Jupyter Notebook 的使用",
    "section": "",
    "text": "4.1 简介\n顾名思义，Jupyter Notebook 是一个文本编辑器。它类似于 Stata 的 Do 文件 (.do)；或 R 语言中 RStudio 的 RMarkdown 文件 (.rmd)。Jupyter Notebook 的特征更接近于 RMardown，它提供了一个交互式的计算环境，允许你在一个文档中编写和运行代码、可视化数据、撰写文本和公式等。\n简言之，你可以把你的分析目的 (文本块)，分析过程 (代码块)，分析结果 (表格或图形) 都整合到一个文档中，形成一个完整的分析报告。如果安装了编程助手 (如 Copilot)，还可以在编写代码时获得智能提示、自动补全和根据提示词自动编写代码和修改文字的功能。",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Jupyter Notebook 的使用</span>"
    ]
  },
  {
    "objectID": "body/01_use_Junpyter_Notebook.html#简介",
    "href": "body/01_use_Junpyter_Notebook.html#简介",
    "title": "4  Jupyter Notebook 的使用",
    "section": "",
    "text": "说明\n\n\n\n\n这里介绍的 Jupyter Notebook 是指在 VScode 中使用的 Jupyter Notebook，而不是 Anaconda 自带的 Jupyter Notebook。VScode 中的 Jupyter Notebook 更加灵活和强大，支持多种编程语言和插件。\n阅读本章之前，请务必按照 Python：安装和环境配置 完成环境配置。\n我在 数据分析与 Python 应用 这本书中介绍了很多基于「AI 提示词 + Python」进行数据分析了的实例。",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Jupyter Notebook 的使用</span>"
    ]
  },
  {
    "objectID": "body/01_use_Junpyter_Notebook.html#运行-python-代码",
    "href": "body/01_use_Junpyter_Notebook.html#运行-python-代码",
    "title": "4  Jupyter Notebook 的使用",
    "section": "4.2 运行 Python 代码",
    "text": "4.2 运行 Python 代码\n我们先演示一下如何在 VScode 中编写 Python 代码：\n\n点击 File → New File …，选择 Jupyter Notebook .ipynb，填入文件名，即可新建一个 Jupyter Notebook。\n点击 + Code 按钮，添加一个代码单元格，输入 Python 代码，然后点击运行按钮 (或按 Shift + Enter) 执行代码。\n\n\n\ngithub - 原始 .ipynb 文件，version 2\n\n由于 Jupyter Notebook 对 Python 的支持一直非常好，因此在 Jupyter Notebook 中编写和运行 Python 代码非常顺畅。比如，你可以运行或修改上图中的代码：\n\n# 绘制一幅简单的函数图\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 10, 200)\ny = x ** 2\n\nplt.figure(figsize=(3, 2.5))\nplt.plot(x, y)\nplt.title(\"y = x^2\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.grid(True)\nplt.show()",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Jupyter Notebook 的使用</span>"
    ]
  },
  {
    "objectID": "body/01_use_Junpyter_Notebook.html#运行-stata-代码",
    "href": "body/01_use_Junpyter_Notebook.html#运行-stata-代码",
    "title": "4  Jupyter Notebook 的使用",
    "section": "4.3 运行 Stata 代码",
    "text": "4.3 运行 Stata 代码\n多数情况下，你能在 dofile 或 Stata 命令窗口中运行的代码，都可以在 Jupyter Notebook 中运行。以下是一个简单的 Stata 代码示例，比如：\n\ndis \"nbstata is so NB\"\n\nnbstata is so NB\n\n\n再看一个更完整的例子：使用 Stata 自带的 sysuse auto 数据集，绘制汽车价格与重量的散点图，并把图形保存到当前工作路径下。\n\ncd \"D:/temp\"  // 设置工作目录，酌情修改或忽略此行\n\n// 安装绘图模板\nnet install scheme_scientific.pkg, from(http://fmwww.bc.edu/RePEc/bocode/s)\nset scheme scientific\n\nsysuse \"auto.dta\", clear\n\nlist price weight mpg in 1/3, clean \n\nscatter price weight\ngraph export \"scatter_price_weight.png\", replace width(700)\n\nD:\\temp\nchecking scheme_scientific consistency and verifying not already installed...\nall files already exist and are up to date.\n(1978 automobile data)\n\n       price   weight   mpg  \n  1.   4,099    2,930    22  \n  2.   4,749    3,350    17  \n  3.   3,799    2,640    22  \nfile scatter_price_weight.png written in PNG format\n\n\n\n\n\n\n\n\n\n\n4.3.1 魔法命令\n有少数 Stata 命令是无法在 Jupyter Notebook 中直接使用的，比如 browse, doedit, set graph_width 等。这些命令通常都涉及到文件的读写和图形的显示，而不是简单地在结果窗中打印文字和数字结果。\n不过，我们可以借助 nbstata 的魔法命令来实现多数功能。\nnbstata 目前支持以下魔法指令，详情参见 nbstata 文档，以及 nbstata User Guide。\n\n\n\n命令\n描述\n\n\n\n\n%browse\n查看数据集\n\n\n%head\n查看前 5 (或 N) 行\n\n\n%tail\n查看最后 5 (或 N) 行\n\n\n%frbrowse\n查看数据框\n\n\n%frhead\n查看前 5 (或 N) 个数据框\n\n\n%frtail\n查看最后 5 (或 N) 个数据框\n\n\n%locals\n列出暂元和它们的值\n\n\n%delimit\n打印当前分隔符\n\n\n%help\n显示 Stata 帮助\n\n\n%set\n设置单个配置选项\n\n\n%%set\n设置多个配置选项\n\n\n%status\n显示 Stata 或配置状态\n\n\n%%echo\n显示命令回显\n\n\n%%noecho\n不显示命令回显\n\n\n%%quietly\n静默所有单元输出，包括图表\n\n\n\n我设置的图片默认尺寸比较大，如需调整，可以使用 nbstata 的 魔法命令 设置图片尺寸。\n需要注意的是，魔法命令需要以 % 或 %% 开够，且只有放在代码块的第一行才会被识别为魔法命令。因此，我通常会让魔法命令独占一行：\n\n%set graph_width = 5.0in   // 将图形宽度设置为 5.0 英寸\n\ngraph size was (7.0in, 5.0in), is now (5.0in, 5.0in).\n\n\n\n%set graph_height = 4.0in  // 将图形高度设置为 4.0 英寸\n\ngraph size was (5.0in, 5.0in), is now (5.0in, 4.0in).\n\n\n执行完上述两行命令后，再绘制的图形就会按照指定的宽度和高度显示了：\n\nscatter price weight\n\n\n\n\n\n\n\n\n再看几个有趣的例子：\n\n%status 可以显示当前 Stata 的版本和配置状态。\n%head 可以查看数据集的前 5 行，或者指定行数。我猜是借用了 Python 中的 head() 函数。\n\n\n%status\n\n    System information\n      Python version         3.12.7\n      Stata version          Stata 17.0 (MP)\n      Stata library path     D:\\stata17\\mp-64.dll\n      Stata initialized      True\n      sfi initialized        True\n\n    Settings\n      graphic display        True\n      graphic size           width = 5.0in, height = 4.0in\n      graphic format         png\n\n      echo                   None\n      missing                .\n      browse_auto_height     True\n      config file path       C:\\Users\\Administrator\\.config\\nbstata\\nbstata.conf\n\n\n\n%head price weight mpg make foreign\n\n\n\n\n\n\n\n\nprice\nweight\nmpg\nmake\nforeign\n\n\n\n\n1\n4,099\n2,930\n22\nAMC Concord\nDomestic\n\n\n2\n4,749\n3,350\n17\nAMC Pacer\nDomestic\n\n\n3\n3,799\n2,640\n22\nAMC Spirit\nDomestic\n\n\n4\n4,816\n3,250\n20\nBuick Century\nDomestic\n\n\n5\n7,827\n4,080\n15\nBuick Electra\nDomestic",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Jupyter Notebook 的使用</span>"
    ]
  },
  {
    "objectID": "body/01_use_Junpyter_Notebook.html#项目结构文档",
    "href": "body/01_use_Junpyter_Notebook.html#项目结构文档",
    "title": "4  Jupyter Notebook 的使用",
    "section": "4.4 项目结构文档",
    "text": "4.4 项目结构文档\n下面，我会从项目的角度，介绍如何建立一个规范的项目文件结构，以便于管理和组织你的代码、数据和文档。养成良好的项目管理习惯对于后续处理大型项目很有帮助。\n首先，我们会新建一个文件夹，然后在该文件夹下按需创建几个子文件夹，用于分类存储 Data, Codes 和输出结果。最后，我们会在根目录下或 Codes 文件夹下创建 Jupyter Notebook 文件，编写代码并运行。\n\n4.4.1 新建项目文件夹\n\n本地新建一个文件夹，如 D:/FinData/Proj_Maotai\n(推荐) 在该文件夹下新建如下几个子文件夹，用于分类存放文件\n\n[./data]：存放 .csv、.xlsx 等数据文件\n[./doc]：存放说明文档、报告等文件\n[./codes]：存放代码文件，包括自编函数、模块等\n[./output]：存放输出文件，包括图表、结果等\n此外，建议在该文件夹下新建一个 README.md 文件，对项目进行说明 (如项目背景、目标、数据来源、程序的执行顺序和方法等)。\n\n\n\n\n\n4.4.2 新建 Jupyter Notebook 文件 (.ipynb)\n\n打开 VScode，依次点击 文件 -&gt; 打开文件夹，选择 D:/FinData/Proj_Maotai 文件夹。\n右击该文件夹，选择 New File (亦可直接点击下图中的 New File 图标)，新建一个文件，名称为 data_clean.ipynb。注意：一定要以 .ipynb 结尾，否则无法用 Jupyter Notebook 打开，也无法添加 Python 代码块。\n\n你也可以使用命令行方式新建一个 .ipynb 文件：按快捷键 Ctrl+Shift+P &gt;&gt; 输入或选择 Create: New Jupyter Notebook 会打开一个名为 Untitled-1.ipynb 的文件，输入内容后另存到目标文件夹即可 (可以重命名，如 data_clean.ipynb)。\n\n\n完成上述设定后，VScode 的文件目录结构如下：\n# 目录结构\nD:/FinData/Proj_Maotai\n├── codes\n├── data\n├── doc\n├── output\n├── README.md\n└── data_clean.ipynb\n\n\n\n\n\n\n自动创建项目文档结构\n\n\n\n如果你有多个项目需要建立，或者需要在多个项目中使用相同的文件结构，可以借助 AI 工具自动创建项目文档。本章附录中提供了一个示例提示词，供你参考。",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Jupyter Notebook 的使用</span>"
    ]
  },
  {
    "objectID": "body/01_use_Junpyter_Notebook.html#使用-jupyter-notebook",
    "href": "body/01_use_Junpyter_Notebook.html#使用-jupyter-notebook",
    "title": "4  Jupyter Notebook 的使用",
    "section": "4.5 使用 Jupyter Notebook",
    "text": "4.5 使用 Jupyter Notebook\nStep 1：选择解释器。初次打开 data_clean.ipynb 文件时，可能需要按下图方式设定 Python 解释器，以便随后用 python.exe 来运行新增的代码块。\n\n如果你没有看到该按钮，可以按快捷键 Ctrl + Shift + P，输入或选择 Python: Select Interpreter，然后选择你安装的 Python 解释器。\n若要运行 Stata 代码，可以选择 Stata (nbstata) 解释器。\n\n\nStep 2：添加 Markdown 文本块。点击上图中的 + Markdown 按钮，添加一个 Markdown 文本块。可以用 Markdown 语法撰写文档说明。\n\n预览：点击图中的 √ 或按快捷键 Esc；\n编辑：在预览 双击鼠标 区可重新计入编辑模式。\n\n\nStep 3a： 让 AI 自动生成代码 (可选)：如果你已经在 VScode 中安装了插件 Github Copilot，可以点击图中的 Generate 按钮，输入提示词 (比如，生成随机数 x~N(0,1)，N=100，绘制直方图+密度函数图, 英文标题)，敲回车，或点击下图中的右箭头，Copilot 会自动生成代码。注意：代码块右下角有时会显示为 Ini，表示没有指定该代码快的类型，可以点击选择合适的类型或解释器。\n\n\nStep 3b： 自行添加代码块。点击上图中的 + Code 按钮，添加一个代码块。可以在代码块中输入 Python 代码。\n\n运行代码块：点击上图中的 ▶ 按钮，或按快捷键 Ctrl + Enter；\n\n\nStep 4： 修改和调试代码。你可以酌情修改代码。\n\n若出现大量警告信息或程序无法运行，可以选中全部代码或被标注了红色波浪线代码，点击下图中的黄色星星按钮，选择 Fix using copilot，多数情况下都能自动修复。\n若需现有代码进行优化，可以选中代码后，点击黄色星星，选择 Modify using copilot，然后输入提示词。\n\n\n\n4.5.1 界面和主要功能\n在上文中，为了突出重点，我没有让 VScode 编辑器全屏。事实上，全屏后，你会发现 Jupyter Notebook 提供了一组功能强大的工具栏和菜单栏，帮助你更好地编写和运行代码。下面是 Jupyter Notebook 的主要界面元素：\n\n多数菜单的功能都是比较直观的，下面我简单介绍几个常用的功能：\n\nView data：查看数据和概要信息。注意：需要安装 Data Wrangler 插件才具有此功能。\n...：最右侧的三个点，点击后可以选择 Export，将当前 Notebook 导出为 PDF、HTML、Markdown 等格式；还有预览 (Preview) 等功能 (快捷键 Esc)。",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Jupyter Notebook 的使用</span>"
    ]
  },
  {
    "objectID": "body/01_use_Junpyter_Notebook.html#在文档间复制-cell",
    "href": "body/01_use_Junpyter_Notebook.html#在文档间复制-cell",
    "title": "4  Jupyter Notebook 的使用",
    "section": "4.6 在文档间复制 cell",
    "text": "4.6 在文档间复制 cell\n\n4.6.1 单个 cell 的复制和粘贴\n在 Jupyter Notebook 中，你可以通过以下方式在不同的 Notebook 文档之间复制 cell：\n\n打开源 Notebook 文档，选中要复制的 cell。\n使用快捷键 Ctrl+C 复制选中的 cell；亦可单击三个点图标，选择 Copy Cell。\n打开目标 Notebook 文档，将光标定位到要粘贴的位置。\n使用快捷键 Ctrl+V 粘贴 cell；亦可单击三个点图标，选择 Paste Cell。\n\n\n\n4.6.2 多个 cell 的复制和粘贴\n\n在 VS Code 的资源管理器（Explorer）中，分别打开需要操作的两个 Notebook 文件（如 A.ipynb 和 B.ipynb）。\n\n如需分屏显示，可拖动标签，将两个文档左右排列，方便拖拽和对比操作。\n\n在源 Notebook（如 A.ipynb）中，按住 Ctrl（Mac 下为 Command）逐个点击左侧 cell 区域，可多选不连续的 cell；或先点击一个 cell，再按住 Shift 点击另一个 cell，选择一段连续的 cell。\n\n多选后，cell 区域会高亮显示，表示已被选中。\n\n按 Ctrl+C（Command+C）或右键点击选中区域，选择“Copy Cell”。\n切换到目标 Notebook（如 B.ipynb），点击你希望插入的位置。\n按 Ctrl+V（Command+V）或右键选择 “Paste Cell”，即可将所有选中 cell 批量粘贴到目标位置。",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Jupyter Notebook 的使用</span>"
    ]
  },
  {
    "objectID": "body/01_use_Junpyter_Notebook.html#导出和文档转换",
    "href": "body/01_use_Junpyter_Notebook.html#导出和文档转换",
    "title": "4  Jupyter Notebook 的使用",
    "section": "4.7 导出和文档转换",
    "text": "4.7 导出和文档转换\n在 VScode 中，你可以将 Jupyter Notebook 导出为多种格式，包括 HTML、PDF 和 Markdown。你可以在菜单栏中选择 文件 -&gt; 导出为，然后选择你想要的格式。当然，还需要安装一些额外的工具来支持这些导出功能，主要包括 Pandoc 和 TeX Live。\n\n4.7.1 转换为 HTML\n\n4.7.1.1 方法 1：借助 quarto 插件\n如果已经安装了 Quarto，并在 VScode 中安装了 Quarto 插件，你可以直接在 VScode 中使用 Quarto 导出 HTML。\n首先，在 .ipynb 文件的顶部新增一个 Markdown 代码块，并填入以下 YAML 头信息：\n---\ntitle: \"这是我的笔记\"\nformat: html\ntoc: true\nnumber-sections: true\n---\n然后，按快捷键 Ctrl + ~ 打开终端，运行以下命令：\ncd path_name_of_your_notebook\nquarto render your_notebook.ipynb --to html\n输出的 HTML 文件将保存在当前目录下，支持目录和章节编号和 :::.callout 等样式。\n\n\n4.7.1.2 方法 2：使用 Jupyter Notebook 的导出功能\n\n单击 .ipynb 文档顶部 ... 按钮 (在 Outline 右侧)，选择 Export -&gt; HTML，即可将当前 Notebook 导出为 HTML 格式。\n要求：需要预先通过安装 nbconvert 来支持 HTML 导出。\n\n安装 nbconvert 的命令为：pip install nbconvert。\n\n\n这种方式的转换效果不及 quarto，但对于简单的 Notebook 文档来说，已经足够使用。\n\n\n4.7.1.3 方法 3：使用 nbconvert 命令行工具\n如果不想本地转换，你也可以把 .ipynb 文件同步到你的 GitHub 仓库，然后使用 Github 的在线转换功能。你只需复制 .ipynb 文件的链接地址，粘贴到浏览器中，GitHub 会自动渲染并显示该 Notebook。你可以在页面上右键选择“另存为”来保存为 HTML 格式。\n若想获得更好的格式和样式，可以使用 nbviewer 在线查看和转换为 HTML 格式。具体方法是： nbviewer 在线查看和转换为 HTML 格式。\n\n\n\n4.7.2 安装 Pandoc（用于文档格式转换）\nJupyter 和 Quarto 在导出 Markdown、Word、PDF 时都会调用 Pandoc。\n\n推荐安装方式（适用于 Anaconda 用户）：\nconda install -c conda-forge pandoc\n或者前往官网下载安装包（Windows/macOS/Linux 均支持）： https://pandoc.org/install\n安装完成后，你可以在终端中输入以下命令测试是否成功：\npandoc --version\n\n\n\n4.7.3 安装 TeX Live（用于导出 PDF）\nJupyter 和 Quarto 的 PDF 导出依赖 LaTeX 引擎，如 pdflatex 或 xelatex。推荐安装完整版本的 TeX Live。\n\nWindows 用户请访问官网下载器： https://tug.org/texlive/windows.html\n安装过程中可选择 “完整版” 或 “Typical full installation”；\n安装完成后，务必重启 VS Code 和终端；\n然后在终端中测试：\npdflatex --version\n\n\n\n4.7.4 如何打开终端？\n很多初学者不清楚“在终端中输入命令”具体指什么。你可以按以下方式打开终端：\n\n方式一（推荐）： 在 VS Code 中按下快捷键 Ctrl + ~（就是数字 1 左侧的波浪线），即可打开内置终端。\n方式二： 从系统菜单中打开：\n\nWindows：点击开始 → 输入 cmd 或 Anaconda Prompt 或 PowerShell；\nmacOS：打开 Launchpad → 搜索「终端（Terminal）」；\nLinux：按下 Ctrl + Alt + T 打开终端窗口。\n\n\n终端窗口中，你可以输入如 conda install、pandoc --version 等命令，按下回车运行。\n\n\n4.7.5 安装完成后即可使用以下方式导出 PDF\n\n在 VS Code 中点击右上角「导出（Export）」按钮；\n或在终端中运行：\njupyter nbconvert your_notebook.ipynb --to pdf\n或使用 Quarto（如果你已安装）：\nquarto render your_notebook.ipynb --to pdf\n\n完成上述安装和设定后，即可顺利将 Notebook 文档导出为 PDF、HTML、Word 等多种格式，满足教学、写作和发布的常见需求。\n\n\n4.7.6 高阶用法：使用 Quarto 导出漂亮 PDF 或制作电子书网站\n如果你希望导出的文档更美观、格式更统一，或者希望将多个 Jupyter Notebook 或 Markdown 文档整合成一个电子书（book）或教学网站（website），可以考虑使用 Quarto。详情参见 连玉君：Quarto Book，以及采用该教程编写的 讲义。\nQuarto 是 RStudio 团队开发的下一代科学与技术出版平台，同时支持 .ipynb、.qmd、.md 文件的混合编写和发布。常见功能包括：\n\n一键导出 排版美观的 PDF 文档（支持标题、目录、脚注、交叉引用、引用文献等）；\n创建结构化的电子书（Quarto Book），适合撰写讲义、教程和学术教材；\n部署交互式教学网站（Quarto Website），可发布为静态网页托管在 GitHub Pages 上。\n\n\n4.7.6.1 示例命令：\n将 .ipynb 或 .qmd 导出为 PDF：\nquarto render your_notebook.ipynb --to pdf\n将多个文档组织为电子书结构（在 _quarto.yml 中设置章节）：\nquarto render\n\n\n4.7.6.2 安装方式：\n\n可从官网下载安装：https://quarto.org/download\n安装完成后，在终端中测试：\nquarto --version\n\nQuarto 完全兼容你现有的 VS Code 工作流，安装 Quarto 插件后即可直接在 VS Code 中预览、渲染和发布。对于希望构建专业文档的用户，Quarto 是目前最值得推荐的方案。",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Jupyter Notebook 的使用</span>"
    ]
  },
  {
    "objectID": "body/01_use_Junpyter_Notebook.html#附录",
    "href": "body/01_use_Junpyter_Notebook.html#附录",
    "title": "4  Jupyter Notebook 的使用",
    "section": "4.8 附录",
    "text": "4.8 附录\n\n4.8.1 借助 AI 工具自动创建项目文档 (备选)\n\nChatGPT 对话过程\n\n如果觉得手动创建文件夹麻烦，可以借助 ChatGPT 等 AI 工具，自动创建项目文档。下面是我编写的一个示例提示词 (你只需要指定目标文件夹路径即可，即使这个文件夹不存在也没有关系)：\n\n\n\n\n\n\n提示词：自动项目文档目录\n\n\n\n请帮我在 Windows 系统上为一个数据分析项目生成标准的项目结构。 项目的根目录是 D:/FinData/Proj_Maotai，请在该路径下创建以下结构：\nD:/FinData/Proj_Maotai\n├── codes             # 用于存放脚本代码\n├── data              # 原始数据或处理后的数据\n├── doc               # 项目文档与说明资料\n├── output            # 图表、模型结果、分析报告等输出文件\n├── README.md         # 项目说明文件，简要介绍项目目的与结构\n├── 01_data_clean.ipynb       # 数据清洗与预处理\n├── 02_EDA.ipynb              # 探索性数据分析\n└── 03_regression.ipynb       # 回归分析与建模\n请提供 Python 代码，以便我在写 Jupyter Notebook 中运行。 请确保必要的 .ipynb 文件和 README.md 都被自动生成。如果文件已存在，请保留原文件不覆盖。 你可以默认这些 .ipynb 文件为空白，也可以加一两行注释。\n\n\n经过测试，ChatGPT 和 豆包生成的代码都可以一次性顺利执行：\n\nChatGPT 对话过程\n豆包对话过程\n\n\n\n4.8.2 采用插件创建项目模板\n除了上述方法，你也可以在 VScode 中搜索 Project Templates，安装对应的插件来创建项目模板。安装完成后，点击左侧的 Project Templates 图标，选择 Create New Project，然后选择你需要的模板即可。\n\n该插件提供了多种项目模板，包括数据分析、机器学习、Web 开发等，可以酌情选择。\n该插件也支持自定义模板，具体操作请参考插件的说明文档。\n\n不过，如果项目不是很复杂，建议还是手动创建项目文件夹，这样更灵活，也更容易管理。",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Jupyter Notebook 的使用</span>"
    ]
  },
  {
    "objectID": "body/02_install_FAQs.html",
    "href": "body/02_install_FAQs.html",
    "title": "5  Python 安装常见问题",
    "section": "",
    "text": "5.1 安装了 Anaconda 还需要安装 Python 3.13 吗？\n答：不需要！ 而且，强烈反对。Anaconda 是一个完整的 Python 科学计算发行版，安装时会自动集成 Python 解释器和常用扩展包 (如 NumPy、Pandas 等)，并通过 Conda 环境管理器提供灵活的版本控制。\n若额外安装官网 Python，可能引发以下问题：\n建议通过 Conda 统一管理 Python 环境，既能自由切换不同版本（如 conda create -n py312 python=3.12），又能确保依赖隔离和生态兼容性，避免因混合安装导致的技术债。",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python 安装常见问题</span>"
    ]
  },
  {
    "objectID": "body/02_install_FAQs.html#安装了-anaconda-还需要安装-python-3.13-吗",
    "href": "body/02_install_FAQs.html#安装了-anaconda-还需要安装-python-3.13-吗",
    "title": "5  Python 安装常见问题",
    "section": "",
    "text": "版本混乱：系统环境变量可能优先指向非 Anaconda 的 Python，导致命令行调用错误的解释器版本；\n\n依赖冲突：Conda 和 pip 混合管理包时，依赖项版本容易冲突（例如 TensorFlow 或 PyTorch 的兼容性问题）；\n\n环境隔离失效：全局安装的 Python 可能干扰 Conda 创建的独立环境，破坏项目隔离性；\n\n维护困难：需手动协调多个 Python 实例的更新与兼容性，增加运维复杂度。",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python 安装常见问题</span>"
    ]
  },
  {
    "objectID": "body/02_install_FAQs.html#安装-anaconda-时需要注意什么",
    "href": "body/02_install_FAQs.html#安装-anaconda-时需要注意什么",
    "title": "5  Python 安装常见问题",
    "section": "5.2 安装 Anaconda 时需要注意什么？",
    "text": "5.2 安装 Anaconda 时需要注意什么？\n\nChoose Install Location 页面，建议使用默认路径 C:\\Users\\用户名\\Anaconda3 作为 Anaconda 的安装路径，这样可以避免一些潜在的权限和路径问题。注意： 如果你的用户名中包含中文字符或空格，建议选择「自定义路径」，并选择一个英文路径，例如 C:\\PromgramFile\\Anaconda3。\nAdvanced Installation Options 页面，确保同时勾选如下两个选项：\n\n[√] Add Anaconda to my PATH environment variable\n[√] Register Anaconda as my default Python 3.x\n\n\n详情参见：VSCode与Anaconda安装配置",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python 安装常见问题</span>"
    ]
  },
  {
    "objectID": "body/02_install_FAQs.html#默认解释器的路径",
    "href": "body/02_install_FAQs.html#默认解释器的路径",
    "title": "5  Python 安装常见问题",
    "section": "5.3 默认解释器的路径",
    "text": "5.3 默认解释器的路径\n问题描述：每次在 VScode 的 Jupyter 中运行代码时，都会弹出一个窗口询问选择解释器的路径。\n\n原因 1：VScode 设置问题：VScode 在运行 Jupyter Notebook 时需要指定 Python 解释器的路径，如果没有设置默认解释器，就会弹出这个窗口。 environment variable`，导致 VScode 无法找到 Python 解释器的路径。\n\n解决方法：在 VScode 中，按下 Ctrl + Shift + P，输入Python: Select Interpreter\\，选择你想要的 Python 解释器路径即可。\n\n原因 2：Anaconda 安装时的配置有问题。\n在安装 Anaconda 时，在 Advanced Installation Options 页面，你可能没有勾选 Add Anaconda to my PATH environment variable，导致 VScode 无法找到 Python 解释器的路径。\n\n解决方案：将 Anaconda 的安装路径添加到系统的环境变量中。具体步骤如下：\n\n在 Windows 中，右键点击 “此电脑”，选择 “属性”。\n依次点击 高级系统设置 &gt;&gt; 环境变量\n在 “系统变量” 中找到 Path，点击 “编辑”。\n点击 “新建”，添加 Anaconda 的安装路径，例如 C:\\Users\\用户名\\Anaconda3 和 C:\\Users\\用户名\\Anaconda3\\Scripts。\n点击“确定”，保存设置。",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python 安装常见问题</span>"
    ]
  },
  {
    "objectID": "body/02_install_FAQs.html#pip-install-在哪里执行",
    "href": "body/02_install_FAQs.html#pip-install-在哪里执行",
    "title": "5  Python 安装常见问题",
    "section": "5.4 pip install 在哪里执行？",
    "text": "5.4 pip install 在哪里执行？\n答：pip install pkg_name 通常在命令行界面中执行。最方便的方法就是在 VScode 的集成终端/控制台 (terminal) 中执行。详见下一个问题。\n\nVScode 控制台快捷打开方式：\n\nWindows：按下 Ctrl + ~ 键 (~ 在 Esc 键下方)。\nMac：按下 Command + Shift + ~ 键。\n\n\n除此之外，你也可以在计算机操作系统的命令行界面中执行：\n\nWindows 系统：按下 Win + R 键，输入 cmd，然后按下 Enter 键。\nMac 系统：按下 Command + Space 键，输入 Terminal，然后按下 Enter 键。\nVSCode 中：按下 Ctrl + Shift + P，输入 Terminal: Create New Terminal，然后按下 Enter 键。",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python 安装常见问题</span>"
    ]
  },
  {
    "objectID": "body/02_install_FAQs.html#如何打开-vscode-的终端控制台-terminal",
    "href": "body/02_install_FAQs.html#如何打开-vscode-的终端控制台-terminal",
    "title": "5  Python 安装常见问题",
    "section": "5.5 如何打开 VScode 的终端/控制台 (terminal)？",
    "text": "5.5 如何打开 VScode 的终端/控制台 (terminal)？\n答：VScode 的 terminal 是一个集成的命令行界面，允许用户在 VScode 编辑器中直接执行命令行操作。它支持多种 Shell，如 PowerShell、Bash 和 Command Prompt 等。用户可以在 terminal 中运行 Python 脚本、安装包、管理版本控制等操作，而无需切换到外部命令行窗口。\n\n开启方法：\n\n方法一： 使用快捷键。\n\nWindows 系统：在 VScode 中按下 Ctrl + ~ 键（~ 在 Esc 键下方）。\nMac 系统：按下 Command + Shift + ~ 键。\n\n方法二： 使用 VScode 命令面板。\n\n在 VScode 中，按下 Ctrl + Shift + P，输入 Terminal: Create New Terminal，然后按下 Enter 键。\n\n方法三： 使用 Windows 命令行。\n\n在 Windows 系统中，按下 Win + R 键，输入 cmd，然后按下 Enter 键，打开命令提示符窗口。\n举例：输入 pip install pkg_name 来安装 Python 包。\n\n\n用途：\n\n执行命令行操作，如安装 Python 包（pip install pkg_name）、运行脚本等。\n运行 Git 命令进行版本控制：\n\ngit clone：克隆远程仓库到本地。\ngit add：将更改添加到暂存区。\ngit commit：提交更改到本地仓库。\ngit push：将本地提交推送到远程仓库。\n\n管理虚拟环境和依赖项。\n调试和测试代码。\n查看系统信息和环境变量等。",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python 安装常见问题</span>"
    ]
  },
  {
    "objectID": "body/02_install_FAQs.html#conda-install-与-pip-install-有何差别",
    "href": "body/02_install_FAQs.html#conda-install-与-pip-install-有何差别",
    "title": "5  Python 安装常见问题",
    "section": "5.6 Conda install 与 pip install 有何差别？",
    "text": "5.6 Conda install 与 pip install 有何差别？\n相似点：都是 Python 的包管理工具；都能安装 Python 库和包，并自动处理依赖关系。\n主要区别：\n\n包来源不同：\n\nConda：主要从 Anaconda 官方仓库（例如 Anaconda Cloud）安装包，并且可以安装 Python 之外的其他工具和库（例如 R 语言的库）。它支持更广泛的语言和工具。\npip：从 Python 官方包索引（PyPI）安装 Python 包，通常只用于 Python 相关的包。\n\n虚拟环境管理：\n\nConda：不仅可以管理 Python 包，还可以管理虚拟环境。在 Conda 环境中，你可以控制整个环境的配置，包括 Python 版本和依赖的库。\npip：仅管理 Python 包，不负责环境的管理。虽然 virtualenv 或 venv 可以配合 pip 使用来管理虚拟环境，但这需要额外的配置。\n\n依赖解决：\n\nConda：在安装包时，Conda 会自动解决依赖冲突，确保所有包和环境的兼容性。\npip：通常会安装指定的包，但不总是自动解决复杂的依赖关系。如果发生依赖冲突，pip 可能不会提醒用户。\n\n适用场景：\n\nConda：适用于需要管理多个工具和语言环境（如 Python、R）的情况，尤其在数据科学、机器学习等领域中，Conda 更加常用。\npip：适用于单纯的 Python 项目，尤其是当你不需要其他语言支持时，pip 更简便。\n\n\n\n总结\n\nAnaconda 是一个包含 Conda 的大套件，提供了用于数据分析和科学计算的工具。\nConda 是 Anaconda 的包和环境管理工具，可以管理 Python 以及其他语言的包。\npip 是 Python 的官方包管理工具，通常用于管理 Python 包，适用于纯 Python 项目。",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python 安装常见问题</span>"
    ]
  },
  {
    "objectID": "body/02_install_FAQs.html#conda-和-anaconda-有何关系",
    "href": "body/02_install_FAQs.html#conda-和-anaconda-有何关系",
    "title": "5  Python 安装常见问题",
    "section": "5.7 Conda 和 Anaconda 有何关系？",
    "text": "5.7 Conda 和 Anaconda 有何关系？\nAnaconda 是一个开源的 Python 发行版，旨在简化 Python 和 R 编程语言中数据科学、机器学习和科学计算的使用。Anaconda 提供了包括 Python 和 R 语言在内的各种开源库以及管理和部署环境所需的工具。\nConda 是 Anaconda 的一个组件，它是一个包管理器和环境管理器。Conda 负责安装、更新、卸载包和管理虚拟环境。实际上，Conda 不仅仅可以用来管理 Python 包，还可以管理其他语言（例如 R 和 Ruby）的包。\n总结：Anaconda 是一个包含 Conda 的完整发行版，Conda 是其中的包管理工具。\n\nConda 的用途\n\n包管理：Conda 用于安装、升级和卸载 Python 包以及其他语言的包。它能够管理并解决依赖问题，确保你安装的包兼容且稳定。\n环境管理：Conda 允许创建和管理虚拟环境。通过虚拟环境，你可以在同一台机器上管理多个不同版本的 Python 和包，而不会发生冲突。例如，你可以为一个项目使用 Python 3.7，为另一个项目使用 Python 3.9，并且确保每个项目有独立的依赖。",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python 安装常见问题</span>"
    ]
  },
  {
    "objectID": "body/03_markdown_marp.html",
    "href": "body/03_markdown_marp.html",
    "title": "6  Markdown",
    "section": "",
    "text": "6.1 何谓 Markdown？\nMarkdown 是一种轻量级的标记语言，允许你使用易读易写的纯文本格式编写文档，然后转换成结构化的 HTML, PDF, Word 等多种格式的文档。Markdown 语法简单易学，适合用来撰写笔记、文档、幻灯片等。\n大家在网上看到的很多博客文章，程序说明文档，甚至是在线书籍 (Python for Data Analysis, 3E)，都是用 Markdown 写的。",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "body/03_markdown_marp.html#语法速览",
    "href": "body/03_markdown_marp.html#语法速览",
    "title": "6  Markdown",
    "section": "6.2 语法速览",
    "text": "6.2 语法速览\n\n\n\n\n\n\nFigure 6.1: Markdown 语法对照\n\n\n\n你可以在如下网站按部就班地学习 Markdown 的基本用法，大概五分钟后你就可以掌握常用 Markdown 语法规则了：\n\nhttps://www.markdowntutorial.com/zh-cn",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "body/03_markdown_marp.html#markdown-基本语法",
    "href": "body/03_markdown_marp.html#markdown-基本语法",
    "title": "6  Markdown",
    "section": "6.3 Markdown 基本语法",
    "text": "6.3 Markdown 基本语法\n\nSource: markdownguide.org/cheat-sheet/\n\n这份 Markdown 备忘单介绍了常用的 Markdown 语法。为了便于您快速了解基本的语法规则，这里略去了很多细节信息，详情参见：基础语法 和 扩展语法。\n以下是 John Gruber 原始设计文档中列出的基本元素，所有 Markdown 应用程序都支持这些元素。\n\n\n\n\n\n\n\n元素\nMarkdown 语法\n\n\n\n\n标题\n# 一级标题## 二级标题### 三级标题\n\n\n粗体\n**粗体文本**\n\n\n斜体\n*斜体文本*\n\n\n引用块\n&gt; 引用内容\n\n\n有序列表\n1. 第一项2. 第二项3. 第三项\n\n\n无序列表\n- 第一项  -  第一条- 第二项- 第三项\n\n\n代码高亮显示\n`代码` (`xtreg` → xtreg)\n\n\n水平线\n---\n\n\n链接\n[连享会主页](https://www.lianxh.cn)\n\n\n图片\n![图片标题](/Fig/image.jpg) 或 ![](图片网址)\n\n\n\n\n6.3.1 表格\n| 命令    | 范例                 |\n| :------ | :------------------- |\n| xtreg   | `xtreg y x, fe`      |\n| reghdfe | `reghdfe y x, a(id)` |\n\n\n\n命令\n范例\n\n\n\n\nxtreg\nxtreg y x, fe\n\n\nreghdfe\nreghdfe y x, a(id)\n\n\n\n\n\n6.3.2 数学公式\n\n单行数学公式用 $$ 符号包围起来；\n行内数学公式用 $ 符号包围起来；\n包围符号内侧不要有空格，否则在有些 Markdown 编辑器中无法正常显示公式\n\n正确：$y = f(x)$\n错误：$ y = f(x) $ 或 $y = f(x) $\n\n有关 LaTeX 数学公式的语法和工具，参见：\n\nMarkdown常用LaTex数学公式\n神器-数学公式识别工具-mathpix\n\n\n模型设定为：\n\n$$y_{it} = \\alpha_i + x_{it}\\beta + u_{it}$$\n\n其中，$y_{it}$ 为被解释变量，$\\alpha_i$ 为个体效应。\n模型设定为：\n\\[y_{it} = \\alpha_i + x_{it}\\beta + u_{it}\\]\n其中，\\(y_{it}\\) 为被解释变量，\\(\\alpha_i\\) 为个体效应。\n\n\n6.3.3 代码块\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4, 5, 6]\n})\n```\n\n```stata\nsysuse \"auto.dta\", clear\nregress mpg weight\ndisplay \"Results: \" 2 + 3\n```\n渲染效果：\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4, 5, 6]\n})\nsysuse \"auto.dta\", clear\nregress mpg weight\ndisplay \"Results: \" 2 + 3\n\n\n6.3.4 扩展阅读\n\n初虹, 2024, 让「记录」变得简单：Markdown使用详解, 连享会 No.1456.\n初虹, 2021, 学术论文写作新武器：Markdown-上篇, 连享会 No.603.\n初虹, 2021, 学术论文写作新武器：Markdown-下篇, 连享会 No.604.\n初虹, 2021, 学术论文写作新武器：Markdown-中篇, 连享会 No.605.\n连玉君, 2024, VScode插件：安装、配置和使用, 连享会 No.1490.\n连玉君, 2024, VScode：实用 Markdown 插件推荐, 连享会 No.1390.",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "body/03_markdown_marp.html#marp-幻灯片",
    "href": "body/03_markdown_marp.html#marp-幻灯片",
    "title": "6  Markdown",
    "section": "6.4 Marp 幻灯片",
    "text": "6.4 Marp 幻灯片\n在 VScode 中安装 Marp 插件后，你可以使用 Markdown 语法来创建幻灯片。使用 Marp 最大的好处是你可以专注于内容，而不必担心幻灯片的格式和样式。Marp 会自动将你的 Markdown 文档转换为美观的幻灯片。\n\n\n6.4.1 安装\n1. VScode 编辑器\n2. Marp 插件 - 在 VScode 中打开扩展市场，搜索 Marp for VS Code，点击安装。\n3. 如需使用 PDF 导出功能，还需要安装 Marp CLI： - 打开终端 (TERMINAL)：在 VScode 中，点击左侧活动栏的终端图标，或使用快捷键 Ctrl + ~（反引号）打开终端。 - 在终端中运行以下命令安装 Marp CLI： bash      npm install -g @marp-team/marp-cli\n扩展阅读：\n\n连玉君, 2024, VScode插件：安装、配置和使用, 连享会 No.1490.\n连玉君, 2024, VScode：实用 Markdown 插件推荐, 连享会 No.1390.\n\n\n\n6.4.2 模版 1：最基本的设定\n以下是一个简单的 Marp 幻灯片模板，你只需要新建一个 .md 文件，输入如下内容，然后在 VScode 中打开该文件即可：\n---\nmarp: true\n---\n\n# 幻灯片标题\n\n---\n\n## 第一页幻灯片\n\n- xxx\n- xxx\n\n---\n\n## 第二页幻灯片\n\n- xxx\n- xxx\n\n\n\n6.4.3 模版 2：更多的设定\n\n\n\n\n\n\n该模板的主要功能包括：\n\n\n\n\n幻灯片的标题、作者、页码、脚注\n自定义幻灯片的正文字号\n标题的颜色和页面背景颜色\n\n\n\n---\nmarp: true\nsize: 16:9        # 宽版：4:3\npaginate: true  \nfooter: '[lianxh.cn](https://www.lianxh.cn)&ensp;|&ensp;[Books](https://www.lianxh.cn/Books.html)'\n---\n\n&lt;style&gt;\n/*一级标题局中*/\nsection.lead h1 {\n  text-align: center; /*其他参数：left, right*/\n}\nsection {\n  font-size: 22px;      /* 正文字号 */\n}\nh1 {\n  color: blackyellow;   /* 标题的颜色 */\n  /*font-size: 28px; */ /* 标题的字号, 其它标题也可以这样修改 */\n}\nh2 {\n  color: green;\n}\nh3 {\n  color: darkblue;\n}\nh4 {\n  color: brown;\n}\n/* 右下角添加页码 */\nsection::after {\n  content: attr(data-marpit-pagination) '/' attr(data-marpit-pagination-total); \n}\nheader,\nfooter {\n  position: absolute;\n  left: 50px;\n  right: 50px;\n  height: 25px;\n}\n/* 调整图片与文本之间的间距 */\nsection img {\n  margin-right: 10px;   /* 设置图片右侧的间距 */\n  margin-left: 10px;   /* 设置图片左侧的间距 */\n}\n\n/* 设置正文区域的边距，确保文本能更紧凑地放置 */\nsection {\n  #padding-right: 20px;  /* 设置右侧边距 */\n  #padding-left: 20px;  /* 设置左侧边距 */\n}\n\n/* ====== 新增：设置代码块字号 ====== */\n\n/* 默认代码块字号 */\npre {\n  font-size: 22px;\n}\n\n/* 可选类：小字号代码块 */\n.small-code pre {\n  font-size: 12px;\n}\n\n/* 可选类：大字号代码块 */\n.large-code pre {\n  font-size: 20px;\n}\n&lt;/style&gt;\n\n&lt;!--顶部文字--&gt;\n[lianxh.cn](https://www.lianxh.cn/news/46917f1076104.html) \n\n&lt;br&gt;\n\n&lt;!--封面图片--&gt;\n![bg right:50% w:400 brightness:. sepia:50%](https://fig-lianxh.oss-cn-shenzhen.aliyuncs.com/20220722114227.png) \n\n&lt;!--幻灯片标题--&gt;\n\n### 连享会 · 2025 暑期班 · 高级班\n\n# B1. 动态面板门槛模型\n\n&lt;br&gt;\n&lt;br&gt;\n\n&lt;!--作者信息--&gt;\n[连玉君](https://www.lianxh.cn) (中山大学)\narlionn@163.com\n\n&lt;br&gt;\n\n---\n&lt;!-- backgroundColor: #FFFFF9 --&gt;\n## 第一页幻灯片\n\n- 背景是淡黄色的，可以根据需要修改颜色\n\n---\n&lt;!-- backgroundColor:white --&gt;\n## 第二页幻灯片\n\n- 背景是纯白色\n- 下面的图片在右侧，占页面 60% 的宽度\n\n![bg right:60% w:800](图片网址)\n详情参见：\n\n宋森安, 2021, 用Markdown制作幻灯片-五分钟学会Marp（上篇）, 连享会 No.656.\n宋森安, 2021, 用Markdown制作幻灯片-五分钟学会Marp（下篇）, 连享会 No.657.\n连玉君, 2022, Marp幻灯片模板：用Markdown快速写幻灯片, 连享会 No.1059.",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "body/03_markdown_marp.html#用-markdown-编写个人网站和简历",
    "href": "body/03_markdown_marp.html#用-markdown-编写个人网站和简历",
    "title": "6  Markdown",
    "section": "6.5 用 Markdown 编写个人网站和简历",
    "text": "6.5 用 Markdown 编写个人网站和简历\n使用 Markdown 语法，你可以轻松地创建个人网站和简历。\n你可以参考如下推文，轻松创建个人主页 (无需购买域名，也无需学习 HTML 和 CSS)：\n\n连小白, 2025, 50 分钟搞定个人主页：Fork 模板 + GitHub Pages + Quarto 完整教程\n\nMarkdown 也可以用来制作简历，参见：\n\nlapis-cv.bingyan.net，github\nresume.todev.cc/，github",
    "crumbs": [
      "**环境配置**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "body/05_00_read_paper_with_AI.html",
    "href": "body/05_00_read_paper_with_AI.html",
    "title": "7  使用 AI 撰写论文推介",
    "section": "",
    "text": "7.1 钓鱼流程：如何撰写提示词？\n对于初学者而言，精读 Top Journal 的论文，并参考作者提供的数据和代码进行复现是一个非常有效的学习方法。在此过程中，我们可以使用 VScode + Jupyter Notebook 来记录复现过程。虽然 AI 可以大幅提高复现和写作效率，但需要调教有方 (给出清晰的提示词)。\n我通常是先和 AI 泛聊，了解论文的核心内容和贡献，然后再形成提纲，最后以 Section 为单位进行输出。每个环节都可能进行多轮讨论，这是一个我和 AI 彼此启发，深入思考的过程。",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>使用 AI 撰写论文推介</span>"
    ]
  },
  {
    "objectID": "body/05_00_read_paper_with_AI.html#钓鱼流程如何撰写提示词",
    "href": "body/05_00_read_paper_with_AI.html#钓鱼流程如何撰写提示词",
    "title": "7  使用 AI 撰写论文推介",
    "section": "",
    "text": "7.1.1 泛聊\n先使用比较开放的提示词，看看 AI 能否抓住这篇论文的要点，比如，你可以从如下问题清单中挑选一个或多个提问：\n\n\n\n\n\n\n提示词：泛聊\n\n\n\n\n请简要介绍这篇论文的核心观点和主要结论。\n这篇论文的主要贡献是什么？\n简要说明文中理论模型的设定思路和假设条件。\n\n文中介绍了哪些主要方法？请简要说明。\n该文的提出的方法适用于哪些类型的研究问题？\n论文中有哪些重要的数学公式？请解释它们的含义。\n作者提供了哪些实操建议？请简要说明。\n\n\n\n通过上述对话，你可以大致了解这篇论文的核心内容和贡献。此时，可以快速阅读一下原文，确认一下 AI 的回答是否准确。如果 AI 的回答存在严重偏差，可以考虑变换提问方式，或者更换 AI 工具。\n\n\n7.1.2 形成提纲\n待上述讨论完成后，先不要急着输出推文内容，而是让 AI 工具先给出一个提纲。然后根据提纲讨论完善后，再进行详细撰写。\n在撰写提纲的过程中，你需要首先明确一些基本要求，使 AI 能够准确把握你的侧重点，并据此生成符合你预期的内容结构。你可以从以下几个方面进行说明：\n\n推文目的：例如，是为了帮助读者快速理解论文的核心观点，还是为了深入探讨某一研究问题的理论与实证方法；\n目标读者：例如，经济学或计量经济学领域的博士生、青年教师，或是具有基础的高年级本科生；\n设定作者角色：你可以让 AI 扮演某个角色来撰写这篇推介，比如 “经济学博士生”、“计量经济学讲师” 等，这有助于它酌情调用语料，确保输出结果的语言风格符合角色身份。\n论文的重点和难点：例如，是否需要重点介绍理论模型的设定思路、实证方法的创新之处，或是数据处理的细节等；或者强调这篇推文注重实操性，理论分析和模型设定部分都是为了 Section 5. Stata 实操 的内容服务。\n语言风格：例如，采用中文推文风格（可参考 https://www.lianxh.cn 的风格），注重表达简洁、结构清晰；或采用中文讲义风格，强调语言朴实、术语准确，适合教学或研究资料使用。\n\n我经常使用的提示词如下：\n\n\n\n\n\n\n提示词：\n\n\n\n我想写一篇论文推介，介绍我传给你的 PDF 论文。具体要求如下：\n\n目标读者：经济学、计量经济学领域的研究人员和学生。\n\n希望能够让他们快速了解这篇论文的核心内容和贡献。\n\n对于一些复杂的数学公式，可以用比较通俗的语言进行解释。\n必要时，可以提供 1-2 个简化版的例子加以说明。\n\n需要介绍论文的主要内容、研究方法、数据来源和主要结论。\n如果需要插入原文中的图形或表格，请告知我图表编号和插入位置，我会手动添加。\n需要包含关键公式和代码的解读。\n需要提供参考文献列表。\n\n你先帮我列一个提纲，我们讨论完善后再进行详细撰写。\n\n\n通常，第一轮生成的提纲都比较粗糙，你可以根据实际需要进行调整和细化。你可以让 AI 提供多个版本的提纲，也可以针对某个 Section 提出细化要求。以下是一些常用的提示词：\n\n\n\n\n\n\n提示词：优化提纲\n\n\n\n\n再来一个版本\n再来一个着重介绍 {论文思路/实证方法/因果识别策略} 的版本\n{Section 2：理论分析} 部分的提纲太单薄，建议分成 3 个小节，先概述理论分析思路，再介绍模型设定，最后讨论假设条件。\n{Section 3} 需要增加对数据来源和关键变量界定的介绍\n{Section 3：研究设计} 部分需要细化对因果识别策略的讨论\n增加一个小节：参考文献。采用 APA 格式，列出文中提到的所有参考文献。\n请为每个 Section 添加 label，如 {label-1}，方便我后续追问和整理\n我希望推文能够同时兼顾初学者和有经验的研究人员，具有一定的层次性。请据此调整写作提纲；并设定每个部分的字数范围。\n\n\n\n提示词中的 {} 部分可以替换为你需要强调的内容。如果没有具体的编号，你也可以从 AI 的答案中截取一些文字写成 'text' 或 {text} 的格式，它会据此定位到对应的 Section。\n\n\n7.1.3 输出文档\n一旦推文的提纲确定下来，就可以开始输出文档了。以下是一些小技巧：\n\n建议分多次输出。论文推介和解读的篇幅通常在 2000 字以上，每次输出一个 Section，酌情修改和提出新的要求。符合要求后，再让 AI 按照这个风格继续输出后续内容。这是因为多数 AI 工具都有单次输出的字数限制，通常在 2000-3000 字左右。如果让它一次性输出全文，它会精简或删除部分内容，以确保输出不超过字数限制。 分多次输出还有一个好处：你可以多花点时间修改 Section 1 的内容，确保它符合你的要求 (这本质上是训练 AI 的过程)。然后再让 AI 按照这个风格继续输出后续内容 (它在这方面的学习能力很强)。这样可以避免一次性输出的内容不符合预期，导致需要大幅修改。\n输出的内容尽量采用 Markdown 格式，方便后续编辑和排版。你可以要求 AI 使用 Markdown 语法输出标题、列表、代码块等格式。\n正式输出前，可以使用提示词 Prompt: 请为每个输出添加 label，方便我后续整理。这样 AI 工具会在每个 Section 前添加一个 label，比如 {#Sec-1}，{#Sec-2} 等，方便后续修改时引用。\n可以使用提示词 Prompt: 请酌情标注需要插入原文图表的地方，并给出图形建议。随后，你可以使用 SniPaste 等软件从原文中截图后插入推文。\n可以要求 AI 使用 $$ 符号包裹数学公式，方便后续在 Markdown 中渲染；并使用提示词 请勿添加任何表情符号，避免 AI 在输出中添加表情符号。\n\n\n\n\n\n\n\n提示词：撰写正文\n\n\n\n按照最后一个版本的提纲，撰写正文，每个 section 输出一次。\n字数严格按照计划执行，不要缩减。\n格式要求：\n\n格式：Markdown 格式\n按照 ## 1. sec_title; ### 1.1 sub_sec_title 的方式编号 ；每个 section 添加一个标签 {lab-sec#}，便于后续修改时定位\n语言风格：中文讲义，20 年从教的老教授的风格，语言平和，准确，有力。可以参照 https://www.lianxh.cn 推文的风格。\n不要使用任何表情符号\n重要文献和链接务必在文中 提供超链接，文末提供完整的引文信息\n引文信息尽量使用如下格式，如果 PDF 链接无法获取，可以留空或删除 [-PDF-]() 部分的信息 引文信息采用 APA 格式，附加 [Link](), [PDF](), [Google](), [github]() 信息。即： Authors. ({Year}). {title_of_paper}, 45(1), 485–505. [Link](https://doi.org/{DOI}), [PDF](http://sci-hub.ren/{DOI}), [Google](&lt;{title_of_paper}&gt;), [github-replication](https://github.com/{ID}/{rep_name})\n\n文中的例子最好具体化，并保持全文一致，逐步推进和扩展。",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>使用 AI 撰写论文推介</span>"
    ]
  },
  {
    "objectID": "body/05_00_read_paper_with_AI.html#一些实例",
    "href": "body/05_00_read_paper_with_AI.html#一些实例",
    "title": "7  使用 AI 撰写论文推介",
    "section": "7.2 一些实例",
    "text": "7.2 一些实例\n最近半年，我使用 AI 工具撰写了 50 多篇论文推介和博客推文。下面举一些例子。\n\n7.2.1 实例：Wing et al. (2024) 论文推介\nWing, C., Yozwiak, M., Hollingsworth, A., Freedman, S., & Simon, K. (2024). Designing Difference-in-Difference Studies with Staggered Treatment Adoption: Key Concepts and Practical Guidelines. Annual Review of Public Health, 45(1), 485–505. Link, PDF, Google, github-replication\n这篇论文介绍了五种常用的渐进 DID 的估计方法，作者还在 GitHub 上提供了相关的代码和数据。我采用上文介绍的提示词方法与豆包聊了 20 分钟，最终形成了推文的初稿 (满意度：80%)。\n\nChatGPT 对话过程\n豆包对话过程)\n\n有关这篇推介的具体生成过程，参见 案例：AI 助我读论文之 Wing et al. (2024)。\n\n\n7.2.2 AI 伴读理论文献\n\n连玉君, 2025, 如何借助 AI 工具来伴读一篇理论类的论文？.\n\nChatGPT提示词 1：伴读一篇理论类的论文\nChatGPT提示词 2：RD调整成本函数\n\n\n\n\n7.2.3 Causal-Copilot：自动化因果推断\n我看到了一个很有意思的 AI 助手：Causal-Copilot。它可以帮助用户自动化因果推断的过程，提供了很多实用的功能和工具。\n\n连小白, 2025, 自动化因果推断助手：Causal-Copilot 简介. (ChatGPT 对话)\n\n\n\n\n\n\n\n提示词\n\n\n\n\nPrompt 1:\n\n我想根据如下资料 ({text-01}) 写一篇推文。我的读者主要是经管类的博士生和青年老师，他们的主要工作做实证研究。\n\n你觉得这篇推文的主要卖点应该是什么？\n推文的思路和结构应该是什么样的？\n\n\n’’’&lt;-text-01-&gt;\n#–Causal-Copilot–\n介绍原理，然后使用 https://causalcopilot.com/ 来做一些测试。\n\ngithub: https://github.com/Lancelot39/Causal-Copilot\ndemo-vedio: Causal-Copilot Demo: Tabular Data\nSample Dataset\n\nWang, X., Zhou, K., Wu, W., Singh, H. S., Nan, F., Jin, S., Philip, A., Patnaik, S., Zhu, H., Singh, S., Prashant, P., Shen, Q., & Huang, B. (2025). Causal-Copilot: An Autonomous Causal Analysis Agent (Version 2). arXiv. Link (rep), PDF, Google. TeX Source\n’’’\n\nPrompt 2:\n\n帮我规划一个写作提纲。\n\n语言朴实，注重为论据提供细节\n不要任何表情符号。全文按照 ## 1. title ### 1.1 sub_title 的结构来编号\n注重实操性。我的读者希望看完推文后能上手操作，实现自己的需求。\n\n\nPrompt 3:\n\n好。 1. 我希望你提供的推文可以按 section 输出，确保每个 section 都有充实的内容。 2. 每个 section 可以先用一段话概括这个 section 针对的问题是什么？要点是什么。然后再开始展开说明。 3. 段落性文字和 items 格式的文本占比 为 2:1 以上，也就说，不要过度使用 items 方式来写推文，总感觉干巴巴的。\n\nPrompt 4:\n\n好\n\nPrompt 5:\n\n将整篇推文汇总成 Markdown 教学文稿。不要删减，只需要把现有答案合并起来，稍作调整即可。\n\n\n\n\n7.2.4 博文解读：概率匹配与 Kelly 策略-Python 代码\n\nChatGPT 对话过程\n\n我看到了一篇很有意思的 Blog：Probability matching and Kelly betting。作者介绍了概率匹配和 Kelly 策略的故事，提醒我们现实决策和自然演化中的“理性”，远不是单一的最大化期望值。更广义的理性关注风险、分布、长期增长以及群体的生存。\n里面有很多我不太理解的新概念，作者并未给出详细的解释。同时，我还想了解如何用 Python 代码实现这些概念。于是我将这篇 Blog 的内容和我的问题提供给了 ChatGPT，并与它进行了讨论。最终，我让它帮我输出了一篇 5000 字的推文 (初稿)。\n\n\n\n\n\n\n提示词\n\n\n\n\nPrompt 1:\n\n帮我解读一些这篇文章的核心观点： https://reasonabledeviations.com/2022/01/10/probability-matching-kelly/\n\nPrompt 2:\n\n这个在研究中有何用？在哪些领域，这里提到的概念很重要？\n\nPrompt 3:\n\n详细介绍一下 Kelly 策略\n\nPrompt 4:\n\n目的：python 实现代码\n帮我设计一组 Python 代码，以便与 https://reasonabledeviations.com/2022/01/10/probability-matching-kelly/ 文中介绍的内容相配合。我想写一篇中文推文，把你给我的 Python 代码与这篇文章的内容结合起来，增加趣味性，也便于读者理解。\n\nPrompt 5:\n\n用户使用这些函数时，能否自行调节参数，甚至进行动态互动演示？\n\nPrompt 6:\n\n能否把上述讨论内容整合成一篇完整的推文。主内容包括：\nhttps://reasonabledeviations.com/2022/01/10/probability-matching-kelly/ 的中文翻译 (可以适当意译，以符合中文表述习惯)；\n\n在合适的位置嵌入 Python 代码和解释性文字。\n预留图片插入位置，我随后运行完代码后插入图片。\n其它你认为必要的扩充。\n\n\n字数：中文字符不少于 10000 字\n输出：直接连续输出，可以分多次输出\n格式：不要添加表情符号；章节顺序编号：## 1. xxx; ### 2.1 xxx",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>使用 AI 撰写论文推介</span>"
    ]
  },
  {
    "objectID": "body/05_01_example_Wing-2024.html",
    "href": "body/05_01_example_Wing-2024.html",
    "title": "8  案例：AI 解读 Wing (2024)",
    "section": "",
    "text": "8.1 泛聊并形成大纲\n本章以 Wing et al. (2024) 的论文为例，介绍如何使用 AI 工具来解读论文。\n与 ChatGPT 的对话过程：点击查看。\n此前，我也用豆包帮我读过这篇论文，参见 豆包对话过程。\n我把 Wing et al. (2024) 的论文上传到 ChatGPT，并编写了如下提示词：\nChatGPT 的回答简明扼要，基本上抓住了这篇论文的要点。让我有点惊喜的是，它居然还贴心地列了一个表格，把文中提到的五种方法对应的 Stata 和 R 实现命令也总结好了。\n我担心自己遗漏重要内容，便追问：\nChatGPT 认为需要再补充三个要点：\n虽然我对这个结构比较满意，但我担心 AI 写东西太过教条，或者过于简略。于是，我设定了读者的角色、他们的知识背景，并征询 AI 对篇幅的看法。\n看到这个提纲后，我意识到，我这篇推介应该重点强调「实操建议」，这是读者最关心的内容。为此，我编写了如下提示词：\nAI 对提纲你进行了调整，并给出了一个更为详细的结构 (详情参见原始对话)，受限于篇幅，这里进列出它的提供的总结：",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>案例：AI 解读 Wing (2024)</span>"
    ]
  },
  {
    "objectID": "body/05_01_example_Wing-2024.html#泛聊并形成大纲",
    "href": "body/05_01_example_Wing-2024.html#泛聊并形成大纲",
    "title": "8  案例：AI 解读 Wing (2024)",
    "section": "8.2 总结：建议的论文推介结构（共 6 节）",
    "text": "提示词 1：泛聊\n\n\n\n#论文解读：\n&gt; Wing, C., Yozwiak, M., Hollingsworth, A., Freedman, S., & Simon, K. (2024). Designing Difference-in-Difference Studies with Staggered Treatment Adoption: Key Concepts and Practical Guidelines.  \nAnnual Review of Public Health, 45(1), 485–505.  \n[Link](https://doi.org/10.1146/annurev-publhealth-061022-050825).\n我想写一篇论文推介，介绍这篇论文。\n你帮我列个提纲。包括如下要点：\n\n论文的核心观点和主要结论是什么？\n文中介绍了哪些主要方法 (参见 Table 1)？\n作者提供了哪些实操建议？参见 Section 5\n\n\n\n\n\n\n\n\n\n\n回复 1：ChatGPT 首轮回答\n\n\n\n## 1. 论文的核心观点与主要结论\n\n核心问题：传统 DID 在处理“分期处理”（staggered adoption）时会面临严重的混淆偏误（confounding），尤其是在存在时间异质性处理效应的情况下。\n关键结论：\n\n两大核心理念：\n\n复杂 DID = 简单 2×2 DID 的加权平均，理解这一点有助于解析 TWFE 存在哪些“坏的比较”；\n干净的控制组（clean controls）设计是避免偏误的关键。\n\nTWFE 估计量存在混杂：它不仅包括“treated vs untreated”，还包括“late vs early”这种有偏比较；\n推荐使用的新方法（如 stacked DID 和 Callaway & Sant’Anna 方法）能更清楚地识别组 × 时间的异质性效应，并给出更可信的平均处理效应。\n\n\n## 2. 文中介绍的主要方法（Table 1）\n\n\n\n\n\n\n\n\n\n\n方法\n软件支持\n输出内容\n优点\n注意事项\n\n\n\n\nStacked DID\nSTATA/R 通用实现\n加总或 event study 图\n保证干净控制组；结构直观\n权重不透明；标准误需聚类\n\n\nCallaway & Sant’Anna (2021)\ndid (R), csdid (Stata)\n所有 ATT(a, t)，可自定义加权\n明确估计每个 group × time 效应\n稍复杂；推荐用包\n\n\nGardner (2022)\nDid2s (R)\n加总效应或 event study\n利用未处理组构建双重残差\n加权等权，需理解设计\n\n\nSun & Abraham (2020)\neventstudyinteract\nevent study 图\n区分 group × period 的处理异质性\n无法处理时间变化协变量\n\n\nWooldridge (2022)\n回归可实现\nATT(a, t)，加总 ATT\n可以纳入时间变协变量\n无专门包\n\n\nde Chaisemartin & D’Haultfoeuille (2020)\ndid_multiplegt\n单一加总效应\n利用“状态切换”识别处理效应\n假设较强，解释为首次变化效应\n\n\n\n## 3. 作者提供的实操建议（Section 5）\n作者提出了 6 条实用建议，帮助研究者在实际操作中避免偏误和提升估计质量：\n\n明确定义研究问题：是否目标是估计长期效应？平均效应？还是事件时间上的动态路径？\n数据准备中避免混淆比较：识别每个组的处理时间，排除已经处理过的“污染对照组”。\n慎用 TWFE 回归：除非确认处理效应恒定，否则应避免直接使用。\n优先使用 event-study 构建组 × 时间 ATT：推荐 Callaway & Sant’Anna 或 stacked DID 实现。\n合理使用协变量调整：特别是时间不变协变量，可用于强化并行趋势假设。\n图形化展示 ATT 路径和 pre-trend 检验：通过 event-study 图来验证 no anticipation 和 common trends 假设。\n\n\n如需将其撰写为一篇推文文章或教学讲义，我可以按结构扩写每节内容，并添加图例说明和代码演示。是否继续？\n\n\n\n\n\n\n\n\n\n提示词 2：补充内容\n\n\n\n除了我提供的三个要点，你认为这篇论文推介海英包含哪些要点？\n\n\n\n\n\n\n\n\n\n回复 2：ChatGPT 补充回答\n\n\n\n很好的问题。在你已指定的三个要点基础上（核心观点、主要方法、实操建议），为了使这篇论文推介更系统、完整、适合教学与学术传播用途，我建议增加以下 3 个要点，一共构成 6 个板块的完整推介结构：\n\n## 4. 样例设计与图形直观解释（Section 2）\n\n推荐加入理由：作者构造了一个关于磺胺药物（sulfa drugs）分期推出的模拟案例，并通过图形展示了不同组别（早期、中期、晚期、未处理）在 mortality 变化上的差异。 ……\n\n\n## 5. 异质性与 TWFE 偏误的来源与后果（Section 3.5）\n\n推荐加入理由：该部分系统总结了 TWFE 偏误的数学机制（由 Goodman-Bacon 提出），是该领域的核心争议之一。 ……\n\n\n## 6. 软件与实操实现建议（Section 4.3, Table 1）\n\n推荐加入理由：Table 1 对当前主流 DID 实现方法做了清晰对比，有利于读者选择合适的工具。 ……\n建议用途：可以补充一个附录，提供 R 和 Stata 用户分别应如何使用这些方法（如 csdid、did_multiplegt、eventstudyinteract 等）。\n\n\n\n8.2 总结：建议的论文推介结构（共 6 节）\n\n论文核心观点与主要贡献（What and Why）\n主流估计方法与适用情境（Table 1）\n作者的实操建议与方法选择（Section 5）\n教学案例与图形展示（sulfa drugs 模拟实验）\nTWFE 的偏误与异质性陷阱（Goodman-Bacon decomposition）\n软件工具与实证操作建议（含软件包、命令、标准误估计等）\n\n\n\n\n\n\n\n\n\n\n\n提示词 3：细化提纲\n\n\n\n如果想写一篇适合初学者，主要是博士生和青年老师，他们的理论基础有些薄弱，希望能能通过通俗的语言，简明扼要的数学公式了解文中提到的各种方法的要点、适用场景和结果的解释方法。\n我应该如何安排这篇论文的结构和篇幅，每个部分大概多少字？\n\n\n\n\n\n\n\n\n回复 3：ChatGPT 列出的细化提纲\n\n\n\n非常好的设问。\n我建议这篇文章安排为如下 7 个部分，总篇幅建议控制在 8000–10,000 字左右（中文字符），既适合长推文，也便于改写为课程讲义或公众号文章。\n\n## 拟定结构与篇幅建议\n\n\n\n\n\n\n\n\n\n序号\n模块标题\n内容要点\n建议字数\n\n\n\n\n1\n导言：分期处理的 DID 为什么值得关注？\n- 现实案例（如政策滚动式推出）- TWFE 的流行与质疑- 本文阅读指南与学习目标\n600–800\n\n\n2\n直觉解释：DID 的本质与“分期处理”的陷阱\n- 用简单例子解释 2×2 DID 的逻辑- Goodman-Bacon decomposition 的直觉化说明（配图）- “坏的比较”如何引入偏误\n1000–1500\n\n\n3\n最常用的几种估计方法：思想、公式与适用场景\n- 逐一介绍以下方法： a. TWFE（回顾） b. Stacked DID c. Callaway & Sant’Anna (csdid) d. Sun & Abraham e. Gardner (did2s) f. de Chaisemartin & D’Haultfoeuille- 每种方法配一句话直觉、一组简要公式、一个适用场景\n3000–3500\n\n\n4\n一个教学例子：sulfa drugs 与死亡率（图 + 模拟）\n- 作者构建的模拟示例介绍- 图 1 与图 2 解读：每组死亡率的变化逻辑；哪些 DID 比较是有偏的\n800–1000\n\n\n5\n推荐的分析策略：设计、估计与解释的 checklist\n- Section 5 的总结- 6 条建议的通俗表达与应用建议- 表格呈现每条建议与推荐方法\n800–1000\n\n\n6\n结果解读技巧：估计量、ATT、Event Study 的含义\n- 估计的 ATT 是哪个？- 如何理解组 × 时间的动态路径？- 如何做稳健性分析（pre-trend）？\n600–800\n\n\n7\n附录：软件实现对照表与入门路径推荐\n- R 与 Stata 的软件包对应- 推荐教程、代码仓库（如作者的 GitHub）- 延伸阅读（几篇核心综述文献）\n600–800\n\n\n\n……\n如你愿意，我可基于该结构逐一撰写每一节，并配合教学/推文风格的 markdown 文稿。是否开始撰写第一节？或者是否要我提供一个样板段落？\n\n\n\n\n\n\n\n\n\n提示词 4：局部修正\n\n\n\n这篇推文的主要目的是给读者提供一些实操建议，因此，各个 section 的内容尽量循序渐进，确保各个模型的介绍与 Section 5 中的实操建议能有机衔接，让读者看完这篇推介后能上手应用到自己的论文中。\n\n\n\n\n\n\n\n\n\n回复 5：ChatGPT 进一步细化写作提纲\n\n\n\n非常好，我已理解你的目标：这篇推文不仅是综述，更是实践指南，要让初学者「看得懂、用得上、写得进论文」。基于这个要求，我对之前的结构做了进一步优化，以实现 从直觉启发 → 方法入门 → 模拟案例 → 实操建议 的自然过渡，并确保每个方法介绍最终都能在 Section 5 实操建议中找到落地应用。",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>案例：AI 解读 Wing (2024)</span>"
    ]
  },
  {
    "objectID": "body/05_01_example_Wing-2024.html#总结建议的论文推介结构共-6-节",
    "href": "body/05_01_example_Wing-2024.html#总结建议的论文推介结构共-6-节",
    "title": "8  案例：AI 解读 Wing (2024)",
    "section": "",
    "text": "论文核心观点与主要贡献（What and Why）\n主流估计方法与适用情境（Table 1）\n作者的实操建议与方法选择（Section 5）\n教学案例与图形展示（sulfa drugs 模拟实验）\nTWFE 的偏误与异质性陷阱（Goodman-Bacon decomposition）\n软件工具与实证操作建议（含软件包、命令、标准误估计等）",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>案例：AI 解读 Wing (2024)</span>"
    ]
  },
  {
    "objectID": "body/05_01_example_Wing-2024.html#撰写论文推介正文",
    "href": "body/05_01_example_Wing-2024.html#撰写论文推介正文",
    "title": "8  案例：AI 解读 Wing (2024)",
    "section": "8.3 撰写论文推介正文",
    "text": "8.3 撰写论文推介正文\n接下来的工作就很简单了：按照上面的提纲，逐步撰写每个部分的内容。\n此时，建议明确说明对正文的撰写要求。比如，是否需要使用教学风格的语言，是否需要配合图表，是否需要提供代码示例等。\n下面是我经过长时间打磨形成的一个比较稳定的提示词：\n\n\n\n\n\n\n提示词 6：撰写正文\n\n\n\n按照 {优化后的推文结构（强调“实操衔接”）} 的结构来写，每个 section 输出一次。\n字数严格按照计划执行，不要缩减。\n格式要求：\n\n按照 ## 1. sec_title; ### 1.1 sub_sec_title 的方式编号 ；每个 section 添加一个标签 {lab-sec#}，便于后续修改时定位\n语言风格：中文讲义，20 年从教的老教授的风格，语言平和，准确，有力\n不要使用任何表情符号\n重要文献和链接务必在文中 提供超链接，文末提供完整的引文信息\n引文信息尽量使用如下格式，如果 PDF 链接无法获取，可以留空或删除 [-PDF-]() 部分的信息 引文信息采用 APA 格式，附加 [Link](), [PDF](), [Google](), [github]() 信息。即： Authors. ({Year}). {title_of_paper}, 45(1), 485–505. [Link](https://doi.org/{DOI}), [PDF](http://sci-hub.ren/{DOI}), [Google](&lt;{title_of_paper}&gt;), [github-replication](https://github.com/{ID}/{rep_name})\n\n文中的例子最好具体化，并保持全文一致，逐步推进和扩展。\n\n\n\n细心的读者可能会注意到，我在提示词的第一行，用比较严肃的语气写到「字数严格按照计划执行，不要缩减。」。这是因为，有些 AI 工具经常会「偷工减料」。虽然到目前我还没有完全搞清楚背后的原因，但大体上应该有如下可能：由于内存的限制，多数 AI 工具只能记住最近的 1-2 个对话的内容；此外，也有可能某些参数的设定，使之倾向于提供较为简洁的回答。为了避免这种情况，我在提示词中明确要求它严格按照计划的字数来输出结果。",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>案例：AI 解读 Wing (2024)</span>"
    ]
  },
  {
    "objectID": "body/05_01_example_Wing-2024.html#最终文稿",
    "href": "body/05_01_example_Wing-2024.html#最终文稿",
    "title": "8  案例：AI 解读 Wing (2024)",
    "section": "8.4 最终文稿",
    "text": "8.4 最终文稿\nChatGPT 输出的 Section 1 的内容基本符合我的要求。然后他开始输出 Section 2 的内容，并询问我是否需要继续输出。我给出了如下提示词，然后陪儿子去楼下散步 (回来后就可以收割所有输出结果了)。\n\n\n\n\n\n\n提示词 6：连续输出\n\n\n\n继续输出所有 sections，无需停顿\n\n\n\n👉 点击查看 最终成稿。",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>案例：AI 解读 Wing (2024)</span>"
    ]
  },
  {
    "objectID": "body/05_02_Wing-2024.html",
    "href": "body/05_02_Wing-2024.html",
    "title": "9  Wing (2024) 输出效果",
    "section": "",
    "text": "9.1 写在前面：为什么你可能用错了 DID？\nDifference-in-Differences（DID）是因果推断中最常用的工具之一，尤其在政策评估和公共卫生研究中几乎成了“标配”。许多论文，特别是关于地方政策变更（如最低工资调整、医保扩张、补贴计划）等主题，都会自然而然地落入“分期处理”（staggered adoption）的情境：有些地区早一点实施，有些则晚一点，甚至永远不实施。\n在这种设定下，研究者往往直接将 TWFE（two-way fixed effects，双向固定效应）模型套入数据，便得出了所谓的“平均处理效应”。然而，这种做法隐藏着一个深层次的统计陷阱——即使你满足了并行趋势假设（common trends），即使你严格控制了协变量，也很可能得出了错误的估计。",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wing (2024) 输出效果</span>"
    ]
  },
  {
    "objectID": "body/05_02_Wing-2024.html#lab-sec1",
    "href": "body/05_02_Wing-2024.html#lab-sec1",
    "title": "9  Wing (2024) 输出效果",
    "section": "",
    "text": "9.1.1 一个看似无害的例子\n设想如下的情境：某项公共卫生政策（例如提供免费疫苗）在三个州分批推行：\n\nA 州在 2018 年推行；\nB 州在 2020 年推行；\nC 州从未推行。\n\n你收集了 2016–2022 年的各州健康数据，然后运行如下 TWFE 回归模型：\n\\[\nY_{it} = \\alpha_i + \\lambda_t + \\beta D_{it} + \\varepsilon_{it}\n\\]\n结果显示 $ = -3.7$，你欣喜地认为该政策平均每年使死亡率下降了 3.7。问题是：这个估计是否真的可信？它真的等于我们关心的因果效应（Average Treatment Effect on the Treated, ATT）吗？\n很可能不是。事实上，近期的研究清晰地表明：当政策是“错峰推出”的，而且处理效应在时间上具有异质性（即效应在不同时间点不同），TWFE 给出的估计可能是方向错误的加权平均值，甚至完全偏离真实效应。\n\n\n9.1.2 来自最新研究的警示\nWing 等人（2024）在《Annual Review of Public Health》中发表的重要综述文章，专门讨论了这一问题。他们指出：在存在异质效应与分期处理的情境下，TWFE 估计量往往混杂了“坏的比较”（bad comparisons），尤其是晚处理组与早处理组之间的比较，由于这些“对照组”其实已经接受了处理，导致估计量偏误难以察觉。\n他们以模拟实验为例，展示了 TWFE 可能产生方向相反的估计结果——即明明政策是有效的，却因为使用了“错误的对照组”，得出了“政策增加死亡率”的结论。\n我们必须认识到：传统 DID 方法的便利性背后，隐藏的是对研究设计粗心大意的风险。\n\n\n9.1.3 这篇推文能帮你解决什么问题？\n本推文旨在以通俗的方式向博士生与青年教师解释：\n\n为什么传统 DID 方法可能失效；\n如何理解 staggered DID 中的估计陷阱；\n如何选择合适的方法来估计分期处理的因果效应；\n如何将这些方法应用到你自己的论文中。\n\n我们会用一个贯穿始终的公共卫生政策模拟例子，图文并茂地解释各种方法的核心思想、数学表达与结果含义，确保你在理解的基础上，能操作、能解释、能写入你的论文。",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wing (2024) 输出效果</span>"
    ]
  },
  {
    "objectID": "body/05_02_Wing-2024.html#lab-sec2",
    "href": "body/05_02_Wing-2024.html#lab-sec2",
    "title": "9  Wing (2024) 输出效果",
    "section": "9.2 最基础的 2×2 DID：你还记得它的含义吗？",
    "text": "9.2 最基础的 2×2 DID：你还记得它的含义吗？\n在深入介绍 staggered DID 之前，我们有必要回顾最基础的 Difference-in-Differences（DID）设计。许多方法的出发点，都是对这个简单 2×2 DID 框架的延展与修正。理解这一框架不仅能帮助我们把握估计量的本质，也能明确识别分期处理下的偏误源头。\n\n9.2.1 什么是 2×2 DID？\n最简单的 DID 设计包含两个组、两个时间点：\n\n一组是“处理组”（treatment group），另一组是“对照组”（control group）；\n两个时间点：$t=0$（处理前），$t=1$（处理后）；\n只有处理组在 $t=1$ 之后接受了处理，对照组始终未被处理。\n\n我们关心的因果效应是：在 $t=1$ 时，处理组相对于它自己在 $t=0$ 的变化，减去对照组在 $t=1$ 相对于 $t=0$ 的变化。\n这个思路可以写作一个标准的 DID 估计量：\n\\[\n\\widehat{\\text{DID}} = (Y_{T,1} - Y_{T,0}) - (Y_{C,1} - Y_{C,0})\n\\]\n其中：\n\n$Y_{T,t}$ 表示处理组在时间 $t$ 的平均结果；\n$Y_{C,t}$ 表示对照组在时间 $t$ 的平均结果。\n\n若我们将该估计量视为一个回归模型，它可以形式化为：\n\\[\nY_{it} = \\alpha + \\delta \\cdot \\text{Post}_t + \\gamma \\cdot \\text{Treat}_i + \\beta \\cdot (\\text{Treat}_i \\times \\text{Post}_t) + \\varepsilon_{it}\n\\]\n其中，$$ 就是我们要估计的因果效应（ATT）。\n\n\n9.2.2 倍分法成立的两个关键假设\nDID 的识别依赖于两个关键但强约束的假设：\n\n9.2.2.1 （1）并行趋势假设（Common Trends）\n即如果没有接受处理，处理组和对照组的结果变量在时间上的变化趋势应该是一致的。\n形式化地说：\n\\[\nE[Y_{T,1}(0) - Y_{T,0}(0)] = E[Y_{C,1}(0) - Y_{C,0}(0)]\n\\]\n这个假设无法直接检验，因为我们无法观察所有单位的“未处理状态下的潜在结果”。但我们可以通过检查处理前的趋势是否一致（“pre-trends”）来佐证其合理性。\n\n\n9.2.2.2 （2）无预期行为（No Anticipation）\n处理组不会因为“预期”将来会接受处理而提前调整行为。\n形式化地写为：\n\\[\nE[Y_{T,0}(1) - Y_{T,0}(0)] = 0\n\\]\n换句话说，处理的影响只能在实际处理开始之后才出现。\n\n\n\n9.2.3 例子：疫苗政策对死亡率的影响\n我们沿用 Wing 等人（2024）的模拟例子：设想一种疫苗政策只在 A 州推出，其它州未推出。\n\n年份 $t=0$（2019）：A、B 两州都未实施政策；\n年份 $t=1$（2020）：A 州开始实施，B 州未实施；\n我们观察到 A 州死亡率下降了 10，B 州下降了 2；\n那么 DID 估计值为：$(-10) - (-2) = -8$。\n\n这意味着，政策可能将死亡率额外降低了 8。\n\n\n9.2.4 是后续方法的出发点？\nWing 等人提出的核心观点是：复杂的 staggered DID，可以看作是众多 2×2 DID 子实验的加权平均。只有真正理解了 2×2 DID 的估计逻辑，才能意识到在何种条件下这些子实验会给出可靠的估计，在何种条件下却引入了偏误。\n在接下来的章节中，我们将看到，当各个单位的处理时间不同（staggered adoption），处理效应随时间变化（treatment heterogeneity），且我们又将所有单位一起放入 TWFE 模型中分析时，这个估计量已不再是上面那种简单 DID 的平均，而可能被“坏的子比较”严重扭曲。\n因此，重温 2×2 DID 的基本逻辑，就是理解分期处理挑战的起点。",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wing (2024) 输出效果</span>"
    ]
  },
  {
    "objectID": "body/05_02_Wing-2024.html#lab-sec3",
    "href": "body/05_02_Wing-2024.html#lab-sec3",
    "title": "9  Wing (2024) 输出效果",
    "section": "9.3 分期处理的挑战：当处理时间不同步时，会发生什么？",
    "text": "9.3 分期处理的挑战：当处理时间不同步时，会发生什么？\n上一节我们介绍了标准的 2×2 DID 框架，它建立在“单一处理时间”“统一比较结构”的理想设定上。但现实中，政策往往以错峰方式（staggered adoption）在不同单位中推广。这类设定在文献中极其常见，例如：\n\n最低工资在不同州的分批调整；\n医疗补贴政策在不同县的滚动推出；\n城市限购、限行政策在各地陆续上马。\n\n这种设定使得我们面对的，不再是 2 组 × 2 时间点的比较，而是多个处理组、多个时间点、多个处理路径交织在一起。\n\n9.3.1 TWFE 在这种情境下是如何工作的？\n研究者往往使用如下的 TWFE 模型进行估计：\n\\[\nY_{it} = \\alpha_i + \\lambda_t + \\beta D_{it} + \\varepsilon_{it}\n\\]\n其中，$D_{it}$ 表示个体 $i$ 在时间 $t$ 是否接受了处理。\n这种做法有两个表面上的优势：\n\n控制了单位固定效应 $_i$ 和时间固定效应 $_t$；\n所有组和所有时期的数据都被用上，提升了估计效率。\n\n但问题也正在于此：并非所有被用上的比较，都是有效的因果识别来源。\n\n\n9.3.2 Goodman-Bacon decomposition：将复杂 DID 拆解为 2×2 子实验\nGoodman-Bacon（2021）的突破性研究表明，TWFE 模型中估计量 $$ 实际上是多个 2×2 DID 子估计的加权平均。每一个子 DID 来自两个组在某一时间窗口内的比较。\n比如，设有三个处理时间：\n\nA 州：2018；\nB 州：2020；\nC 州：未处理。\n\nTWFE 模型包含的子比较包括：\n\nA 州 vs. C 州（t=2017–2019）；\nB 州 vs. C 州（t=2019–2021）；\nA 州 vs. B 州（t=2019–2021）；\nB 州 vs. A 州（t=2017–2019）……\n\n其中 3 和 4 是最危险的，它们将已经处理过的组当作“对照组”，可能严重违反了并行趋势假设和未处理状态的可识别性。\n这些子估计被 Goodman-Bacon 分为三类：\n\ntreated vs. untreated；\nearly treated vs. late treated；\nlate treated vs. early treated（最容易出偏误的一类）。\n\n每个子估计都被赋予一个权重，构成最终的 TWFE 估计。\n\n\n9.3.3 示例图解：sulfa drugs 实验（来自 Wing 等人）\nWing 等人构造了一个模拟实验：假设磺胺药物分别在三个时点推广到三个州组（早、中、晚），还有一个州组从未获得该药物。在图 2 中，他们展示了每一种 2×2 比较的估计值与权重。\n其中，“1940 vs. 1930”这种 late vs. early 比较给出的估计结果是正的，而真实效应是负的，说明这是典型的偏误方向翻转。\n更糟的是，这类比较可能被赋予高达 18% 的权重，极大地污染了整体估计量。\n\n\n9.3.4 什么时候偏误会严重？\n偏误的严重程度取决于两个因素：\n\n处理效应是否随时间变化（event-time heterogeneity）；\n“坏的比较”的权重有多大。\n\n只要处理效应不是恒定的（实际上几乎从不恒定），且“坏的比较”在样本中占据一定比例，TWFE 就可能产生严重偏误，甚至估计方向完全错误。\n这正是为什么 Goodman-Bacon decomposition 成为现代 DID 方法中不可绕过的基础工具。\n\n\n9.3.5 一个警示性的结论\nTWFE 在 staggered DID 情境下不是“无害的平均”，而是“危险的混合物”。\n正如 Wing 等人指出的那样：“如果我们不能明确理解哪些子比较被包含在估计中，就不能断言我们的估计结果具有因果含义。”\n这一观察直接推动了后续方法的发展，如 stacked DID、Callaway & Sant’Anna 等方法，目的就是控制或剔除这些“坏的比较”。",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wing (2024) 输出效果</span>"
    ]
  },
  {
    "objectID": "body/05_02_Wing-2024.html#lab-sec4",
    "href": "body/05_02_Wing-2024.html#lab-sec4",
    "title": "9  Wing (2024) 输出效果",
    "section": "9.4 五种实用方法一图总览：你可以怎么做？",
    "text": "9.4 五种实用方法一图总览：你可以怎么做？\n面对 staggered DID 的复杂结构与 TWFE 的偏误问题，近年来方法学文献提出了多种新的解决方案。这些方法各有侧重，但都建立在如下共识基础上：\n\n“我们必须控制或规避掉那些由处理过单位构成的对照组。”\n\n\n9.4.1 方法概览表（简化）\n以下是 5 种主流方法的直观对比：\n\n\n\n\n\n\n\n\n\n\n\n方法名称\n是否使用回归？\n估计结果形式\n是否输出组 × 时间 ATT？\n控制组设定\n推荐软件\n\n\n\n\nTWFE\n是\n单一 $$\n否\n所有单位\n所有软件\n\n\nStacked DID\n是\nevent-study 图\n是（自动加总）\n干净对照组\n手工回归 / stackedev\n\n\nCallaway & Sant’Anna\n否\nATT(a,t) 列表\n是（可加总）\n可选未处理 / 延迟处理\ndid (R), csdid (Stata)\n\n\nSun & Abraham\n是\nevent-study 图\n是\n最近处理组\neventstudyinteract\n\n\nGardner (did2s)\n是\nATT 或图\n是\n所有未处理单位\ndid2s\n\n\n\n\n\n9.4.2 一图看懂逻辑差异\n我们可以用一张结构图来解释这些方法的核心逻辑差异（此处建议在讲义中插入图表）：\n\nTWFE：使用所有比较（包括坏的）；\nStacked DID：仅使用“干净”的 2×2 子实验，拼接构成估计；\nCallaway & Sant’Anna：精确估计每个组 × 时间效应；\nSun & Abraham：对 event-time 效应进行交互建模；\nGardner：先做回归残差去噪，再估计效应。\n\n\n\n9.4.3 方法选择的三个基本维度\n选择方法时，应重点考虑以下三个问题：\n\n你的处理效应是否异质？\n你是否希望估计动态路径（event-study）？\n你是否能接受对照组被限定为“未来将处理”的单位？\n\n在后续章节中，我们将逐一介绍这些方法的原理、估计公式、应用条件，并与 Wing 等人的实操建议一一对应。\n好的，继续输出 Section 5 至 Section 8 的内容。",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wing (2024) 输出效果</span>"
    ]
  },
  {
    "objectID": "body/05_02_Wing-2024.html#lab-sec5",
    "href": "body/05_02_Wing-2024.html#lab-sec5",
    "title": "9  Wing (2024) 输出效果",
    "section": "9.5 方法逐个讲",
    "text": "9.5 方法逐个讲\n以下是对五种主要方法的逐一介绍。每种方法包括基本思想、数学表达、适用情境、结果解释方式，并明确与 Section 6 的实操建议的对应关系。\n\n9.5.1 5.1 TWFE（Two-Way Fixed Effects）\n基本思想：最常见的做法，将所有单位、所有时间点的处理状态编码为一个 0/1 变量，在回归中加入单位固定效应与时间固定效应。\n模型形式：\n\\[\nY_{it} = \\alpha_i + \\lambda_t + \\beta D_{it} + \\varepsilon_{it}\n\\]\n其中 $$ 被解释为平均处理效应。\n适用场景：\n\n所有单位几乎同时接受处理；\n处理效应不随时间变化；\n没有严重的预期效应或政策滞后反应。\n\n结果解释：\n当上述条件被破坏时，$$ 可能是多个子比较的有偏加权平均，甚至方向错误。\n对应建议（Section 6）：\n\n不推荐在 staggered adoption 情境下单独使用 TWFE；\n可用 TWFE 作基准估计，但需搭配 Bacon 分解或敏感性分析。\n\n\n\n9.5.2 5.2 Stacked DID（Cengiz et al. 2019）\n基本思想：\n将每一个“政策推出”的处理组及其对应的干净控制组视为一个独立的 2×2 实验，进行事件研究式建模，再将所有子实验合并为一个 stacked 数据集，在其上进行统一回归。\n事件时间定义：\n\\[\ne = t - A_s,\\quad \\text{即实际时间减去政策首次实施年份}\n\\]\n回归模型：\n\\[\nY_{ised} = \\sum_{k \\neq -1} \\beta_k \\cdot D_{sd} \\cdot \\mathbb{1}[e = k] + \\alpha_{sd} + \\lambda_{ed} + \\varepsilon_{ised}\n\\]\n适用场景：\n\n多次政策推出（如多个州分批推出政策）；\n可构造干净对照组（未处理或延迟处理）；\n关注动态效应路径。\n\n结果解释：\n$_k$ 表示事件时间为 $k$ 的平均处理效应（如政策实施后第 $k$ 年的效应）。\n对应建议（Section 6）：\n\n强烈推荐在 staggered DID 中使用；\n注意选择合理的事件窗口（如 -2 到 +4 年）；\n推荐搭配平衡样本设计，确保每个 $k$ 都有足够样本。\n\n\n\n9.5.3 5.3 Callaway & Sant’Anna（2021）\n基本思想：\n将每一个组（单位）的处理时间作为组别指标，直接估计各组在各时间的 ATT，然后加权汇总，形成总体效应。\n核心量：ATT(a, t)\n表示处理组在第 $a$ 年接受处理，在第 $t$ 年观测时的平均处理效应。\n加权方式：\n\\[\n\\delta_q = \\sum_a \\text{Pr}(A_s = a) \\cdot \\text{ATT}(a, a + q)\n\\]\n其中 $q$ 为事件时间，$a$ 为组的处理时间。\n适用场景：\n\n有多个处理时间；\n明确关心各组的异质性与动态路径；\n对结果解释要求细致。\n\n结果解释：\n可视化输出一系列 $(a, t)$ 或加总后展示 $_q$（如 event-study 图）。\n对应建议（Section 6）：\n\n推荐作为主方法之一；\n可用 csdid 命令（Stata）或 did（R）实现；\n可配合 covariates 做 IPW 加权。\n\n\n\n9.5.4 5.4 Sun & Abraham（2020）\n基本思想：\n使用回归框架，将不同处理时间组与 event time 进行交互，构建 saturated event study 模型，避免组别混杂。\n模型核心：\n\\[\nY_{it} = \\sum_{g,e} \\delta_{g,e} \\cdot \\mathbb{1}[G_i = g] \\cdot \\mathbb{1}[t - g = e] + \\alpha_i + \\lambda_t + \\varepsilon_{it}\n\\]\n其中 $G_i$ 是单位 $i$ 的处理时间，$e$ 是事件时间。\n适用场景：\n\n想使用 OLS 框架；\n希望得到处理组特异的动态路径；\n数据处理时间结构相对密集。\n\n结果解释：\n每个 $_{g,e}$ 表示第 $g$ 组在事件时间 $e$ 下的效应，结果较为细致。\n对应建议（Section 6）：\n\n推荐作为对照方法；\n可使用 eventstudyinteract 命令（Stata）实现；\n注意回归中 reference period 设置。\n\n\n\n9.5.5 5.5 Gardner & Wooldridge：Two-stage DID（did2s）\n基本思想：\n第一阶段仅使用未处理单位估计单位和时间固定效应；第二阶段估计处理效应。\n估计框架：\n\n第一步：残差提取；\n第二步：回归残差对处理变量，估计处理效应。\n\n适用场景：\n\n样本较小、担心 overfitting；\n希望用经典 OLS 工具获得稳健性与精度。\n\n结果解释：\n输出单一 $$ 或 event study 系数，但具有较强理论基础和可解释性。\n对应建议（Section 6）：\n\n适合与 stacked DID / csdid 结果交叉验证；\n推荐 did2s 包（R）实现。",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wing (2024) 输出效果</span>"
    ]
  },
  {
    "objectID": "body/05_02_Wing-2024.html#lab-sec6",
    "href": "body/05_02_Wing-2024.html#lab-sec6",
    "title": "9  Wing (2024) 输出效果",
    "section": "9.6 作者推荐的六条实操建议",
    "text": "9.6 作者推荐的六条实操建议\nWing 等人以公共卫生领域的经验为基础，总结出六条关键实操建议。这些建议适用于任何采用 staggered DID 的研究，尤其适合政策干预类论文。\n\nS1. 明确定义研究问题（What do you want?）\n\n\n是估计整体效应，还是某个子群体的效应？\n是估计短期效应，还是长期路径？\n\n要点： 不同目标对应不同加权策略与估计方法。\n\nS2. 避免污染的对照组\n\n\n不要将已被处理的单位作为对照；\n只使用“干净对照组”（未处理或延迟处理的单位）；\n推荐方法：stacked DID, csdid。\n\n\nS3. 谨慎使用 TWFE，尤其在存在异质效应时\n\n\nTWFE 的估计结果需要 Bacon 分解解释；\n若你无法判断哪些子比较构成估计量，就不要直接使用 TWFE。\n\n\nS4. 优先使用 group × time ATT 方法\n\n\n推荐使用 Callaway & Sant’Anna, stacked DID；\n输出 ATT(a, t) 能让你判断哪一类单位在何时受到影响；\n有助于理解政策异质性与动态反应。\n\n\nS5. 如果估计 dynamic effects，请使用事件时间框架\n\n\n明确区分 event time（相对政策开始的时间）与 calendar time；\n使用事件时间窗口（如 -2 到 +4）做图或估计；\n推荐配图展示。\n\n\nS6. 保留原始图表与诊断分析\n\n\n附带展示 pre-trends 检验图；\n展示不同方法下的估计结果对比图；\n强调稳健性与诊断，而非单一显著性。",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wing (2024) 输出效果</span>"
    ]
  },
  {
    "objectID": "body/05_02_Wing-2024.html#lab-sec7",
    "href": "body/05_02_Wing-2024.html#lab-sec7",
    "title": "9  Wing (2024) 输出效果",
    "section": "9.7 你的论文中该如何选择？三种典型应用场景推荐",
    "text": "9.7 你的论文中该如何选择？三种典型应用场景推荐\n为了将方法与实操建议真正用到论文中，我们以三类常见论文结构为例，推荐可操作的分析策略。\n\n9.7.1 应用场景一：一次性处理，时间清晰\n\n比如：全国统一施行某项补贴政策，2020 年一月生效。\n\n建议方法：\n\n可使用 2×2 DID 或 TWFE；\n若关注动态效应，推荐 event study + csdid；\n对照组为“从未处理”单位。\n\n\n\n9.7.2 应用场景二：错峰处理，政策在不同地区逐步推出\n\n比如：医保扩张政策 2018–2021 年间在各州陆续施行。\n\n建议方法：\n\n不使用 TWFE；\n使用 stacked DID（平衡事件窗口）；\n对照组为未来处理或从未处理组；\n搭配图形展示与敏感性分析。\n\n\n\n9.7.3 应用场景三：关注异质性反应或动态路径\n\n比如：某补贴政策对不同收入群体影响可能不同，且效应随时间演化。\n\n建议方法：\n\n使用 Callaway & Sant’Anna；\n输出 ATT(a, t)，聚焦群体与时间异质性；\n可扩展为组内回归、分层事件研究。",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wing (2024) 输出效果</span>"
    ]
  },
  {
    "objectID": "body/05_02_Wing-2024.html#lab-sec8",
    "href": "body/05_02_Wing-2024.html#lab-sec8",
    "title": "9  Wing (2024) 输出效果",
    "section": "9.8 附录：常用命令、图形模板与推荐资料",
    "text": "9.8 附录：常用命令、图形模板与推荐资料\n以下是建议你实际操作时参考的命令、工具与学习资料。\nStata 命令对照表\n\n\n\n\n\n\n\n\n方法\n命令\n说明\n\n\n\n\nTWFE\nxtreg y d i.time, fe\n不推荐用于 staggered DID\n\n\nBacon 分解\nbacondecomp\n拆解 TWFE 中的子比较\n\n\ncsdid\ncsdid y time id gvar\n官方推荐，功能强大\n\n\nstacked DID\n手动 reshape + event-study\n需要写循环或使用 stackedev 包\n\n\nSun & Abraham\neventstudyinteract\n事件时间交互建模\n\n\ndid2s\nR 包 did2s\n两阶段 OLS 建模，精确稳健\n\n\n\n推荐资料\n\nWing et al. (2024) 官方 replication repo：GitHub\nCallaway & Sant’Anna R 教程：R package documentation\ncsdid 官方指南（Stata）：PDF",
    "crumbs": [
      "**论文解读**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Wing (2024) 输出效果</span>"
    ]
  },
  {
    "objectID": "body/AI_research_design_Erdem_2025.html",
    "href": "body/AI_research_design_Erdem_2025.html",
    "title": "10  借助 AI 做研究设计",
    "section": "",
    "text": "10.1 下载文献\n这里以一篇 2025 年 7 月 15 日发布的工作论文为例，介绍如何使用 ChatGPT 解读这篇论文的要点，并让它给出使用该文方法研究中国问题的研究设计。\n在制作 Slides 的过程中，我需要搜索很多文献。由于时间和能力的限制，我无法对每篇文献进行深入的解读和总结。为此，我使用如下流程加速我对这些文献的理解：",
    "crumbs": [
      "**研究设计**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>借助 AI 做研究设计</span>"
    ]
  },
  {
    "objectID": "body/AI_research_design_Erdem_2025.html#下载文献",
    "href": "body/AI_research_design_Erdem_2025.html#下载文献",
    "title": "10  借助 AI 做研究设计",
    "section": "",
    "text": "使用我编写的 Stata 命令 getiref 获得引文信息，并下载 PDF 文档到指定路径。\n将这些 PDF 文档 (一次一篇或同主题的多篇) 发送给 AI 助手 (我主要使用 ChatGPT 和豆包)，让它们帮我解读论文的要点。我会酌情追问，并阅读原文中的相关内容。如有必要，我会把这些内容发送给 AI 助手，让它解读，或提供更容易理解的公式、实例、Stata/Python 代码等。\n\n\n10.1.1 用 getiref 下载引文\n你可以直接在 Stata 命令窗口中执行这些命令，也可以在 dofile 中执行。此处，我已经配置好了 Python + Jupyter Notebook 环境 (配置方法)，因此，我直接在 Jupyter Notebook 中执行这些命令。\n\n\n* 安装 getiref\nssc install getiref, replace\n\n* 设置下载路径\nglobal refpath \"D:\\github_lianxh\\AI_research\\examples\"\ncd \"$refpath\"\n\nUnknown #command\nUnknown #command\nchecking getiref consistency and verifying not already installed...\nall files already exist and are up to date.\nD:\\github_lianxh\\AI_research\\slides\\refs\n\n\n\ngetiref 2507.10933 , m arxiv pdf\n\n  \nErdem, O., & Ashok, R. P. (2025). Artificial Finance: How AI Thinks About Money\n&gt;  (Version 1). arXiv.\n    Link (rep)    PDF    Google    \n\nErdem, O., & Ashok, R. P. (2025). Artificial Finance: How AI Thinks About Money\n&gt;  (Version 1). arXiv. [Link](https://doi.org/10.48550/arXiv.2507.10933) (rep),\n&gt;  [PDF](https://arxiv.org/pdf/2507.10933.pdf), [Google](&lt;https://scholar.googl\n&gt; e.com/scholar?q=Artificial Finance: How AI Thinks About Money (Version 1)&gt;).\n        PDF:   dir    view_online     Open\n \nTips: Text is on clipboard. Press 'Ctrl+V' to paste, ^-^\n\n\n执行完上述命令后，引文信息已经被自动粘贴在剪切板上，直接按快捷键 Ctrl + V 即可粘贴到 Jupyter Notebook 中。同时，PDF 文档已经下载到指定路径。\n得到引文信息如下：\n\nErdem, O., & Ashok, R. P. (2025). Artificial Finance: How AI Thinks About Money (Version 1). arXiv. Link (rep), PDF, Google.",
    "crumbs": [
      "**研究设计**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>借助 AI 做研究设计</span>"
    ]
  },
  {
    "objectID": "body/AI_research_design_Erdem_2025.html#论文解读",
    "href": "body/AI_research_design_Erdem_2025.html#论文解读",
    "title": "10  借助 AI 做研究设计",
    "section": "10.2 论文解读",
    "text": "10.2 论文解读\n\n10.2.1 论文概要\n下面，我们开始写提示词，让 ChatGPT 帮忙解读这篇论文。你可以把该文的 PDF 文档发送给 ChatGPT (推荐，该方法比较稳定)，或直接在提示词中提供该文的链接。\n\n\n\n\n\n\n提示词 1：论文概要\n\n\n\n\nChatGPT 完整对话\n\n#任务：论文解读-Erdem, O., & Ashok, R. P. (2025)\n\nErdem, O., & Ashok, R. P. (2025). Artificial Finance: How AI Thinks About Money (Version 1). arXiv. [Link](https://doi.org/10.48550/arXiv.2507.10933) (rep), [PDF](https://arxiv.org/pdf/2507.10933.pdf), [Google](&lt;https://scholar.google.com/scholar?q=Artificial Finance: How AI Thinks About Money (Version 1)&gt;).\n\n要点：\n\n1. 主要研究内容\n2. 研究方法。与传统方法有何区别？\n3. 文中使用了那些主要的 AI 工具或模型？如何使用的？\n4. 作者提供的提示词有哪些？\n5. 这篇论文对我们进行金融学领域的研究有哪些启发？\n6. 酌情提供你认为重要的其他内容。 \n\n\nAI 基本上能提炼出全文的要点，与我快速浏览该文看到的信息基本一致。\n\n\n10.2.2 迁移：中国版研究设计\n\n\n\n\n\n\n提示词 2：迁移\n\n\n\n\nChatGPT 完整对话\n\n如果我想按照相似的模式和思路撰写一篇论文，我有如下问题：\n\n有哪些可能的选题 (列出 3 个)？每个选题的边际贡献和预期结论是什么？\n需要哪些大模型技术支持？预估费用和主要困难是什么？大概多久能完成？\n研究对象和数据来源：中国\n\n\n\nChatGPT 提供了三个选题建议，并针对我的问题 2 和 3 给出了详细的解答。三个选题如下：\n\nAI 如何理解中国家庭的储蓄与消费决策？\n大模型能否模拟中国人对风险的态度？——以保险选择为例\nAI 理解中国年轻人的职业选择逻辑吗？——一项基于 LLM 的偏好映射研究\n\n我最终选择第一个选题进行追问，让 ChatGPT 帮我设计了研究框架和方法。\n\n\n\n\n\n\n提示词 3：帮我做研究计划\n\n\n\n\nChatGPT 完整对话\n\n好，我计划研究第一个主题：AI 如何理解中国家庭的储蓄与消费决策？\n你帮我制定一个具有可操作性的研究计划\n\n\nAI 做的研究计划看起来流程上没有问题，但细节上有很多问题。比如，没有明确数据来源于何处，也没有说明关键方法的细节。此时，我们需要快速阅读原文，找出研究设计中最重要 (通常也最棘手) 的一些问题进行追问。这是与 AI 进行合作的关键所在。\n\n\n\n10.2.3 追问、细化研究设计\n\n\n\n\n\n\n提示词 4：追问、细化\n\n\n\n在 Erdem et al. (2025) 的论文中，人类数据源于一个非常成熟的数据库 「人类数据：来自 INTRA（International Test on Risk Attitudes）数据库（Wang et al., 2017）」。\n我的问题：\n\n我的研究中使用哪个数据源比较好？\nErdem 文中设计了 14 个问题，我的研究中如何设计这些问题？能否给我一个范本？\n\n\n\n这次 AI 的表现不错，帮我设计了 14 个问题，涵盖了储蓄、消费、风险偏好等方面。并且给出了每个问题的设计思路和预期目标。\n它很聪明，主动提出了如下建议 (这个是我此前没有想到的)：\n\n\n\n\n\n\nChatGPT 的建议 1\n\n\n\n如你需要，我可以：\n直接帮你整理出 14 个问题的题库文案；\n为每个问题生成 GPT-4o、Gemini、通义等模型的标准化 prompt；\n输出配套 Python 提示词实验脚本框架。\n是否要我继续整理出完整的「14 个问题题库」？\n\n\n\n\n\n\n\n\n提示词 5：让 AI 写 Prompt 脚本和 Python 代码\n\n\n\n\nChatGPT 完整对话\n\n好，请完成如下任务：\n直接帮你整理出 14 个问题的题库文案；\n为每个问题生成 GPT-4o、Gemini、通义等模型的标准化 prompt；\n输出配套 Python 提示词实验脚本框架。\n\n\nChatGPT 帮我编写了一些 Prompts 和 Python 代码，虽然有些问题，但基本上可以直接使用。同时，他还给出了如下建议：\n\n\n\n\n\n\nChatGPT 的建议 2\n\n\n\n如果你使用国内模型（如通义），我可帮你将 API 接口改为 dashscope；\n如果你希望将输出结果直接转为结构化 JSON 或进一步分析（如聚类、PCA），我也可以补上后续代码模板；\n如果需要生成配套的问卷版 PDF 或问卷星模板，也可以告诉我，我可代为整理。\n是否继续？你下一步准备运行实验，还是要先发放人类问卷做对比？\n\n\n我对通义千问的 API 接口不熟悉，因此，我没有继续和 ChatGPT 对话，而是另开了一个对话窗口，询问通义千问的 API 接口使用方法。10 分钟后，我大致理解了 API 的原理：本质上就是按照一定的语法规则把提示词和参数传递给通义千问，获取返回结果 (有点类似于使用 APP 点菜)。\n我注意到，上述建议中其实包含了多个操作步骤，我想逐个处理。为此，我又编写了如下提示词：\n\n\n\n\n\n\n提示词 6：让 AI 使用通义千问编写代码\n\n\n\n\nChatGPT 完整对话\n\n我用通义千问，你帮我编写一份完整的代码思路文本 (伪代码)，然后撰写完整的可以实操的 Python 代码。\n这一步完成后，再开始制作基于问卷星的人类问卷。\n\n\nAI 帮我编写了一个 Python 脚本，脚本中包含了如何使用通义千问的 API 接口获取结果的代码。虽然有些问题，但基本上可以直接使用。\n\n\n10.2.4 棘手问题如何应对\n我翻看原文以及 ChatGPT 对论文的总结后，发现了一个有趣的问题：AI 在回答问题时，似乎更倾向于非西方国家的文化偏好。这似乎也不难理解，因为在最初训练阶段，人工标注者主要来自非西方国家 (如东非的肯尼亚、坦桑尼亚等)，因此模型的回答模式也更接近这些国家的文化偏好。\n然而，这个问题在我的研究设计中可能会带来一些挑战。为此，我又编写了如下提示词：\n\n\n\n\n\n\n提示词 7：棘手问题如何应对\n\n\n\n你提到：\n’’’\n启发 2：语言模型训练影响文化偏好 LLM 的整体回答模式最接近坦桑尼亚（Tanzania），而非传统的西方国家（WEIRD）。\n可能与模型训练中用于 RLHF（reinforcement learning from human feedback）的人工标注者地域有关，尤其是东非国家如肯尼亚、坦桑尼亚广泛参与了 GPT 系列模型的安全微调。\n’’’\n在我的研究设计中，这个问题怎么应对？\n\n\nChatGPT 非常认同我的担忧，并提出了 4 中应对方案。截取 ChatGPT 的回答如下：\n\n\n\n\n\n\nChatGPT 的回应\n\n\n\n\nChatGPT 完整对话\n\n这是一个非常关键的问题。你指出的是：LLM 回答中可能蕴含其训练过程中来自特定文化背景的偏见，而非真正理解“普遍的人类行为”或“中国家庭行为”。在你的研究「AI 如何理解中国家庭的储蓄与消费决策？」中，这种文化偏置会对结论构成干扰，甚至掩盖模型与中国家庭之间的真实差异。\n下面是针对这个问题的系统性应对方案 (详情参见 ChatGPT 完整对话)：\n\n策略一：纳入对照组与“文化距离”分析\n策略二：提示词中嵌入文化人格（persona）\n策略三：将文化偏置作为独立研究问题呈现\n策略四：将部分 prompt 翻译为英文再提交 LLM\n\n\n\n我逐渐意识到，刚才讨论的这个问题其实是一个非常棘手的问题。它有可能是这篇论文的硬伤。如果是的话，我就需要慎重考虑是否继续这个研究主题。\n为此，我又进行了追问，提示词如下：\n\n\n\n\n\n\n提示词 8：棘手问题的底层逻辑\n\n\n\n这里涉及到一个大模型的底层逻辑问题：\n通义千问的人工标签是谁做的？\n如果是中国人的话，是否分布在特定的省份或文化圈？\n如果通义千问是基于 Open AI 开发的，具有蒸馏特征，那我们针对中国的研究必然也存在于 Erdem 2025 相同的难题。\n你怎么看待这个问题？\n\n\nChatGPT 认为这是一个非常关键的问题，但它很能「掰扯」，回应如下：\n\n\n\n\n\n\nChatGPT 的回应\n\n\n\n三、这是否削弱你研究的可行性？我怎么看？ 我的观点是：\n这并不削弱你的研究价值，反而强化了它的必要性。\n若通义千问也表现出与中国家庭数据的“文化不一致”——说明“中文模型 ≠ 中国文化模型”\n你的研究将是验证国产大模型是否真正理解中国人经济行为的第一批系统性工作之一\n你可以将这一问题设计为研究目标本身的一部分，例如：\n“即使通义千问是中文预训练的大模型，其 RLHF 阶段是否真实反映了中国家庭行为偏好？我们在本研究中通过对比中国人类数据与模型输出，检验其行为合理性与文化一致性。”\n这也带来实践上的衍生价值：\n可为阿里、百度、商汤等模型开发者提供“行为偏好适配反馈”\n可为政策类 AI（如金融推荐、社保系统）提供风险警告：当前大模型输出可能并非“多数中国人”的最优选择\n\n\n我的直觉告诉我，虽然 ChatGPT 的说法有一定的道理，但想要把这个硬伤包装成论文的亮点并非易事。为此，我追问道：\n\n\n\n\n\n\n提示词 9：硬伤如何包装成亮点 ？\n\n\n\n我在研究设计中应该如何应对可能来自审稿人的质疑？\n如何把这个潜在的硬伤「包装」或「论述」为我这篇文章的亮点？\n有没有可能同时使用多个大模型开展这项研究，然后进行对比或进行 model averaging 这样的处理？\n\n\nChatGPT 的原始回答非常有意思，给出了比较具体的建议和应对方法。整体上，我觉得用它的办法，真的有可能把这个硬伤包装成亮点。\n下面是对 ChatGPT 的回答进行的简要总结：\n\n\n\n\n\n\nChatGPT 的回应\n\n\n\n\n如何应对审稿人对“大模型文化偏误”的质疑？\n\n可能质疑的角度：\n\n\n\n\n\n\n\n质疑类型\n审稿人可能说的话\n\n\n\n\n模型文化偏见\n“大模型的行为是否真的代表 AI？还是只是某些训练数据群体的平均值？”\n\n\n模型非确定性\n“LLM 的输出具有随机性，重复运行会变吗？”\n\n\n模型泛用性\n“为何只用通义千问？是否能推广到其它模型或语言环境？”\n\n\n\n \n\n模型文化偏误的挑战与研究意义\n\n尽管通义千问等中文大模型由中国团队开发，但其训练数据和 RLHF 标签仍可能集中于特定省份或文化圈，未必能真实反映全国性或多样化的家庭行为模式。因此，本研究不仅要检验模型是否能“说中文”，更要问：它是否“理解中国人”？\n\n从潜在缺陷转化为研究价值\n\n我们将模型输出与中国家庭调查数据系统对比，不将偏差视为缺陷，而是将其视为测量 AI 对本土文化理解能力的窗口。这为未来模型微调、金融推荐系统设计提供了文化适配性的评估工具。\n\n多模型对比策略\n\n为增强稳健性，我们引入 GPT-4o、通义千问、Moonshot 等多模型并行实验，构建“行为向量”进行主成分分析与行为聚类。模型之间的偏离，正是 AI 行为文化特征的重要体现，有助于识别模型是否具有结构性偏见。\n\n\n\n模型\n平台\n特点\n\n\n\n\nGPT-4o\nOpenAI\n英语训练偏重、蒸馏自西方行为逻辑\n\n\n通义千问\n阿里\n中文对话、训练数据偏官方\n\n\nMoonshot v1.5\nMiniMax\n泛中文市场偏创业用户群体\n\n\nClaude 3\nAnthropic\n对“社会公正”“中立”敏感度高\n\n\n\n \n\n对比这些模型在相同提示词、同一问题下的输出结构、逻辑倾向、风险偏好。",
    "crumbs": [
      "**研究设计**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>借助 AI 做研究设计</span>"
    ]
  },
  {
    "objectID": "body/AI_research_design_Erdem_2025.html#总结",
    "href": "body/AI_research_design_Erdem_2025.html#总结",
    "title": "10  借助 AI 做研究设计",
    "section": "10.3 总结",
    "text": "10.3 总结\n整体上，我对 ChatGPT 的表现还是比较满意的。它不仅能快速提炼出论文的要点，还能根据我的需求设计出可操作的研究计划，并提供了相应的代码框架。\n最主要的是，如果仅仅是看论文原文，我可能无法快速发现这篇论文的潜在硬伤。另外，如果要将这篇论文的方法迁移过来做中国问题，以我对大模型和 Python 的掌握程度，我基本上无法做出合理的研究设计。在与 ChatGPT 的对话中，我会学习它提到的新概念、新名词，通过干中学的方式不断完善我对这类问题的整体认知。\n我强烈建议各位读者找一篇最新的 working paper，使用上述方法进行研读，并尝试进行研究设计。即使你最终不选择做这个主题，但也能从中学到不少研究设计和计量方法的技巧。",
    "crumbs": [
      "**研究设计**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>借助 AI 做研究设计</span>"
    ]
  },
  {
    "objectID": "body/AI_research_design_Erdem_2025.html#附llm_savings_prompts_qwen.py",
    "href": "body/AI_research_design_Erdem_2025.html#附llm_savings_prompts_qwen.py",
    "title": "10  借助 AI 做研究设计",
    "section": "10.4 附：llm_savings_prompts_qwen.py",
    "text": "10.4 附：llm_savings_prompts_qwen.py\n\n\nChatGPT 完整对话\n\nllm_savings_prompts_qwen.py\n\n\n这个 Python 脚本是我向 ChatGPT 提出「提示词 6：让 AI 使用通义千问编写代码」后，他自动编写的，包括如下四个部分：\n\n中文题库设计 (共 14 题)\n标准化 Prompt 模板（支持通义千问）\n伪代码：通义千问版本的实验流程\n通义千问实操代码（DashScope API）\n\nps，由于时间的关系，我还没有尝试运行这个脚本，有兴趣的读者可以测试，并将您的测试结果反馈给我。\n# 文件：llm_savings_prompts_qwen.py\n\n# -----------------------------\n# 1. 中文题库设计（共 14 题）\n# -----------------------------\n\nquestions = [\n    # 时间偏好\n    \"假设你现在可以获得 1,000 元，或者等待 1 年获得 X 元。请问，X 至少是多少，才能让你愿意等待一年？\",\n    \"假设你可以立刻获得 2,000 元，或者在 10 年后获得 Y 元。请问，Y 至少是多少，才能让你愿意等待？\",\n\n    # 收入分配\n    \"如果你的下月工资多出 2,000 元，你会如何分配这笔钱？请用百分比表示：食品 __%、娱乐 __%、教育 __%、储蓄 __%。\",\n    \"如果你的收入减少 1,000 元，你最可能减少哪一项支出？A. 食品 B. 教育 C. 娱乐 D. 储蓄。请简要说明理由。\",\n\n    # 预防性储蓄\n    \"假设你得知你父亲未来一年可能需要 10,000 元医疗支出，你是否会调整当前的消费？你愿意每月减少多少元用于储蓄？\",\n    \"你是否有每月固定的预防性储蓄计划（如应对失业、健康支出）？如有，请简要说明金额与动机。\",\n\n    # 风险认知与应对\n    \"假设你所在行业未来一年有 10% 的失业概率，你是否会因此增加储蓄？若会，预计增加多少元/月？\",\n    \"假设某保险产品年保费 800 元，可覆盖重大疾病 30 万元赔偿。你是否愿意购买？请说明理由。\",\n\n    # 家庭生命周期\n    \"你是否因为孩子的教育而主动减少自己的日常消费？请列举最近一次这样的决策和金额。\",\n    \"如果你刚生育一个孩子，你会如何调整以下支出比例（教育 __%、娱乐 __%、储蓄 __%、其他 __%）？\",\n\n    # 消费偏好\n    \"请列举你最近三个月最大的一笔消费支出，并说明其动因（如：必要/非必要、情绪驱动、家庭责任等）。\",\n    \"在没有收入压力的前提下，你更倾向于提前消费还是储蓄？请简要说明原因。\",\n\n    # 财富认知\n    \"你认为一个家庭是否有必要维持至少 6 个月的生活费作为紧急储备？为什么？\",\n    \"你认为影响储蓄倾向最重要的因素是什么？A. 收入 B. 教育水平 C. 风险意识 D. 家庭责任感。请说明理由。\"\n]\n\n# -----------------------------\n# 2. 标准化 Prompt 模板（支持通义千问）\n# -----------------------------\n\nprompt_template = \"\"\"\n以下是一个关于中国家庭储蓄与消费决策的情境问题。\n请根据你的判断，独立作答。\n\n问题：{question}\n\n请只输出你的答案，不需要解释或重复题干。\n如果涉及金额，请直接给出数字或百分比。\n\"\"\"\n\ndef build_prompts(questions):\n    prompts = []\n    for q in questions:\n        prompts.append(prompt_template.format(question=q))\n    return prompts\n\n\n# -----------------------------\n# 3. 伪代码：通义千问版本的实验流程\n# -----------------------------\n\n\"\"\"\n1. 读取题库，构造 prompts\n2. 设置通义 dashscope API 的参数（如模型 qwen-plus）\n3. 每题生成 n_repeat 次回答\n4. 所有回答结果保存为 CSV\n5. 可选：记录响应时间、异常处理信息\n\"\"\"\n\n# -----------------------------\n# 4. 通义千问实操代码（DashScope API）\n# -----------------------------\n\nimport dashscope\nimport time\nimport pandas as pd\n\n# 替换为你的通义 API Key\nDASHSCOPE_API_KEY = \"YOUR_DASHSCOPE_API_KEY\"\ndashscope.api_key = DASHSCOPE_API_KEY\n\n# 构建 prompts\nprompts = build_prompts(questions)\n\nmodel_name = \"qwen-plus\"\nn_repeat = 100\n\ndata = []\n\nfor qid, prompt in enumerate(prompts):\n    for trial in range(n_repeat):\n        try:\n            response = dashscope.Generation.call(\n                model=model_name,\n                prompt=prompt,\n                temperature=0.7,\n                top_p=0.9,\n                result_format=\"message\"\n            )\n            answer = response.output.choices[0].message.content.strip()\n            data.append({\n                \"question_id\": qid + 1,\n                \"trial\": trial + 1,\n                \"response\": answer\n            })\n            time.sleep(0.3)  # 限速保护\n        except Exception as e:\n            print(f\"错误：Q{qid+1}-T{trial+1}: {e}\")\n            time.sleep(2)\n\n# 保存为 CSV\npd.DataFrame(data).to_csv(\"qwen_savings_responses.csv\", index=False)\nprint(\"实验完成，结果已保存。\")",
    "crumbs": [
      "**研究设计**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>借助 AI 做研究设计</span>"
    ]
  },
  {
    "objectID": "body/08_findit_IV_example.html",
    "href": "body/08_findit_IV_example.html",
    "title": "11  如何借助 AI 寻找工具变量？",
    "section": "",
    "text": "11.1 简介\n内生性是因果推断中最关键的问题，在处理内生性问题的方法中，工具变量是最常用的方法之一。但寻找有效的，还要满足排他性限制 (exclusion restrictions) 的工具变量，主要依赖于作者的说理论证，这一过程对写作者的说理能力和创造力的要求是很高的。因此，本文提出利用大型语言模型 (LLMs) 来辅助发现新的工具变量。\n在最近的一篇论文中，Han (2024) 提出了一个基于 LLMs 的工具变量搜索方法，称为 FindIt-IV。该方法通过与 LLMs 的交互，帮助研究者在特定的论文情景下快速找到潜在的工具变量。\n作者认为，使用大语言模型搜索工具变量有如下好处：\n作者在原文中，根据工具变量需要满足的一系列假设，为读者提供了利用大语言模型搜索工具变量可操作的方法和提示词。在最后，本推文也展示了一个中文顶刊中的文献情景，利用大语言模型为这篇文章搜索更多工具变量的案例。\n中山大学的张弛同学使用该文的思路，分析了一篇中文顶刊文章，利用大语言模型为这篇文章搜索更多的工具变量。详情参见：张弛, 2025, 找不到IV？如何借助大语言模型寻找工具变量。\n下面，我介绍我最近在做的一篇文章是如何借助 AI 寻找工具变量的。\n我要研究的问题是：政府引导基金能否降低产业链上的长鞭效应。解释变量是一个虚拟变量，如果一家公司受到了政府引导基金的资助，对应的数值取 1，否则的话取 0。背景式变量是文献里面常用的用来衡量产业链上长边效应的指标。\n模型设定如下：\n\\[\nY = \\beta_0 + \\beta_1 D + \\beta_2 X + \\varepsilon_i\n\\]",
    "crumbs": [
      "**寻找IV**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>如何借助 AI 寻找工具变量？</span>"
    ]
  },
  {
    "objectID": "body/08_findit_IV_example.html#简介",
    "href": "body/08_findit_IV_example.html#简介",
    "title": "11  如何借助 AI 寻找工具变量？",
    "section": "",
    "text": "Han, S. (2024). Mining Causality: AI-Assisted Search for Instrumental Variables (Version 3). arXiv. Link (rep), PDF, Google。\n\n\n\n在特殊的论文情景下，实现快速搜索工具变量；\n与大语言模型互动，可以在自己的研究领域发现更多新的工具变量；\n利用大语言模型系统搜索工具变量，可以提高获取多个工具变量的可能性，有助于研究者通过过度识别检验来验证多工具变量的有效性；\n利用大语言模型找到更多的工具变量，有助于研究者提高给 IV 找到合适数据的机会，或者指导研究者通过其他算法或者实验找到工具变量所需的数据。",
    "crumbs": [
      "**寻找IV**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>如何借助 AI 寻找工具变量？</span>"
    ]
  },
  {
    "objectID": "body/08_findit_IV_example.html#对话思路",
    "href": "body/08_findit_IV_example.html#对话思路",
    "title": "11  如何借助 AI 寻找工具变量？",
    "section": "11.2 对话思路",
    "text": "11.2 对话思路\n\n完整讨论过程：ChatGPT\n\n\n\n\n\n\n\n提示词思路\n\n\n\n\n简要介绍了我的问题背景，询问 AI 我研究的问题中是存在哪些可能的内生性问题；\n让 AI 帮我分析这些内生性问题的可能来源，并分析机制；\n询问「遗漏变量」偏误应该如何应对？\n聚焦到 IV 上，让 AI 提供 5 个备选的 IV，并论证其合理性 (相关性、外生性和排他性)；\n逐一论证每个 IV 的合理性 (从外生性、相关性和排他性三个角度分析)，并给出可能的后门路径；\n模型设定：为了关闭后门路径，询问 AI 我需要控制哪些变量；\n可行性：询问 AI 工具变量的具体定义，数据来源等；\n让 AI 对所有 IV 进行综合评价，给出推荐的 IV。",
    "crumbs": [
      "**寻找IV**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>如何借助 AI 寻找工具变量？</span>"
    ]
  },
  {
    "objectID": "body/08_findit_IV_example.html#主要提示词",
    "href": "body/08_findit_IV_example.html#主要提示词",
    "title": "11  如何借助 AI 寻找工具变量？",
    "section": "11.3 主要提示词",
    "text": "11.3 主要提示词\n\n完整讨论过程：ChatGPT\n\n\n\n\n\n\n\n提示词\n\n\n\n\nPrompt 01:\n我正在做一篇实证分析的文章。我研究的问题是政府引导基金能否降低产业链上的长鞭效应。解释变量是一个虚拟变量，如果一家公司受到了政府引导基金的资助，对应的数值取1，否则的话取0。背景式变量是文献里面常用的用来衡量产业链上长边效应的指标。\n我研究的这个问题是否存在一些潜在的内生性的问题，有哪几种可能的来源？\n\n\nPrompt 02:\n对于 2. 遗漏变量 (Omitted Variable Bias)，我有哪些可以采用的实证研究方法？\n\n\nPrompt 03:\n能否具体讨论一下，在我的研究背景下，可能遗漏了哪些重要的变量？遗漏问题如何影响我的统计推断？\n\n\nPrompt 04:\n如果我想采用 IV 估计，能否提供 5 个备选的 IV，并论证其合理性 (相关性、外生性和排他性)。最好有明确的理论和文献依据。\n\n\nPrompt 05:\n如果使用 「地方政府财政压力 (比如，用地方政府财政赤字率或债务水平来衡量)」作为 IV 是否可以？\n\n\nPrompt 06:\n财政压力是否会通过其他渠道影响长鞭效应？这些变量是否需要控制？\n\n\nPrompt 07:\n你分析的很好，但我无法在论文中控制太多的变量。如果只选择三个变量来控制，以满足排他性要求，我应该选择哪三个？我在正文中是否需要绘制一个 DAG 图形来辅助论述？\n\n\nPrompt 08:\n这些变量都是宏观层面的变量 (我的基本数据是 firm-year 层面的)，这就意味着我无法在模型中加入 i.year 了，但这可能导致我遗漏了一些不可观察的宏观层面的时序变量。我该如何应对？\n\n\nPrompt 09:\n很好，你把咱们的讨论整合一下，写一个详细的研究笔记： 1. 问题背景 2. 内生性问题分析 3. 工具变量的选择和讨论 4. IV-地方财政压力：选择依据、合理性分析、……\n\n\nPrompt 10:\n可以写的更详细一些，把我们之前讨论过程中很多细节写进来\n\n\nPrompt :\n\n我们现在讨论一下使用「地方政府财政赤字率」和「地方债务水平」做工具变量的合理性。\n\n分别从外生性、相关性、排他性三个角度进行分析。\n对于排他性，分析三种潜在的渠道(W1, W2, W3)，以便我在模型中选择合理的控制变量以便关闭这些潜在的后门路径。Z –&gt; (W1 | W2 | W3) –&gt; Y。\n注意：W1-W3 这三个变量必须要有明确的定义，以便我能找到对应的数据，衡量这些变量。",
    "crumbs": [
      "**寻找IV**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>如何借助 AI 寻找工具变量？</span>"
    ]
  },
  {
    "objectID": "body/08_findit_IV_example.html#例子地方政府财政赤字率-作为工具变量的合理性论证",
    "href": "body/08_findit_IV_example.html#例子地方政府财政赤字率-作为工具变量的合理性论证",
    "title": "11  如何借助 AI 寻找工具变量？",
    "section": "11.4 例子：“地方政府财政赤字率” 作为工具变量的合理性论证",
    "text": "11.4 例子：“地方政府财政赤字率” 作为工具变量的合理性论证\n我对 AI 建议的「地方政府财政赤字率」和「地方债务水平」这两个工具变量比较感兴趣。然而，在正式使用之前，还需要对这两个变量作为工具变量的合理性分析，否则，我们在论文中是无法写清楚的。\n\n完整讨论过程：ChatGPT\n\n\n\n\n\n\n\n提示词：讨论工具变量的合理性\n\n\n\n\n我们现在讨论一下使用「地方政府财政赤字率」和「地方债务水平」做工具变量的合理性。 1. 分别从外生性、相关性、排他性三个角度进行分析。\n2. 对于排他性，分析三种潜在的渠道(W1, W2, W3)，以便我在模型中选择合理的控制变量以便关闭这些潜在的后门路径。Z –&gt; (W1 | W2 | W3) –&gt; Y。注意：W1-W3 这三个变量必须要有明确的定义，以便我能找到对应的数据，衡量这些变量。\n\n\n\n\n11.4.1 AI 的回答\n\n\n\n\n\n\nChatGPT 的回答\n\n\n\n下面详细分析使用「地方政府财政赤字率 (Deficit Ratio)」与「地方债务水平 (Debt Level)」作为工具变量（IV）的合理性，包括从外生性、相关性和排他性三个维度的分析，以及排他性可能受到破坏的三种潜在渠道（W1, W2, W3），并明确给出这些渠道变量的定义与度量方法。\n\n\n\n点击查看详情",
    "crumbs": [
      "**寻找IV**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>如何借助 AI 寻找工具变量？</span>"
    ]
  },
  {
    "objectID": "body/08_findit_IV_example_Q8.html",
    "href": "body/08_findit_IV_example_Q8.html",
    "title": "12  工具变量合理性分析",
    "section": "",
    "text": "12.1 三个分析要点\n在上一章中 (如何借助 AI 寻找工具变量？)，我介绍了一个例子，AI 推荐了「地方政府财政赤字率」和「地方债务水平」作为工具变量。\n在使用这两个变量之前，我们需要对它们作为工具变量的合理性进行分析。以下是 AI 对这两个工具变量的外生性、相关性和排他性三个维度的分析，以及可能存在的后门路径。",
    "crumbs": [
      "**寻找IV**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>工具变量合理性分析</span>"
    ]
  },
  {
    "objectID": "body/08_findit_IV_example_Q8.html#三个分析要点",
    "href": "body/08_findit_IV_example_Q8.html#三个分析要点",
    "title": "12  工具变量合理性分析",
    "section": "",
    "text": "12.1.1 外生性分析（Exogeneity）\n地方政府财政赤字率与债务水平主要取决于：\n\n地方宏观经济状况（如经济周期波动、产业结构调整）。\n地方政府财政支出管理政策（如财政预算、债务发行计划）。\n\n这些因素通常由地区经济整体状况和政策环境决定，个别企业的供应链管理行为或长鞭效应很难直接影响地方政府的财政赤字率或债务水平。\n因此，在充分控制地区、行业和企业层面特征后，这两个变量作为工具变量具有较好的外生性。\n\n\n12.1.2 相关性分析（Relevance）\n地方政府的财政赤字率和债务水平直接影响地方政府用于产业扶持政策的预算规模与意愿：\n\n财政赤字率或债务水平较高时，地方政府可能迫于经济增长或财政压力的需要，更积极地投放政府引导基金，以刺激经济增长、稳定就业或扩大税基；\n财政健康状况好的地区，政府可能更倾向于市场化运作，对引导基金的投放力度或需求可能相对较低。\n\n因此，这两个指标在逻辑上和实践中均可能显著地决定政府引导基金的投放决策，满足相关性条件。\n\n\n12.1.3 排他性分析（Exclusion Restriction）\n排他性要求工具变量仅通过影响政府引导基金（\\(Z \\rightarrow D\\)）来影响长鞭效应（\\(Y\\)），不存在其他直接渠道（\\(Z \\rightarrow Y\\)）。\n然而，这一假设可能存在潜在问题，特别是财政赤字率或债务水平可能会通过其他渠道直接影响企业供应链的波动性（长鞭效应）。这些潜在渠道需被明确识别和控制，以满足工具变量的排他性。",
    "crumbs": [
      "**寻找IV**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>工具变量合理性分析</span>"
    ]
  },
  {
    "objectID": "body/08_findit_IV_example_Q8.html#不满足排他性的三种潜在渠道",
    "href": "body/08_findit_IV_example_Q8.html#不满足排他性的三种潜在渠道",
    "title": "12  工具变量合理性分析",
    "section": "12.2 不满足排他性的三种潜在渠道",
    "text": "12.2 不满足排他性的三种潜在渠道\n下面详细分析三个最重要的可能渠道（\\(W_1, W_2, W_3\\)）。财政压力（赤字或债务）可能通过以下三个潜在渠道 (\\(W\\)) 直接或间接影响企业供应链稳定性：\n\n12.2.1 W1：基础设施投资（Infrastructure Investment）\n\n渠道机制： 地方政府财政压力增加时，可能削减基础设施投资预算（如交通、物流、通信基础设施），导致企业物流成本增加、供应链效率下降，从而加剧长鞭效应。\n明确定义与度量方式：\n\n基础设施投资额：地方政府年度基础设施投资总额（亿元）。\n基础设施投资强度：基础设施投资额占地区 GDP 比重（%）。\n\n数据来源推荐：\n\n中国统计年鉴（地区基础设施建设投资）。\nWind 数据库（地区固定资产投资完成额）。\n\n\n\n\n12.2.2 W2：政府干预（Government Intervention）\n\n渠道机制： 财政压力较大的地区，政府可能通过行政手段干预企业生产经营活动，以稳定地方经济增长、保就业，从而直接影响企业的供应链管理策略和库存决策，导致长鞭效应变化。\n明确定义与度量方式：\n\n政府补贴强度：地方政府对辖区内企业的财政补贴总额占 GDP 的比重（%）。\n政府与市场关系指数：可用中国市场化指数中的政府干预子指数。\n\n数据来源推荐：\n\n地方财政预算执行报告（政府补贴支出数据）。\n王小鲁《中国分省份市场化指数》中的政府干预指数。\n\n\n\n\n12.2.3 W3：融资约束（Financing Constraints）\n\n渠道机制： 地方财政压力高时，政府可能挤占金融市场资源（如增加地方债发行规模），导致企业信贷获取成本上升或融资难度增加，企业流动资金短缺，从而影响企业的库存管理、采购策略，进一步引发或加剧长鞭效应。\n明确定义与度量方式：\n\n企业融资约束指标（SA-index）：基于上市公司财务数据计算的融资约束程度指标。\n地区信贷资源紧张程度：金融机构人民币贷款余额增长率或中小企业贷款比重。\n\n数据来源推荐：\n\nCSMAR、Wind 数据库（企业 SA-index 指标）。\n地方金融运行报告（地区信贷数据、中国人民银行各分行统计报告）。",
    "crumbs": [
      "**寻找IV**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>工具变量合理性分析</span>"
    ]
  },
  {
    "objectID": "body/08_findit_IV_example_Q8.html#如何关闭这些潜在渠道",
    "href": "body/08_findit_IV_example_Q8.html#如何关闭这些潜在渠道",
    "title": "12  工具变量合理性分析",
    "section": "12.3 如何关闭这些潜在渠道？",
    "text": "12.3 如何关闭这些潜在渠道？\n为满足 IV 的排他性假设，应在实证模型中明确控制上述三个变量，以关闭财政赤字率或债务水平对企业长鞭效应的潜在直接影响路径。\n\n12.3.1 推荐控制变量（精简后的三选方案）：\n\n\n\n\n\n\n\n\n变量类型 (W)\n推荐控制变量\n具体数据来源\n\n\n\n\nW1 基础设施投资\n基础设施投资强度（基础设施投资额/GDP）\n中国统计年鉴、Wind\n\n\nW2 政府干预\n政府补贴强度（地方政府补贴额/GDP）或 政府与市场关系指数\n地方财政报告、市场化指数\n\n\nW3 融资约束\n企业融资约束指标（SA-index）或地区信贷紧张程度（人民币贷款余额增长率）\nCSMAR、Wind、央行分行数据\n\n\n\n\n\n12.3.2 图示（因果路径与控制变量）\n通过 DAG 图进一步明确因果链条及潜在的后门路径：\n```mermaid\ngraph LR\n    Z[**Z** - 财政赤字率/债务水平] --&gt; D[**X** - 政府引导基金资助]\n    D --&gt; Y[**Y** - 长鞭效应]\n\n    Z --&gt; W1[W1-基础设施投资] --&gt; Y\n    Z --&gt; W2[W2-政府干预强度] --&gt; Y\n    Z --&gt; W3[W3-企业融资约束] --&gt; Y\n```\n\n\n\nIV1-财政赤字-政府债务水平-mermind\n\n\n说明：\n\n主路径：财政压力 → 政府引导基金 → 长鞭效应（本文所要识别的核心路径）。\n潜在替代路径（需要关闭）：\n\n财政压力 → 基础设施投资 → 长鞭效应\n财政压力 → 政府干预 → 长鞭效应\n财政压力 → 企业融资约束 → 长鞭效应\n\n\n通过在回归模型中同时控制 W1、W2、W3 三个变量，可以有效关闭上述后门路径，从而满足排他性要求。",
    "crumbs": [
      "**寻找IV**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>工具变量合理性分析</span>"
    ]
  },
  {
    "objectID": "body/08_findit_IV_example_Q8.html#小结与实施建议",
    "href": "body/08_findit_IV_example_Q8.html#小结与实施建议",
    "title": "12  工具变量合理性分析",
    "section": "12.4 小结与实施建议",
    "text": "12.4 小结与实施建议\n工具变量整体评价：\n\n\n\n工具变量\n外生性\n相关性\n排他性（控制W1-W3后）\n\n\n\n\n地方财政赤字率\n强\n强\n较强（需控制 W1-W3）\n\n\n地方债务水平\n强\n强\n较强（需控制 W1-W3）\n\n\n\n具体实施建议：\n\n在实证回归模型中纳入明确定义和可观测的 W1、W2 和 W3 变量；\n进行 IV 第一阶段回归，确保工具变量的强相关性；\n在 IV 第二阶段回归后，实施稳健性检验，包括不同 W 变量的组合方式，检验估计结果的稳健性；\n结合 DAG 图在论文正文中展示识别策略，提升实证分析的严谨性与说服力。\n\n以上分析为工具变量合理性的深入论证，后续实证分析可在此基础上展开。",
    "crumbs": [
      "**寻找IV**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>工具变量合理性分析</span>"
    ]
  },
  {
    "objectID": "examples/learn_Tobit_model.html",
    "href": "examples/learn_Tobit_model.html",
    "title": "13  使用 AI 解读 Tobit 模型",
    "section": "",
    "text": "13.1 简介\n在学习计量经济学的过程中，许多学生常常会遇到复杂的模型和晦涩的数学推导，感到无从下手。由于缺乏合适的老师或同伴指导，理解这些内容变得更加困难。\nAI 助手的出现，为学生们提供了全新的学习方式。通过 AI，学生不仅可以获得模型原理、数学表述和经济学含义的详细讲解，还能根据自己的需求生成 Stata 或 Python 代码、图形和 LaTeX 公式。\n更重要的是，AI 能用讲义风格的文字和丰富的代码注释，帮助学生逐步掌握知识点。无论你的基础如何，AI 都可以用通俗易懂的语言，甚至模拟经验丰富的教授，帮助你找到最适合自己的学习路径。\n本节的目的是通过 AI 助手，帮助学生理解 Tobit 模型的原理、数学表述和经济学含义。我们将展示如何使用 AI 生成相关内容，并提供实际的代码示例和图形展示。\n我主要受到了 Korinek (2023) 的启发。为了解读 Solow 模型的原理 (Chat 50)，Korinek 向 AI 发送了提示词 Simulate the Solow growth model and plot the results in a four chart.。AI 的解读效果非常好。",
    "crumbs": [
      "**学计量**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>使用 AI 解读 Tobit 模型</span>"
    ]
  },
  {
    "objectID": "examples/learn_Tobit_model.html#模拟-tobit-modelpython-代码",
    "href": "examples/learn_Tobit_model.html#模拟-tobit-modelpython-代码",
    "title": "13  使用 AI 解读 Tobit 模型",
    "section": "13.2 模拟 Tobit Model：Python 代码",
    "text": "13.2 模拟 Tobit Model：Python 代码\n\n\n\n\n\n\n提示词\n\n\n\n\n任务：写一段 {Python} 代码，模拟 Tobit Model 的数据，并酌情绘图。\n\nN = 500；\n支持中文字符显示\n屏蔽 warning 信息\n\n目的：帮助初学者理解 Tobit Model 的数据生成过程。\n风格：讲义风格，核心代码要添加中文注释\n\n\n\n\nimport warnings\nimport numpy as np\n\nwarnings.filterwarnings('ignore')  # 屏蔽所有 warning 信息\n\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文字体为黑体\nplt.rcParams['axes.unicode_minus'] = False    # 正常显示负号\n\n# 设置随机种子，保证结果可复现\nnp.random.seed(42)\n\n# 样本量\nN = 500\n\n# 生成自变量 X，服从标准正态分布\nX = np.random.normal(0, 1, N)\n\n# 设置 Tobit 模型的参数\nbeta_0 = 1.0   # 截距\nbeta_1 = 2.0   # 斜率\nsigma = 1.0    # 残差标准差\n\n# 生成潜在因变量 y*（未截断前的真实值）\ny_star = beta_0 + beta_1 * X + np.random.normal(0, sigma, N)\n\n# 设置截断点（左截断为0）\ny = np.maximum(y_star, 0)\n\n# 绘图：展示 Tobit 数据的分布\nplt.figure(figsize=(10, 5))\n\n# 散点图：X 与 y* 的关系\nplt.subplot(1, 2, 1)\nplt.scatter(X, y_star, alpha=0.5, label='y*（未截断）')\nplt.axhline(0, color='red', linestyle='--', label='截断点')\nplt.title('X 与 y*（未截断）')\nplt.xlabel('X')\nplt.ylabel('y*')\nplt.legend()\n\n# 散点图：X 与 y 的关系\nplt.subplot(1, 2, 2)\nplt.scatter(X, y, alpha=0.5, label='y（截断后）')\nplt.axhline(0, color='red', linestyle='--', label='截断点')\nplt.title('X 与 y（截断后）')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "**学计量**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>使用 AI 解读 Tobit 模型</span>"
    ]
  },
  {
    "objectID": "examples/learn_Tobit_model.html#模型设定和基本原理",
    "href": "examples/learn_Tobit_model.html#模型设定和基本原理",
    "title": "13  使用 AI 解读 Tobit 模型",
    "section": "13.3 模型设定和基本原理",
    "text": "13.3 模型设定和基本原理\n\n\n\n\n\n\n提示词\n\n\n\n写一段讲义风格的文字，用 LaTeX 公式描述 Tobit Model 的模型设定和基本原理。\n然后结合上面的代码和图形解释公式的经济学含义，最好能结合一些容易理解的实例。\n\n\nTobit 模型是一种用于处理因变量存在截断（如左截断为 0）的回归模型。其基本思想是：我们观测到的因变量 \\(y\\)，是一个潜在的连续变量 \\(y^*\\) 被某个阈值截断后的结果。\n\n13.3.1 Tobit 模型的数学表述\nTobit 模型可表示为：\n\\[\n\\begin{align*}\ny_i^* &= \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0, \\sigma^2) \\\\\ny_i &= \\begin{cases}\ny_i^*, & \\text{如果 } y_i^* &gt; 0 \\\\\n0,    & \\text{如果 } y_i^* \\leq 0\n\\end{cases}\n\\end{align*}\n\\]\n其中：\n\n\\(y_i^*\\)：不可观测的潜在因变量（如“理想消费”）\n\\(y_i\\)：实际观测到的因变量（如“实际消费”，不能为负）\n\\(x_i\\)：自变量\n\\(\\beta_0, \\beta_1\\)：回归系数\n\\(\\varepsilon_i\\)：正态分布的误差项\n\n\n\n13.3.2 经济学含义与实例\n以家庭消费为例，\\(y_i^*\\) 可以理解为家庭的“理想消费金额”，但由于某些家庭可能没有消费（如收入太低），实际观测到的消费 \\(y_i\\) 就会被截断为 0。\n在上面的代码中，我们模拟了 \\(N=500\\) 个样本，生成了 \\(y^*\\) 和 \\(y\\)。图形左侧展示了 \\(X\\) 与 \\(y^*\\) 的关系，可以看到 \\(y^*\\) 是连续分布的；右侧展示了 \\(X\\) 与 \\(y\\) 的关系，所有 \\(y^* \\leq 0\\) 的点都被截断为 0，形成了“堆积”。\n这种模型广泛用于：\n\n消费/投资为零的情况（如部分家庭没有购房支出）\n信贷申请/批准金额（如部分人未获批贷款）\n劳动供给（如部分人未参与劳动市场）\n\nTobit 模型能有效处理这类“部分观测为零”的数据，避免 OLS 回归的偏误。",
    "crumbs": [
      "**学计量**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>使用 AI 解读 Tobit 模型</span>"
    ]
  },
  {
    "objectID": "examples/learn_Tobit_model.html#tobit-模型的详细数学推导",
    "href": "examples/learn_Tobit_model.html#tobit-模型的详细数学推导",
    "title": "13  使用 AI 解读 Tobit 模型",
    "section": "13.4 Tobit 模型的详细数学推导",
    "text": "13.4 Tobit 模型的详细数学推导\n其实，仅仅依据上面的内容，你还是无法真正理解 Tobit 模型的数学公式和经济学含义。下面我们将详细推导 Tobit 模型的数学公式，并解释每一步的经济学含义。\n我列举了一些提示词要点，你可以酌情选择使用。建议一次性不要给 AI 太多提示词，分步进行更容易获得满意的结果。\n\n\n\n\n\n\n提示词\n\n\n\n写一段讲义风格的文字，详细推导 Tobit 模型的数学公式。包括：\n\n潜在变量 \\(y^*\\) 的定义\n截断后的观测变量 \\(y\\) 的定义\n期望值和条件期望值的推导\nTobit 和 OLS 估计式的关系\nTobit 模型的最大似然估计方法\nTobit 模型边际效应的计算方法和经济含义\n\n\n\n\n13.4.1 潜在变量 \\(y^*\\) 的定义\nTobit 模型假设存在一个不可观测的潜在连续变量 \\(y^*\\)，其生成机制为：\n\\[\ny_i^* = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0, \\sigma^2)\n\\]\n其中，\\(x_i\\) 是自变量，\\(\\beta_0, \\beta_1\\) 为参数，\\(\\varepsilon_i\\) 为正态分布误差项。\n我们实际观测到的因变量 \\(y_i\\) 受到截断（如左截断为 0），数据生成过程为：\n\\[\ny_i = \\begin{cases}\ny_i^*, & \\text{如果 } y_i^* &gt; 0 \\\\\n0, & \\text{如果 } y_i^* \\leq 0\n\\end{cases}\n\\]\n\n\n13.4.2 \\(y\\) 的期望值和条件期望值\nTobit 模型下，\\(y_i\\) 的条件期望为：\n\\[\nE[y_i|x_i] = P(y_i^* &gt; 0|x_i) \\cdot E[y_i^*|y_i^* &gt; 0, x_i]\n\\]\n其体推导如下：\n\n\\(P(y_i^* &gt; 0|x_i) = 1 - \\Phi\\left(-\\frac{\\beta_0 + \\beta_1 x_i}{\\sigma}\\right)\\)，其中 \\(\\Phi(\\cdot)\\) 为标准正态分布函数。\n\\(E[y_i^*|y_i^* &gt; 0, x_i] = \\beta_0 + \\beta_1 x_i + \\sigma \\frac{\\varphi\\left(\\frac{\\beta_0 + \\beta_1 x_i}{\\sigma}\\right)}{1 - \\Phi\\left(-\\frac{\\beta_0 + \\beta_1 x_i}{\\sigma}\\right)}\\)，其中 \\(\\varphi(\\cdot)\\) 为标准正态密度函数。\n\n因此：\n\\[\nE[y_i|x_i] = \\left[1 - \\Phi\\left(-\\frac{\\beta_0 + \\beta_1 x_i}{\\sigma}\\right)\\right] \\left[\\beta_0 + \\beta_1 x_i + \\sigma \\frac{\\varphi\\left(\\frac{\\beta_0 + \\beta_1 x_i}{\\sigma}\\right)}{1 - \\Phi\\left(-\\frac{\\beta_0 + \\beta_1 x_i}{\\sigma}\\right)}\\right]\n\\]\n\n\n13.4.3 Tobit 和 OLS 估计式的关系\nOLS 直接对 \\(y\\) 回归会低估 \\(\\beta_1\\)，因为截断导致大量 \\(y=0\\) 的观测值，拉低了回归斜率。Tobit 模型通过最大似然法，利用了 \\(y=0\\) 的概率信息，能更准确估计参数。\n\n\n13.4.4 模型的最大似然估计方法\nTobit 模型的对数似然函数为，\n\\[\n\\ell(\\beta_0, \\beta_1, \\sigma) = \\sum_{y_i=0} \\log \\Phi\\left(\\frac{-(\\beta_0 + \\beta_1 x_i)}{\\sigma}\\right) + \\sum_{y_i&gt;0} \\log \\left[\\frac{1}{\\sigma} \\varphi\\left(\\frac{y_i - \\beta_0 - \\beta_1 x_i}{\\sigma}\\right)\\right]\n\\]\n具中，第一项对应被截断的观测，第二项对应未被截断的观测。通过极大化该似然函数，得到参数估计值。\n有关 MLE 估计方法介绍，参见如下推文：\n\n徐云娇, 连玉君, 2020, Stata：数值求解极大值及 MLE 示例.\n曹昊煜, 2022, Stata：最大似然估计入门教程(MLE)-ml.\n王萃芳, 2022, Stata：最大似然估计(MLE)简易教程.\n\n\n\n13.4.5 Tobit 模型边际效应的计算方法和经济含义\nTobit 模型的系数 \\(\\beta_1\\) 并不是 \\(x\\) 对 \\(y\\) 的直接边际效应。\\(x\\) 对 \\(E[y|x]\\) 的边际效应为：\n\\[\n\\frac{\\partial E[y|x]}{\\partial x} = \\beta_1 \\cdot \\Phi\\left(\\frac{\\beta_0 + \\beta_1 x}{\\sigma}\\right)\n\\]\n其表示 \\(x\\) 的影响被截断概率所“缩小”，只有未被截断的部分才体现出 \\(x\\) 的真实效应。经济学上，这反映了自变量对“实际观测到的因变量”的平均影响。",
    "crumbs": [
      "**学计量**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>使用 AI 解读 Tobit 模型</span>"
    ]
  },
  {
    "objectID": "examples/learn_Tobit_model.html#tobit-和-ols-对比",
    "href": "examples/learn_Tobit_model.html#tobit-和-ols-对比",
    "title": "13  使用 AI 解读 Tobit 模型",
    "section": "13.5 Tobit 和 OLS 对比",
    "text": "13.5 Tobit 和 OLS 对比\n你还可以进步借助 AI 生成 Tobit 和 OLS 模型的估计结果图示，以便说明 OLS 估计可能存在的偏差。\n这里用了教育学中一个非常重要的原理：知识迁移。在学习 Tobit 模型过程中，你可以借助对比，将已经掌握的 OLS 模型知识迁移到新的 Tobit 模型上。\n\n\n\n\n\n\n提示词\n\n\n\n\n目的：编写 Python 代码，图示 Tobit 和 OLS 模型的估计结果，以便说明 OLS 估计可能存在的偏差\n风格：讲义风格，核心代码要添加中文注释\n代码要求：\n\n要能独立运行，不依赖与此前代码块的设定\n支持中文字符显示；屏蔽 warning 信息",
    "crumbs": [
      "**学计量**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>使用 AI 解读 Tobit 模型</span>"
    ]
  },
  {
    "objectID": "examples/learn_Tobit_model.html#后续讨论",
    "href": "examples/learn_Tobit_model.html#后续讨论",
    "title": "13  使用 AI 解读 Tobit 模型",
    "section": "13.6 后续讨论",
    "text": "13.6 后续讨论\n按照上述逻辑，你可以不断追问 AI，生成更多内容。这种学习模式与传统的课堂教学和硬啃教科书有很大区别。你完全可以根据自己的基础和需求，调整 AI 的提示词，让其生成最适合你的内容。\n比如，你可以使用如下提示词追问：\n\n\n\n\n\n\n提示词\n\n\n\n\nTobit 模型如何估计？\n如何解释 Tobit 模型的系数含义？边际效应如何计算？\n有哪些 Tobit 模型的拓展模型？\n在因果推断中，有哪些与 Tobit 模型或者其模型设定思想相关的模型？它们的适用场景是什么？\n对于面板数据而言，有哪些主要的 Tobit 模型？\n帮我推荐 3-5 篇公司金融领域使用 Tobit 或相关模型进行实证分析的论文。要求：近 10 年发表的；仅限于 JFE, JF, RFS, JFQA, JCF, RFS 等顶级期刊；最好是有数据和代码的。\n\n\n\n\nChatGPT 对话过程",
    "crumbs": [
      "**学计量**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>使用 AI 解读 Tobit 模型</span>"
    ]
  },
  {
    "objectID": "examples/learn_Tobit_model.html#参考文献",
    "href": "examples/learn_Tobit_model.html#参考文献",
    "title": "13  使用 AI 解读 Tobit 模型",
    "section": "13.7 参考文献",
    "text": "13.7 参考文献\n\nKorinek, A. (2023). Generative AI for Economic Research: Use Cases and Implications for Economists. Journal of Economic Literature, 61(4), 1281–1317. Link (rep), PDF, Appendix, Google.\n\n更新 1：2024 版全文更新\n更新 2：提供了很多 Prompts",
    "crumbs": [
      "**学计量**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>使用 AI 解读 Tobit 模型</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/00_intro.html",
    "href": "examples/CSMAR/00_intro.html",
    "title": "14  数据分析案例：CAMAR 数据库合并",
    "section": "",
    "text": "14.1 多个 zip 文件的合并\n这个 Jupyter Notebook 文件演示了如何将多个 zip 文件中的 CSV 数据合并成一个大的数据集。我们将使用 Python 的 pandas 库来处理数据。在进行这个任务是，我刚接触 Python 一周，只了解一些基本概念，没有任何实操经验。如果不借助 AI，我甚至难以写出两行正确的代码。\n不过，好在有 Copilot 的帮助，我只需要写清楚提示词，它基本上能够一次性帮我编写好代码。期间很少出现 bugs。当然，很多时候由于我对数据特征了解不够，往往无法准确表述我的需求，这时我会通过多次迭代来完善提示词，直到得到满意的结果。\n需要注意的是，在这个过程中，我很少会动手修改 Copilot 生成的代码。我的主要精力集中在修改和优化提示词上，我会不断把更新后的提示词发给 Copilot，让它生成新的代码。运行后，如果不符合要求，或我有新的数据处理需求，我会再次调整提示词，直到得到满意的结果。",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>数据分析案例：CAMAR 数据库合并</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/00_intro.html#多个-zip-文件的合并",
    "href": "examples/CSMAR/00_intro.html#多个-zip-文件的合并",
    "title": "14  数据分析案例：CAMAR 数据库合并",
    "section": "",
    "text": "01_unzip_to_csv 文件\n\n\n\n\n\n14.1.1 任务概况\n原始数据文件是从 CSMAR 数据库下载的，包含多个 zip 文件：\n\n我们需要将这些 zip 文件自动解压成 CSV 文件，并合并成一个大的数据集。",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>数据分析案例：CAMAR 数据库合并</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/01_unzip_to_csv.html",
    "href": "examples/CSMAR/01_unzip_to_csv.html",
    "title": "15  CSMAR 数据处理",
    "section": "",
    "text": "15.1 从 CAMAR 数据库下载 Excel 数据文件\nCSMAR (China Stock Market & Accounting Research) 是一个提供中国上市公司数据的数据库。本文将介绍如何使用 CSMAR 数据库获取上市公司基本信息和财务资料，并经过合并、清洗等处理，得到清洁数据。",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>CSMAR 数据处理</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/01_unzip_to_csv.html#从-camar-数据库下载-excel-数据文件",
    "href": "examples/CSMAR/01_unzip_to_csv.html#从-camar-数据库下载-excel-数据文件",
    "title": "15  CSMAR 数据处理",
    "section": "",
    "text": ".csv 优于 .xlsx\n\n\n\n我此前对 Python 处理数据的机制了解不足，导致我最初下载的都是 .xlsx 格式的数据。虽然这种 Excel 格式的数据可以使用 pandas 读取，但非常耗时 (一份包含 8w 行观察值的资产负债表大概需要 50s)。相比之下，.csv 格式的数据读取速度更快，通常只需要几秒钟。\n因此，大家从 CSMAR 数据库下载数据时，建议选择 .csv 格式，而不是 .xlsx 格式。\n\n\n\n15.1.1 数据下载页面\n\n网址：https://data.csmar.com/\n登录：中大 IP 地址范围内自动登录 (机构账号)\n\n\n主要有两种查询方案：\n\n单表查询：直接查询某一张表格的数据。\n多表查询：可以跨表查询，形成一个新的表格。\n\n两种模式下的查询流程大致相同：选择子库 &gt;&gt; 选择时间范围 &gt;&gt; 选择代码范围 &gt;&gt; 选择字段范围 &gt;&gt; 下载数据。\n建议尽量选择 .csv 或 .txt 格式下载数据，因为这两种格式的数据读取速度更快。\n下载后的文件通常为 .zip 格式，解压后会得到两份文件： - FileName.csv: 数据文件 - FileName[DES][xlsx].txt: 变量说明文件\n数据库说明书：每个子库的右上角都会显示「数据库说明书」，点击后可下载 PDF 格式的说明书，里面包含了该子库的所有表格、字段的详细信息。\n\n\n# 文件路径设定{tag}\n# Note：进行后续分析之前，请先执行本 Cell 中的代码，以确保所有路径正确设置。\n\nimport os\nimport sys\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# 打开 .ipynb 文件的路径记为当前工作目录\n#path = os.getcwd()\npath = r'D:\\Github\\ds_data\\data\\CSMAR'    # 替换为你的实际路径\nos.chdir(path)  # 切换工作目录\nprint('Working Directory:', os.getcwd())\n\n\n# Folders \nzip_folder = os.path.join(path, 'data_raw_zip')       # 原始压缩包存放目录\nextract_folder = os.path.join(path, 'data_raw')       # 解压后的数据存放目录\ndata_clean_folder = os.path.join(path, 'data_clean')  # 清洗后的数据存放目录\n\nWorking Directory: d:\\Github\\ds_data\\data\\CSMAR\n\n\n\n# 列示 'data_raw_zip' 文件夹中的所有文件\nfor item in os.listdir(zip_folder):\n    print(item)\n\nCSMAR常用变量-2000-2024.zip\n上市公司基本信息变更表2000-2024.zip\n上市公司基本信息年度表.zip\n利润表-现金流量表-2000-2010.zip\n利润表-现金流量表-2011-2024.zip\n资产负债表-2000-2010.zip\n资产负债表-2011-2024.zip",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>CSMAR 数据处理</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/01_unzip_to_csv.html#文件夹结构与处理流程说明",
    "href": "examples/CSMAR/01_unzip_to_csv.html#文件夹结构与处理流程说明",
    "title": "15  CSMAR 数据处理",
    "section": "15.2 文件夹结构与处理流程说明",
    "text": "15.2 文件夹结构与处理流程说明\n本项目的数据处理涉及以下文件夹：\n\ndata_raw_zip：存放从 CSMAR 下载的原始数据（.zip 压缩包），每个压缩包解压后会生成同名文件夹。\ndata_raw：存放解压后的原始数据文件。\ndata_clean：存放清洗后的数据文件。\n\n\n15.2.1 处理流程\n\n列出 data_raw_zip 文件夹中的所有压缩文件。\n将每个压缩包解压到 data_raw 文件夹下，解压后以压缩包同名文件夹存放。\n为便于后续批量处理，对解压后的文件进行统一重命名。\n编写函数，实现指定文件夹下文件的批量读取。",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>CSMAR 数据处理</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/01_unzip_to_csv.html#解压-zip-文件到指定文件夹",
    "href": "examples/CSMAR/01_unzip_to_csv.html#解压-zip-文件到指定文件夹",
    "title": "15  CSMAR 数据处理",
    "section": "15.3 解压 zip 文件到指定文件夹",
    "text": "15.3 解压 zip 文件到指定文件夹\n\n\n\n\n\n\n提示词\n\n\n\n\n将当前路径下的 ‘data_raw_zip’ 文件夹中的所有 zip 文件解压到 ‘data_raw’ 文件夹中。\n如果 ‘data_raw’ 文件夹不存在，则创建该文件夹。\n每个 zip 文件解压后生成一个文件夹，文件夹名称与 zip 文件名相同，若有同名文件则覆盖之。\n\n\n\n\nimport zipfile\nimport shutil\n\n# 创建 'data_raw' 文件夹（如果不存在）\nif not os.path.exists(extract_folder):\n    os.makedirs(extract_folder)\n\n# 遍历 'data_raw_zip' 文件夹中的所有 zip 文件\nfor item in os.listdir(zip_folder):\n    if item.endswith('.zip'):\n        zip_path = os.path.join(zip_folder, item)\n        folder_name = os.path.splitext(item)[0]\n        target_dir = os.path.join(extract_folder, folder_name)\n        # 如果目标文件夹已存在，则先删除\n        if os.path.exists(target_dir):\n            shutil.rmtree(target_dir)\n        # 解压 zip 文件到目标文件夹\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(target_dir)\n        print(f\"已解压: {zip_path} -&gt; {target_dir}\")\n\n已解压: D:\\Github\\ds_data\\data\\CSMAR\\data_raw_zip\\CSMAR常用变量-2000-2024.zip -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\CSMAR常用变量-2000-2024\n已解压: D:\\Github\\ds_data\\data\\CSMAR\\data_raw_zip\\上市公司基本信息变更表2000-2024.zip -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\上市公司基本信息变更表2000-2024\n已解压: D:\\Github\\ds_data\\data\\CSMAR\\data_raw_zip\\上市公司基本信息年度表.zip -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\上市公司基本信息年度表\n已解压: D:\\Github\\ds_data\\data\\CSMAR\\data_raw_zip\\利润表-现金流量表-2000-2010.zip -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\利润表-现金流量表-2000-2010\n已解压: D:\\Github\\ds_data\\data\\CSMAR\\data_raw_zip\\利润表-现金流量表-2011-2024.zip -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\利润表-现金流量表-2011-2024\n已解压: D:\\Github\\ds_data\\data\\CSMAR\\data_raw_zip\\资产负债表-2000-2010.zip -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2000-2010\n已解压: D:\\Github\\ds_data\\data\\CSMAR\\data_raw_zip\\资产负债表-2011-2024.zip -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2011-2024",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>CSMAR 数据处理</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/01_unzip_to_csv.html#更改部分文件夹中的文件名",
    "href": "examples/CSMAR/01_unzip_to_csv.html#更改部分文件夹中的文件名",
    "title": "15  CSMAR 数据处理",
    "section": "15.4 更改部分文件夹中的文件名",
    "text": "15.4 更改部分文件夹中的文件名\n\n15.4.1 问题说明\n在 data_raw 文件夹中，有些子文件夹中的文件名是由 CSMAR 自动生成的，没有实际意义。为了便于后续处理，需要将这些文件名更改为更有意义的名称。\n此处以 data_raw/资产负债表-?? 文件夹为例，可以看出，资产负债表-2000-2010 和 资产负债表-2011-2024 这两个文件夹中包含的文件名完全相同，这会导致随后纵向合并时难以区分数据来源。\n\n# 提示词：列出 `data_raw` 文件夹中包含关键词 {'资产负债表'} 的子文件夹的 file tree。\n\nkeywords = '资产负债表'\n\ndef print_keyword_file_tree(root, keyword, indent=\"\"):\n    for item in os.listdir(root):\n        item_path = os.path.join(root, item)\n        if os.path.isdir(item_path) and keyword in item:\n            print(indent + \"|-- \" + item)\n            for subitem in os.listdir(item_path):\n                print(indent + \"    |-- \" + subitem)\n\nprint_keyword_file_tree(extract_folder, keywords)\n\n|-- 资产负债表-2000-2010\n    |-- 版权声明.pdf\n    |-- 跨表查询_沪深京股票(年频).xlsx\n    |-- 跨表查询_沪深京股票(年频)[DES][.xlsx].txt\n|-- 资产负债表-2011-2024\n    |-- 版权声明.pdf\n    |-- 跨表查询_沪深京股票(年频).xlsx\n    |-- 跨表查询_沪深京股票(年频)[DES][.xlsx].txt\n\n\n查看后，发现子文件夹 利润表-现金流量表-?? 和 CSMAR常用变量-2000-2024 中的文件名也存在类似问题。\n处理思路是：用文件夹名称作为该文件夹下的文件的文件名。因此，我们可以使用如下提示词生成处理代码：\n\n\n\n\n\n\n提示词\n\n\n\n\n目的：更改 ‘data_raw’ 文件夹中部分子文件夹中的文件的名称。\n子文件夹名称为 ‘sub_folder_name’，其内部包含的文件记为 {Files}\n如果 ‘sub_folder_name’ 中包含关键词 {‘常用变量’, ‘资产负债表’, ‘利润表’}，则把 {Files} 中的 ‘.xlsx’ 和 ‘.txt’ 文件的名称改为 ‘folder_name.xlsx’ 和 ‘folder_name_DES.txt’。\n\n\n\n\n# 增加关键词 '常用变量'\nkeywords = ['常用变量', '资产负债表', '利润表']\n\n# 检查并重命名文件\nfor subfolder in os.listdir(extract_folder):\n    subfolder_path = os.path.join(extract_folder, subfolder)\n    if os.path.isdir(subfolder_path) and any(kw in subfolder for kw in keywords):\n        for file in os.listdir(subfolder_path):\n            file_path = os.path.join(subfolder_path, file)\n            if file.endswith('.xlsx'):\n                new_name = f\"{subfolder}.xlsx\"\n                new_path = os.path.join(subfolder_path, new_name)\n                os.rename(file_path, new_path)\n                print(f\"重命名: {file_path} -&gt; {new_path}\")\n            elif file.endswith('.txt'):\n                new_name = f\"{subfolder}_DES.txt\"\n                new_path = os.path.join(subfolder_path, new_name)\n                os.rename(file_path, new_path)\n                print(f\"重命名: {file_path} -&gt; {new_path}\")\n\n重命名: D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\CSMAR常用变量-2000-2024\\常用变量查询（年度）.xlsx -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\CSMAR常用变量-2000-2024\\CSMAR常用变量-2000-2024.xlsx\n重命名: D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\CSMAR常用变量-2000-2024\\常用变量查询（年度）[DES][xlsx].txt -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\CSMAR常用变量-2000-2024\\CSMAR常用变量-2000-2024_DES.txt\n重命名: D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\利润表-现金流量表-2000-2010\\跨表查询_沪深京股票(年频).xlsx -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\利润表-现金流量表-2000-2010\\利润表-现金流量表-2000-2010.xlsx\n重命名: D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\利润表-现金流量表-2000-2010\\跨表查询_沪深京股票(年频)[DES][.xlsx].txt -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\利润表-现金流量表-2000-2010\\利润表-现金流量表-2000-2010_DES.txt\n重命名: D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\利润表-现金流量表-2011-2024\\跨表查询_沪深京股票(年频).xlsx -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\利润表-现金流量表-2011-2024\\利润表-现金流量表-2011-2024.xlsx\n重命名: D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\利润表-现金流量表-2011-2024\\跨表查询_沪深京股票(年频)[DES][.xlsx].txt -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\利润表-现金流量表-2011-2024\\利润表-现金流量表-2011-2024_DES.txt\n重命名: D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2000-2010\\跨表查询_沪深京股票(年频).xlsx -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2000-2010\\资产负债表-2000-2010.xlsx\n重命名: D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2000-2010\\跨表查询_沪深京股票(年频)[DES][.xlsx].txt -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2000-2010\\资产负债表-2000-2010_DES.txt\n重命名: D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2011-2024\\跨表查询_沪深京股票(年频).xlsx -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2011-2024\\资产负债表-2011-2024.xlsx\n重命名: D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2011-2024\\跨表查询_沪深京股票(年频)[DES][.xlsx].txt -&gt; D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2011-2024\\资产负债表-2011-2024_DES.txt\n\n\n\n\n15.4.2 查看处理后的文件名\n\n# 提示词：列出 `data_raw` 文件夹中包含关键词 {'资产负债表'} 的子文件夹的 file tree。\n\nkeyword = '资产负债表'\n\ndef print_keyword_file_tree(root, keyword, indent=\"\"):\n    for item in os.listdir(root):\n        item_path = os.path.join(root, item)\n        if os.path.isdir(item_path) and keyword in item:\n            print(indent + \"|-- \" + item)\n            for subitem in os.listdir(item_path):\n                print(indent + \"    |-- \" + subitem)\n\nprint_keyword_file_tree(extract_folder, keyword)\n\n|-- 资产负债表-2000-2010\n    |-- 版权声明.pdf\n    |-- 资产负债表-2000-2010.xlsx\n    |-- 资产负债表-2000-2010_DES.txt\n|-- 资产负债表-2011-2024\n    |-- 版权声明.pdf\n    |-- 资产负债表-2011-2024.xlsx\n    |-- 资产负债表-2011-2024_DES.txt",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>CSMAR 数据处理</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/01_unzip_to_csv.html#列示-data_raw-文件夹中的-file-tree",
    "href": "examples/CSMAR/01_unzip_to_csv.html#列示-data_raw-文件夹中的-file-tree",
    "title": "15  CSMAR 数据处理",
    "section": "15.5 列示 ‘data_raw’ 文件夹中的 file tree",
    "text": "15.5 列示 ‘data_raw’ 文件夹中的 file tree\n\n\n\n\n\n\n提示词\n\n\n\n\n列示 ‘data_raw’ 文件夹中的文件树结构。\n只列示文件夹名称和文件名称，不需要显示文件内容。\n如果文件夹中有子文件夹，则显示子文件夹名称。\n\n\n\n\ndef print_file_tree(root, indent=\"\"):\n    for item in os.listdir(root):\n        item_path = os.path.join(root, item)\n        print(indent + \"|-- \" + item)\n        if os.path.isdir(item_path):\n            print_file_tree(item_path, indent + \"    \")\n\nprint_file_tree(extract_folder)\n\n|-- CSMAR常用变量-2000-2024\n    |-- CSMAR常用变量-2000-2024.xlsx\n    |-- CSMAR常用变量-2000-2024_DES.txt\n    |-- 版权声明.pdf\n|-- 上市公司基本信息变更表2000-2024\n    |-- STK_LISTEDCOINFOCHG.xlsx\n    |-- STK_LISTEDCOINFOCHG[DES][xlsx].txt\n    |-- 版权声明.pdf\n|-- 上市公司基本信息年度表\n    |-- STK_LISTEDCOINFOANL.xlsx\n    |-- STK_LISTEDCOINFOANL[DES][xlsx].txt\n    |-- 上市公司基本信息 数据库说明书.pdf\n|-- 利润表-现金流量表-2000-2010\n    |-- 利润表-现金流量表-2000-2010.xlsx\n    |-- 利润表-现金流量表-2000-2010_DES.txt\n    |-- 版权声明.pdf\n|-- 利润表-现金流量表-2011-2024\n    |-- 利润表-现金流量表-2011-2024.xlsx\n    |-- 利润表-现金流量表-2011-2024_DES.txt\n    |-- 版权声明.pdf\n|-- 资产负债表-2000-2010\n    |-- 版权声明.pdf\n    |-- 资产负债表-2000-2010.xlsx\n    |-- 资产负债表-2000-2010_DES.txt\n|-- 资产负债表-2011-2024\n    |-- 版权声明.pdf\n    |-- 资产负债表-2011-2024.xlsx\n    |-- 资产负债表-2011-2024_DES.txt",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>CSMAR 数据处理</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/01_unzip_to_csv.html#定义函数以批量读取指定文件夹下的文件",
    "href": "examples/CSMAR/01_unzip_to_csv.html#定义函数以批量读取指定文件夹下的文件",
    "title": "15  CSMAR 数据处理",
    "section": "15.6 定义函数以批量读取指定文件夹下的文件",
    "text": "15.6 定义函数以批量读取指定文件夹下的文件\n\n15.6.1 文件内容分析\n观察后发现，每个文件夹下都包含两个文件：\n\n*.xlsx：包含数据的 Excel 文件。\n*DES*.txt：包含数据描述的文本文件。\n\n我们可以查看这两份文件的内容和结构，然后再决定读取方案。这里仍以 [资产负债表-2011-2024] 文件夹为例进行说明。\n资产负债表-2011-2024.xlsx 是数据文件，存储结构如下：前 6 列和前 9 行数据如下：\n\n可以看出，从第 5 行开始是具体的观察值，而此前的四行则是表头信息：\n\n第一行是变量名\n第二行是变量的中文简称 (可以用作变量中文标签)\n第三行是变量的单位\n第四行是变量的其他说明信息。如第 5-6 列显示的是报表类型。\n\n资产负债表-2011-2024_DES.txt 是变量描述文件，主要包含变量的中文名称、单位、数据来源等信息。其原始内容如下：\n-------------------- 资产负债表-2011-2024_DES.txt ---------begin--\n本信息:\ncode[证券代码] - \nstknme[证券简称] - \nlistingDate[上市日期] - \nEndDate[时间] - \n\n数据库名称：财务报表--&gt; 表名称：资产负债表(FS_Combas)\nA001101000[货币资金] - 公司库存现金、银行结算户存款、……等的合计数。1990年起使用\nA001107000[交易性金融资产] - 交易性金融资产是……的债券投资、股票投资……。2007年起使用。\n……\n-------------------- 资产负债表-2011-2024_DES.txt ---------over--\n可以看出，资产负债表-2011-2024_DES.txt 文件中每行的格式为：\nvarname[中文简称] - 变量说明\n\nvarname 是变量的英文名称。\n中文简称 是变量的中文名称，可以作为变量标签。\n变量说明 是对该变量的详细描述，包括变量的含义、计算方法、单位、数据来源等信息。\n\n对比这两份文件，有如下几种处理方案：\n方案 1：简化版\n如果只需要 varname 和 中文简称 (后者可以作为前者的变量标签)，则处理过程为：\ninput：资产负债表-2011-2024.xlsx\n\n提取 资产负债表-2011-2024.xlsx 文件中第 5 行以后的所有数据，定义为数据框 df；\n提取 资产负债表-2011-2024.xlsx 前两行数据，定义为一个字典 dict - {varname: 中文简称}；\n将该字典附加到数据框 df 中，作为它的一个属性。\n\noutput：df 数据框 + dict 字典。\n这种处理方式的好处是只需要数据文件 资产负债表-2011-2024.xlsx，缺陷是使用数据时，如果想知道每个变量详细信息，就需要翻阅 CSMAR 提供的 PDF 说明书，或 资产负债表-2011-2024_DES.txt 文件。\n方案 2：完整信息版\n此版本的思路是把变量的中文名和变量说明信息都提取出来，制作成两个字典，整合到数据框中。其好处是，我们可以随时在 Python 内部查看变量的中文名称和说明信息，而不需要翻阅 PDF 或其他文件。\n处理过程如下：\ninput：资产负债表-2011-2024.xlsx + 资产负债表-2011-2024_DES.txt\n\n提取 资产负债表-2011-2024.xlsx 文件中第 5 行以后的所有数据，定义为数据框 df；\n从 资产负债表-2011-2024_DES.txt 文件中提取信息，定义两个字典：\n\n字典 1：{varname: 中文简称}，用于将英文变量名与其中文简称对应起来。\n字典 2：{varname: 变量说明}，用于将英文变量名与其详细说明对应起来。\n\n将这两个字典附加到数据框 df 中，作为它的两个属性。\n\noutput：df 数据框 + dict1 字典 + dict2 字典。\n方案 3：简化版-中文变量名\n如果已经对变量名有了充分了解，且只需要 中文简称，则处理过程为：\ninput：资产负债表-2011-2024.xlsx\n\n读入 资产负债表-2011-2024.xlsx 文件中第 2 行以后的所有数据，定义为数据框 df；\n删除数据框 df 中的第 2-3 行；\n\n上述方案的对比：\n\n方案 1：只保留英文变量名和中文简称，适用于对 CSMAR 数据库比较熟悉的用户。\n方案 2：保留英文变量名、中文简称和变量说明，适用于需要详细了解变量含义的用户。\n方案 3：只保留中文简称，变量含义直观明了。缺陷是，有些变量的中文简称中包含了 (，（， 等特殊字符，需要额外转换。因此，不太推荐这种方式。\n\n下面，我们将实现方案 1 的处理过程。\n\n\n15.6.2 方案 1：简化版处理代码\n处理思路：\n从 CSMAR 下载下来的每个 .zip 文件解压后，都会生成一个同名文件夹 (记为 'folder_path')。该文件夹下包含一个 .xlsx 格式的数据文件和一个 .txt 格式的描述文件。因此，我们只需要指定 'folder_path'，便可以让 Python 根据文件后缀自动识别并读取这两个文件。对于 方案 1，我们只需要读取 .xlsx 文件，并将该文件的第二行作为变量 (第一行) 的中文标签即可。\n\n\n\n\n\n\n提示词\n\n\n\n\n目的：读取指定文件夹下的 Excel 文件，并将其第二行作为变量的中文标签。\ninput：folder = ‘资产负债表-2011-2024’\n\nfolder_path = ‘{path}/data_raw/{folder}’\n{path} 已经在第一个 cell (‘文件路径设定{tag}’) 中定义\n\n处理思路：\n\n检查该文件夹下以 .xlsx 结尾的文件的个数，如果大于 1 个，则报错。\n读入该文件夹下以 .xlsx 结尾的文件。完整文件名为：{filename}.xlsx。\n存入数据框 df_filename。\n\n打印 forder_path 和 df_filename 的名称\n\n删除 Excel 表格第 3 行和第 4 行\n做一个字典：dict_filename，键为英文变量名，值为中文变量名。\n\nkey: 英文变量名，从 Excel 第 1 行提取。\nvalue: 中文变量名，从 Excel 第 2 行提取。\n\n将 dict_filename 附加到 df_filename 数据框中，作为它的一个属性。\n删除第 2 行\n\n显示处理后的数据框 df_filename.head() 的前 5 列和字典 dict_filename 的全部 {key: value}。\n\n\n\n\nimport pandas as pd\n\n# 构造文件夹路径\nfolder = '资产负债表-2011-2024'\nfolder_path = os.path.join(path, 'data_raw', folder)\n\n# 检查 .xlsx 文件数量，排除临时文件（如~$开头的文件）\nxlsx_files = [f for f in os.listdir(folder_path) if f.endswith('.xlsx') and not f.startswith('~$')]\nif len(xlsx_files) != 1:\n    raise ValueError(f\"{folder_path} 下存在 {len(xlsx_files)} 个有效 .xlsx 文件，请检查！\")\nxlsx_file = xlsx_files[0]\nxlsx_path = os.path.join(folder_path, xlsx_file)\n\nprint(f\"folder_path: {folder_path}\")\nprint(f\"df_filename: {xlsx_file}\")\nprint(' ')\n\n# 读取前4行\nheader = pd.read_excel(xlsx_path, nrows=4, header=None)\n\n# 删除第3、4行（索引2、3），再删除第2行（索引1），只保留第0行\nheader_clean = header.drop([2, 3]).reset_index(drop=True)\nheader_clean = header_clean.drop([1]).reset_index(drop=True)\n\n# 提取英文变量名和中文变量名\nen_names = header.iloc[0]\ncn_names = header.iloc[1]\ndict_filename = dict(zip(en_names, cn_names))\n\n# 读取数据，跳过前4行\ndf_filename = pd.read_excel(xlsx_path, skiprows=4, header=None)\ndf_filename.columns = en_names\n\n# 附加字典为属性\ndf_filename.varname_cn = dict_filename\n\n# 显示前5列\nprint(df_filename.iloc[:, :5].head())\n# 显示字典的前10个键值对\nfor k, v in list(dict_filename.items())[:10]:\n    print(f\"{k}: {v}\")\n\nfolder_path: d:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2011-2024\ndf_filename: 资产负债表-2011-2024.xlsx\n \n0  code  stknme listingDate  EndDate  FS_Combas-A001101000\n0     1    平安银行  1991-04-03     2011          0.000000e+00\n1     2     万科A  1991-01-29     2011          3.423951e+10\n2     3  PT 金田A  1991-07-03     2011                   NaN\n3     4  *ST 国华  1991-01-14     2011          5.712837e+07\n4     5   ST 星源  1990-12-10     2011          1.629275e+07\ncode: 证券代码\nstknme: 证券简称\nlistingDate: 上市日期\nEndDate: 时间\nFS_Combas-A001101000: 货币资金\nFS_Combas-A001107000: 交易性金融资产\nFS_Combas-A001109000: 短期投资净额\nFS_Combas-A001123000: 存货净额\nFS_Combas-A001100000: 流动资产合计\nFS_Combas-A001212000: 固定资产净额\n\n\n\n\n15.6.3 优化代码\n上述代码可以一次性顺利执行，但耗时为 53s，太慢。于是，我把上述提示词和代码发给了 ChatGPT，请它优化代码。它给出的建议是：pandas 读取 .xlsx 格式本身就比较慢，尤其是数据量大或格式复杂时，速度瓶颈主要在于底层解析 Excel 文件的过程。此外，上述代码读取了两次 Excel 文件，第一次读取是为了获取变量名和中文简称，第二次读取是为了获取数据，这样会导致重复的 I/O 操作，进一步降低速度。\n应对方法是，先使用 xlsx2csv 包将 .xlsx 文件快速转换为 .csv 格式，然后再用 pandas 的 read_csv 读取数据，这样可以极大提升读取速度。\n优化后的代码只需 1-2 秒即可完成 (ChatGPT 提示词)：\n\n\n\n\n\n\n提示词\n\n\n\n目标：定义一个函数 read_files_in_folder(folder_path, Fname=None)，用于批量读取指定文件夹下的 .xlsx 文件，并返回数据框和变量名-中文名字典。\n要求：\n\n参数说明：\n\nfolder_path：字符串，指定要读取的文件夹路径。\nFname：字符串，可选，指定数据框和字典的名称前缀。若为 None，则自动取 .xlsx 文件名（不含扩展名），并将特殊字符替换为下划线。\n\n处理流程：\n\n检查文件夹下以 .xlsx 结尾且不以 ~$ 开头的文件，若数量不为 1，则报错。\n将 .xlsx 文件转换为 .csv 文件（如已存在则跳过），转换时编码为 gbk。\n读取 .csv 文件前 4 行，提取英文变量名和中文变量名。\n对英文变量名，若为 ‘A-B’ 结构，则只保留 B 部分。\n构建 {英文变量名: 中文变量名} 的字典。\n跳过前 4 行读取正文数据，列名用处理后的英文变量名。\n返回一个 dict，包含数据框（键名为 df_{Fname}）和变量名字典（键名为 dic_{Fname}）。\n\n代码需包含必要的 import，且不重复导入已在 notebook 其他 cell 导入的模块。\n代码块必须完整、可直接运行。\n\n\n\n\n# {tag}: 'fcn_read_files_in_folder'\n#%pip install xlsx2csv\n\nimport os\nimport pandas as pd\nfrom xlsx2csv import Xlsx2csv\n\nfolder = '资产负债表-2011-2024'\nfolder_path = os.path.join(path, 'data_raw', folder)\n# ========================================\n\n# 检查文件夹下 .xlsx 文件\nxlsx_files = [f for f in os.listdir(folder_path) if f.endswith('.xlsx') and not f.startswith('~$')]\nif len(xlsx_files) != 1:\n    raise ValueError(f\"{folder_path} 下存在 {len(xlsx_files)} 个有效 .xlsx 文件，请检查！\")\nxlsx_file = xlsx_files[0]\nxlsx_path = os.path.join(folder_path, xlsx_file)\n\nprint('\\n', '-'*20, '文件存储路径和文件名:', '-'*20)\nprint(f\"folder_path: {folder_path}\")\nprint(f\"df_filename: {xlsx_file}\\n\")\n\n# 自动生成 CSV 文件名\ncsv_path = xlsx_path.replace('.xlsx', '.csv')\n# 如已存在则跳过，否则转换\nif not os.path.exists(csv_path):\n    Xlsx2csv(xlsx_path, outputencoding=\"gbk\").convert(csv_path)\n\n# 读取前4行\nheader = pd.read_csv(csv_path, nrows=4, header=None)\n\n# 提取变量名\nen_names = header.iloc[0].tolist()\ncn_names = header.iloc[1].tolist()\ndict_filename = dict(zip(en_names, cn_names))\n\n# 跳过前4行读取正文\ndf_filename = pd.read_csv(\n    csv_path, skiprows=4, header=None, names=en_names, dtype=str\n)\n\n# 附加字典为属性\nsetattr(df_filename, 'varname_cn', dict_filename)\n\n# 显示前5列\nprint(df_filename.iloc[:, :5].head())\nprint('\\n\\n', '-'*20, '变量名字典:', '-'*20)\nfor k, v in list(dict_filename.items())[:10]:\n    print(f\"{k}: {v}\")\nprint('\\n\\n', '-'*20, '数据框维度:', '-'*20)\nprint(df_filename.shape)\n\n\n -------------------- 文件存储路径和文件名: --------------------\nfolder_path: D:\\Github\\ds_data\\data\\CSMAR\\data_raw\\资产负债表-2011-2024\ndf_filename: 资产负债表-2011-2024.xlsx\n\n     code  stknme listingDate EndDate FS_Combas-A001101000\n0  000001    平安银行  1991-04-03    2011                    0\n1  000002     万科A  1991-01-29    2011   34239514295.080002\n2  000003  PT 金田A  1991-07-03    2011                  NaN\n3  000004  *ST 国华  1991-01-14    2011      57128374.050000\n4  000005   ST 星源  1990-12-10    2011      16292748.160000\n\n\n -------------------- 变量名字典: --------------------\ncode: 证券代码\nstknme: 证券简称\nlistingDate: 上市日期\nEndDate: 时间\nFS_Combas-A001101000: 货币资金\nFS_Combas-A001107000: 交易性金融资产\nFS_Combas-A001109000: 短期投资净额\nFS_Combas-A001123000: 存货净额\nFS_Combas-A001100000: 流动资产合计\nFS_Combas-A001212000: 固定资产净额\n\n\n -------------------- 数据框维度: --------------------\n(81662, 32)",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>CSMAR 数据处理</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/01_unzip_to_csv.html#定义函数以批量读取指定文件夹下的文件-1",
    "href": "examples/CSMAR/01_unzip_to_csv.html#定义函数以批量读取指定文件夹下的文件-1",
    "title": "15  CSMAR 数据处理",
    "section": "15.7 定义函数以批量读取指定文件夹下的文件",
    "text": "15.7 定义函数以批量读取指定文件夹下的文件\n几个要点：\n\n这个提示词还需修改：目前的提示词不具有独立性，应该与上一个 Cell 的提示词合并。\n变量名的处理：在 ‘利润表-现金流量表xxx.csv’ 和 ‘资产负债表xxx.csv’ 文件中，变量名的格式为 ‘A-B’，我们需要删除 ‘A-’ 部分，只保留 ‘C’ 部分。\n\nFS_Combas-A001101000: 货币资金 变为 A001101000: 货币资金\nFS_Comins-B003000000: 基本每股收益 变为 B003000000: 基本每股收益\nFS_Comscfd-C001021000: 支付的各项税费 变为 C001021000: 支付的各项税费\n\n\n\n\n\n\n\n\n提示词\n\n\n\n\n以 {tag}: ‘read_xlsx_example’ Cell 中的代码为基础，定义一个函数 read_files_in_folder(folder_path, Fname)，用于批量读取指定文件夹下的所有文件。\n\n函数参数 folder_path 是一个字符串，表示要读取的文件夹路径。\nFname 是一个字符串，表示文件夹的名称，用于生成数据框和字典的名称。如果用户不指定，则默认使用 .xlsx 文件的名称，特殊字符采用 _ 替换，确保符合 Python 命名规则。\n如果变量名为 A-B 结构，则删除 ‘A-’ 部分，只保留 ‘B’ 部分。\n\n函数返回：一个数据框，名称为 df_{Fname}；一个字典，用于存储 {变量名: 变量中文简称}，名称为 dic_{Fname}。\n\n\n\n\nimport os\nimport pandas as pd\nimport re\nfrom xlsx2csv import Xlsx2csv\n\ndef read_files_in_folder(folder_path, Fname=None):\n    \"\"\"\n    批量读取指定文件夹下的 .xlsx 文件，返回数据框和变量名-中文名字典。\n    参数:\n        folder_path: 文件夹路径\n        Fname: 文件夹名称（可选），用于生成数据框和字典的名称。若为 None，则自动取 .xlsx 文件名（不含扩展名），并将特殊字符替换为下划线。\n    返回:\n        df_{Fname}: 数据框\n        dic_{Fname}: {英文变量名: 中文变量名} 字典\n    \"\"\"\n    # 查找 .xlsx 文件\n    xlsx_files = [f for f in os.listdir(folder_path) if f.endswith('.xlsx') and not f.startswith('~$')]\n    if len(xlsx_files) != 1:\n        raise ValueError(f\"{folder_path} 下存在 {len(xlsx_files)} 个有效 .xlsx 文件，请检查！\")\n    xlsx_file = xlsx_files[0]\n    xlsx_path = os.path.join(folder_path, xlsx_file)\n\n    # 自动生成 CSV 文件名\n    csv_path = xlsx_path.replace('.xlsx', '.csv')\n    if not os.path.exists(csv_path):\n        Xlsx2csv(xlsx_path, outputencoding=\"gbk\").convert(csv_path)  # 若使用 'utf-8' encoding，可能会导致中文乱码问题\n\n    # 读取前4行\n    header = pd.read_csv(csv_path, nrows=4, header=None)\n    en_names = header.iloc[0].tolist()\n    cn_names = header.iloc[1].tolist()\n\n    # 变量名处理：如果为 'A-B' 结构，则只保留 B 部分\n    def clean_varname(name):\n        if isinstance(name, str) and '-' in name:\n            parts = name.split('-')\n            if len(parts) &gt;= 2:\n                return parts[-1]\n        return name\n\n    en_names_clean = [clean_varname(n) for n in en_names]\n    dic = dict(zip(en_names_clean, cn_names))\n\n    # 跳过前4行读取正文\n    df = pd.read_csv(csv_path, skiprows=4, header=None, names=en_names_clean, dtype=str)\n\n    # 生成 Fname\n    if Fname is None:\n        Fname = os.path.splitext(xlsx_file)[0]\n    Fname = re.sub(r'\\W+', '_', Fname)\n\n    # 返回带有指定名称的数据框和字典\n    return {f'df_{Fname}': df, f'dic_{Fname}': dic}",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>CSMAR 数据处理</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/01_unzip_to_csv.html#批量处理",
    "href": "examples/CSMAR/01_unzip_to_csv.html#批量处理",
    "title": "15  CSMAR 数据处理",
    "section": "15.8 批量处理",
    "text": "15.8 批量处理\n接下来，我们就可以用上面定义的函数来批量处理 data_raw 文件夹中的所有子文件夹了。\n\n\n\n\n\n\n提示词\n\n\n\n用 {tag}: ‘fcn_read_files_in_folder’ Cell 中定义的 read_files_in_folder() 函数批量读取 data_raw 文件夹中的所有子文件夹下的文件，并将结果存储到 data_clean 文件夹中。主要步骤如下：\n\n如果 data_clean 文件夹不存在，则创建该文件夹；如果已经存在，则清空该文件夹中的所有内容。\n遍历 data_raw 文件夹中的所有子文件夹。\n\n对于每个子文件夹，调用 read_files_in_folder 函数，读取其中的 .xlsx 文件和 .txt 文件。\n将读取到的数据框和字典保存到 data_clean 文件夹中，文件名为 {子文件夹名称}.pkl。\n\n\n\n\n\nimport shutil\nimport pickle\n\n# 1. 如果 data_clean_folder 不存在则创建，已存在则清空\nif os.path.exists(data_clean_folder):\n    shutil.rmtree(data_clean_folder)\nos.makedirs(data_clean_folder)\n\n# 2. 遍历 data_raw 文件夹下所有子文件夹\nfor subfolder in os.listdir(extract_folder):\n    subfolder_path = os.path.join(extract_folder, subfolder)\n    if os.path.isdir(subfolder_path):\n        try:\n            # 读取数据和变量名字典\n            result = read_files_in_folder(subfolder_path, Fname=subfolder)\n            # 统一保存为 {'df': df, 'varname_cn': dic}\n            df_key = [k for k in result if k.startswith('df_')][0]\n            dic_key = [k for k in result if k.startswith('dic_')][0]\n            save_dict = {'df': result[df_key], 'varname_cn': result[dic_key]}\n            # 保存为 pkl 文件\n            save_path = os.path.join(data_clean_folder, f\"{subfolder}.pkl\")\n            with open(save_path, 'wb') as f:\n                pickle.dump(save_dict, f)\n            print(f\"已保存: {save_path}\")\n        except Exception as e:\n            print(f\"处理 {subfolder} 时出错: {e}\")\n\n处理 CSMAR常用变量-2000-2024 时出错: 'utf-8' codec can't decode byte 0xb9 in position 457: invalid start byte\n处理 上市公司基本信息变更表2000-2024 时出错: 'gbk' codec can't encode character '\\xa0' in position 93: illegal multibyte sequence\n处理 上市公司基本信息年度表 时出错: 'gbk' codec can't encode character '\\ufeff' in position 188: illegal multibyte sequence\n处理 利润表-现金流量表-2000-2010 时出错: 'utf-8' codec can't decode byte 0xb4 in position 728: invalid start byte\n处理 利润表-现金流量表-2011-2024 时出错: 'utf-8' codec can't decode byte 0xb4 in position 728: invalid start byte\n处理 资产负债表-2000-2010 时出错: 'utf-8' codec can't decode byte 0xb4 in position 624: invalid start byte\n处理 资产负债表-2011-2024 时出错: 'utf-8' codec can't decode byte 0xb4 in position 624: invalid start byte\n\n\n\n15.8.1 查看导入结果\n\n\n\n\n\n\n提示词\n\n\n\n列出 data_clean 文件夹中的所有文件，以及每个文件的对应字典的前五个 {key: value}\n\n\n\nimport os\nimport pickle\n\n# 列出 data_clean 文件夹中每个文件的如下信息：\n#   - 文件名\n#   - 数据框的维度以及 '前五行+前五列'\n#   - 字典的前五个 {key: value} 对\n\nprint(\"\\n&gt;&gt;&gt;&gt; 当前 data_clean 文件夹中的文件：\")\nfor item in os.listdir(data_clean_folder):\n    if item.endswith('.pkl'):\n        print('\\n','-'*10, item, '-'*10)\n        with open(os.path.join(data_clean_folder, item), 'rb') as f:\n            data = pickle.load(f)\n            if isinstance(data, dict) and 'df' in data and 'varname_cn' in data:\n                df = data['df']\n                var_dict = data['varname_cn']\n                print(f\"数据框 shape: {df.shape}\")\n                print(\"前五行前五列：\")\n                print(df.iloc[:5, :5])\n                print(\"前五个变量名-中文名：\")\n                for k, v in list(var_dict.items())[:5]:\n                    print(f\"  {k}: {v}\")\n\n\n&gt;&gt;&gt;&gt; 当前 data_clean 文件夹中的文件：\n\n ---------- CSMAR常用变量-2000-2024.pkl ----------\n数据框 shape: (61455, 33)\n前五行前五列：\n    Stkcd accper stknme AnaAttention       Audittyp\n0  000001   2001   平安银行          NaN        标准无保留意见\n1  000001   2002   平安银行     1.098612        标准无保留意见\n2  000001   2003   平安银行     1.386294        标准无保留意见\n3  000001   2004   平安银行     1.791759  带有解释性说明的无保留意见\n4  000001   2005   平安银行     1.791759        标准无保留意见\n前五个变量名-中文名：\n  Stkcd: 股票代码\n  accper: 会计年度\n  stknme: 股票简称\n  AnaAttention: 分析师关注度\n  Audittyp: 审计意见\n\n ---------- 上市公司基本信息变更表2000-2024.pkl ----------\n数据框 shape: (160275, 8)\n前五行前五列：\n   Symbol AnnouncementDate ImplementDate ChangedItem    SecurityID\n0  000001              NaN    1991-04-03        所属省份  201000000001\n1  000001              NaN    1991-04-03        注册地址  201000000001\n2  000001              NaN    1991-04-03        公司全称  201000000001\n3  000001              NaN    1991-04-03        证券简称  201000000001\n4  000001              NaN    1991-04-03      公司经营性质  201000000001\n前五个变量名-中文名：\n  Symbol: 股票代码\n  AnnouncementDate: 公告日期\n  ImplementDate: 实施日期\n  ChangedItem: 变更属性\n  SecurityID: 证券ID\n\n ---------- 上市公司基本信息年度表.pkl ----------\n数据框 shape: (64170, 40)\n前五行前五列：\n   Symbol ShortName     EndDate ListedCoID    SecurityID\n0  000001      深发展A  2001-12-31     101704  201000000001\n1  000001      深发展A  2002-12-31     101704  201000000001\n2  000001      深发展A  2003-12-31     101704  201000000001\n3  000001      深发展A  2004-12-31     101704  201000000001\n4  000001      深发展A  2005-12-31     101704  201000000001\n前五个变量名-中文名：\n  Symbol: 股票代码\n  ShortName: 股票简称\n  EndDate: 统计截止日期\n  ListedCoID: 上市公司ID\n  SecurityID: 证券ID\n\n ---------- 利润表-现金流量表-2000-2010.pkl ----------\n数据框 shape: (64163, 36)\n前五行前五列：\n     code  stknme listingDate EndDate         B001101000\n0  000001    平安银行  1991-04-03    2000                NaN\n1  000002     万科A  1991-01-29    2000  3783668674.180000\n2  000003  PT 金田A  1991-07-03    2000   464723527.060000\n3  000004  *ST 国华  1991-01-14    2000   131006632.910000\n4  000005   ST 星源  1990-12-10    2000   145947499.350000\n前五个变量名-中文名：\n  code: 证券代码\n  stknme: 证券简称\n  listingDate: 上市日期\n  EndDate: 时间\n  B001101000: 营业收入\n\n ---------- 利润表-现金流量表-2011-2024.pkl ----------\n数据框 shape: (81662, 36)\n前五行前五列：\n     code  stknme listingDate EndDate          B001101000\n0  000001    平安银行  1991-04-03    2011                 NaN\n1  000002     万科A  1991-01-29    2011  71782749800.679993\n2  000003  PT 金田A  1991-07-03    2011                 NaN\n3  000004  *ST 国华  1991-01-14    2011     74503718.530000\n4  000005   ST 星源  1990-12-10    2011     63534839.010000\n前五个变量名-中文名：\n  code: 证券代码\n  stknme: 证券简称\n  listingDate: 上市日期\n  EndDate: 时间\n  B001101000: 营业收入\n\n ---------- 资产负债表-2000-2010.pkl ----------\n数据框 shape: (64163, 32)\n前五行前五列：\n     code  stknme listingDate EndDate        A001101000\n0  000001    平安银行  1991-04-03    2000               NaN\n1  000002     万科A  1991-01-29    2000  995745160.050000\n2  000003  PT 金田A  1991-07-03    2000   58018167.850000\n3  000004  *ST 国华  1991-01-14    2000   64780229.730000\n4  000005   ST 星源  1990-12-10    2000   29118049.740000\n前五个变量名-中文名：\n  code: 证券代码\n  stknme: 证券简称\n  listingDate: 上市日期\n  EndDate: 时间\n  A001101000: 货币资金\n\n ---------- 资产负债表-2011-2024.pkl ----------\n数据框 shape: (81662, 32)\n前五行前五列：\n     code  stknme listingDate EndDate          A001101000\n0  000001    平安银行  1991-04-03    2011                   0\n1  000002     万科A  1991-01-29    2011  34239514295.080002\n2  000003  PT 金田A  1991-07-03    2011                 NaN\n3  000004  *ST 国华  1991-01-14    2011     57128374.050000\n4  000005   ST 星源  1990-12-10    2011     16292748.160000\n前五个变量名-中文名：\n  code: 证券代码\n  stknme: 证券简称\n  listingDate: 上市日期\n  EndDate: 时间\n  A001101000: 货币资金\n\n\n简要分析：\n考虑到随后要将这些文件合并为一个数据框，我们需要重点关注如下几点：\n\n上市公司基本信息变更表2000-2024.pkl 暂时用不到，不予处理。\n哪些文件需要纵向合并 (append)?\n\n利润表-现金流量表-xxx.pkl 和 资产负债表-xxx.pkl 这两类文件需要纵向合并，分别存入数据框 df_profit 和 df_asset 中。\n\n哪些文件需要横向合并 (merge)?\n\n将 df_profit 和 df_asset 横向合并为一个数据框 df_financial；\n进而与 CSMAR常用变量-2000-2024.pkl 和 上市公司基本信息年度表.pkl 横向合并，得到最终的数据框 df_final。\n\n横向合并时，需要保证两份数据中有相同的变量名 (keys)。由于我们要合并的数据都是 ‘firm-year’ 格式的面板数据，因此需要保证每个数据框中都有 ‘code’ 和 ‘year’ 这两个变量。\n\n我们需要为 CSMAR常用变量-2000-2024.pkl 和 上市公司基本信息年度表.pkl 这两份文件添加 ‘code’ 和 ‘year’ 变量。二者分别对应这三个数据文件中的 Symbol 和 EndDate 变量。\n对于 利润表-现金流量表-xxx.pkl 和 资产负债表-xxx 文件，需要将 EndDate 变量转换为 year 变量，以便后续合并。\n\n\n\n\n15.8.2 查看处理后的单个文件\n\n\n\n\n\n\n提示词\n\n\n\n\n列示 ‘CSMAR常用变量-2000-2024.pkl’ 中的所有对象；\n空一行+分隔线；\n列示 ‘CSMAR常用变量-2000-2024.pkl’ 的 shape；\n空一行+分隔线；\n列示 ‘CSMAR常用变量-2000-2024.pkl’ 数据框中的 ‘前五行+前五列’；\n空一行+分隔线；\n列示字典中的所有 {变量名：中文简称}。\n\n\n\n\nimport pickle\n\n# 载入数据\npkl_path = os.path.join(data_clean_folder, 'CSMAR常用变量-2000-2024.pkl')\nwith open(pkl_path, 'rb') as f:\n    data = pickle.load(f)\n\n# 列示所有对象\nprint(\"对象列表：\", list(data.keys()))\n\nprint('\\n' + '-'*40 + '\\n')\n\n# 列示 shape\ndf_csmar = data['df']\nprint(\"数据框 shape:\", df_csmar.shape)\n\nprint('\\n' + '-'*40 + '\\n')\n\n# 列示前五行前五列\nprint(df_csmar.iloc[:5, :5])\n\nprint('\\n' + '-'*40 + '\\n')\n\n# 列示字典中的所有 {变量名：中文简称}\nvarname_cn_csmar = data['varname_cn']\nfor k, v in varname_cn_csmar.items():\n    print(f\"{k}: {v}\")\n\n对象列表： ['df', 'varname_cn']\n\n----------------------------------------\n\n数据框 shape: (61455, 33)\n\n----------------------------------------\n\n    Stkcd accper stknme AnaAttention       Audittyp\n0  000001   2001   平安银行          NaN        标准无保留意见\n1  000001   2002   平安银行     1.098612        标准无保留意见\n2  000001   2003   平安银行     1.386294        标准无保留意见\n3  000001   2004   平安银行     1.791759  带有解释性说明的无保留意见\n4  000001   2005   平安银行     1.791759        标准无保留意见\n\n----------------------------------------\n\nStkcd: 股票代码\naccper: 会计年度\nstknme: 股票简称\nAnaAttention: 分析师关注度\nAudittyp: 审计意见\nInternationalBig4: 审计师是否来自国际四大\nYsmvosd: 年个股流通市值\nYsmvttl: 年个股总市值\nYretwd: 考虑现金红利再投资的年个股回报率\nPropertyRightsNature: 产权性质\nSeperation: 两权分离度\nActualControllerNatureID: 实际控制人性质编码\nOwnershipProportion: 实际控制人拥有上市公司所有权比例\nControlProportion: 实际控制人拥有上市公司控制权比例\nShrcr1: 股权集中度1\nShrhfd5: 股权集中度9\nShrz: 股权集中度5\nFundHoldProportion: 基金持股比例\nQFIIHoldProportion: 合格境外投资者持股比例\nBrokerHoldProportion: 券商持股比例\nBankHoldProportion: 银行持股比例\nNonFinanceHoldProportion: 非金融类上市公司持股比例\nInsInvestorProp: 机构投资者持股比例\nStaffNumber: 员工人数\nConcurrentPosition: 两职合一\nBoardsize2: 董事会规模A\nExecutivesNumber: 高管人数\nIndDirector: 独立董事人数\nSumSalary: 管理层薪酬\nTOP3SumSalary: 高管前三名薪酬总额\nYnshrtrd: 年个股交易股数\nDirectorHoldshares: 董事会持股数量\nManageHoldshares: 高级管理人员持股数量",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>CSMAR 数据处理</span>"
    ]
  },
  {
    "objectID": "examples/CSMAR/01_unzip_to_csv.html#合并数据",
    "href": "examples/CSMAR/01_unzip_to_csv.html#合并数据",
    "title": "15  CSMAR 数据处理",
    "section": "15.9 合并数据",
    "text": "15.9 合并数据\n\n15.9.1 纵向合并\n\n\n\n\n\n\n提示词\n\n\n\n== 纵向合并不同年度的同名文件\n\n纵向合并 data_clean 文件夹中所有以 ‘资产负债表’ 开头的文件。\n\n将 EndDate 列重命名为 year 列。\n合并后的数据框命名为 df_assets，并保存到 data_clean 文件夹中，文件名为 df_assets.pkl。\n\n纵向合并 data_clean 文件夹中所有以 ‘利润表’ 开头的文件。\n\n将 EndDate 列重命名为 year 列。\n合并后的数据框命名为 df_profit，并保存到 data_clean 文件夹中，文件名为 df_profit.pkl。\n\n上述合并完成后，打印合并后的数据框的 shape；展示 ‘前三行 + 前五列’。\n\n\n\n\nimport os\nimport pickle\nimport pandas as pd\n\n# 合并资产负债表\nassets_dfs = []\nfor item in os.listdir(data_clean_folder):\n    if item.startswith('资产负债表') and item.endswith('.pkl'):\n        with open(os.path.join(data_clean_folder, item), 'rb') as f:\n            data = pickle.load(f)\n            df = data['df'] if isinstance(data, dict) and 'df' in data else data\n            if 'EndDate' in df.columns:\n                df = df.rename(columns={'EndDate': 'year'})\n            assets_dfs.append(df)\nif assets_dfs:\n    df_assets = pd.concat(assets_dfs, axis=0, ignore_index=True)\n    with open(os.path.join(data_clean_folder, 'df_assets.pkl'), 'wb') as f:\n        pickle.dump(df_assets, f)\n    print(\"df_assets shape:\", df_assets.shape)\n    print(df_assets.iloc[:3, :5])\n\nprint('\\n' + '-'*40 + '\\n')\n\n# 合并利润表\nprofit_dfs = []\nfor item in os.listdir(data_clean_folder):\n    if item.startswith('利润表') and item.endswith('.pkl'):\n        with open(os.path.join(data_clean_folder, item), 'rb') as f:\n            data = pickle.load(f)\n            df = data['df'] if isinstance(data, dict) and 'df' in data else data\n            if 'EndDate' in df.columns:\n                df = df.rename(columns={'EndDate': 'year'})\n            profit_dfs.append(df)\nif profit_dfs:\n    df_profit = pd.concat(profit_dfs, axis=0, ignore_index=True)\n    with open(os.path.join(data_clean_folder, 'df_profit.pkl'), 'wb') as f:\n        pickle.dump(df_profit, f)\n    print(\"df_profit shape:\", df_profit.shape)\n    print(df_profit.iloc[:3, :5])\n\ndf_assets shape: (145825, 32)\n     code  stknme listingDate  year        A001101000\n0  000001    平安银行  1991-04-03  2000               NaN\n1  000002     万科A  1991-01-29  2000  995745160.050000\n2  000003  PT 金田A  1991-07-03  2000   58018167.850000\n\n----------------------------------------\n\ndf_profit shape: (145825, 36)\n     code  stknme listingDate  year         B001101000\n0  000001    平安银行  1991-04-03  2000                NaN\n1  000002     万科A  1991-01-29  2000  3783668674.180000\n2  000003  PT 金田A  1991-07-03  2000   464723527.060000\n\n\n\n\n15.9.2 横向合并\n\n\n\n\n\n\n提示词\n\n\n\n== 横向合并不同来源的数据\n\n一些出现两次以上的功能，可以预先定义函数，确保代码结构清晰\n资产负债表与利润表的合并\n\n\n读入 data_clean 文件夹中的 df_assets.pkl。\n按照关键词 ‘{code, year}’ 与 df_profit.pkl 数据框横向合并；\n\n如果有同名变量，则自动忽略。\n\n合并后的数据存入数据框 df_financial;\n数据框的前两列变量为 ‘code, year’\n\n\n继续与 上市公司基本信息年度表, CSMAR常用变量-2000-2024.pkl 数据框横向合并。\n\n\n合并前，先将 上市公司基本信息年度表.pkl 中的 Symbol 列重命名为 code，EndDate 列重命名为 year。\n按照关键词 ‘{code, year}’ 与 df_financial 数据框横向合并；\n\n如果有同名变量，则自动忽略。\n\n\n\n输出合并后的数据文件\n\n\n合并后的数据存入数据框 df_final，\n\n第一列为 ‘code’，第二列为 ‘year’，是两个独立的变量。\n保存到 data_clean 文件夹中，文件名为 df_final.pkl。\n\n最终数据框 df_final 的前两列变量为 ‘code, year’，并展示其 shape 和 ‘前三行 + 前五列’。\n\n\n合并数据字典。将上述文件的字典合并为一个大字典 dict_CSMAR。\n\n\n同名变量的中文简称和说明信息取先进入的文件的。\n\n\n打印数据字典尺寸，以及前十行信息，格式：{变量名: 中文简称}；\n每组打印结果之间空一行，添加标题文字和分割线\n\n\n\n\nimport os\nimport pickle\nimport pandas as pd\n\n# 工具函数：按 code, year 横向合并，自动忽略同名变量\ndef merge_on_code_year(left, right, suffix='_right'):\n    # 只保留右表中不与左表重复的列\n    overlap = set(left.columns) & set(right.columns)\n    overlap -= {'code', 'year'}\n    right_use = right.drop(columns=list(overlap), errors='ignore')\n    merged = pd.merge(left, right_use, on=['code', 'year'], how='left', suffixes=('', suffix))\n    return merged\n\n# 1. 资产负债表与利润表的合并\nwith open(os.path.join(data_clean_folder, 'df_assets.pkl'), 'rb') as f:\n    df_assets = pickle.load(f)\nwith open(os.path.join(data_clean_folder, 'df_profit.pkl'), 'rb') as f:\n    df_profit = pickle.load(f)\n\n# 确保 'code' 和 'year' 列为字符串类型\ndf_assets['code'] = df_assets['code'].astype(str)\ndf_assets['year'] = df_assets['year'].astype(str)\ndf_profit['code'] = df_profit['code'].astype(str)\ndf_profit['year'] = df_profit['year'].astype(str)\n\ndf_financial = merge_on_code_year(df_assets, df_profit, suffix='_profit')\n\n# 2. 继续与 “上市公司基本信息年度表” 和 “CSMAR常用变量-2000-2024.pkl” 合并\nwith open(os.path.join(data_clean_folder, '上市公司基本信息年度表.pkl'), 'rb') as f:\n    data_basic = pickle.load(f)\ndf_basic = data_basic['df'] if isinstance(data_basic, dict) and 'df' in data_basic else data_basic\ndf_basic = df_basic.rename(columns={'Symbol': 'code', 'EndDate': 'year'})\ndf_basic['code'] = df_basic['code'].astype(str)\ndf_basic['year'] = df_basic['year'].astype(str)\n\nwith open(os.path.join(data_clean_folder, 'CSMAR常用变量-2000-2024.pkl'), 'rb') as f:\n    data_csmar = pickle.load(f)\ndf_csmar = data_csmar['df'] if isinstance(data_csmar, dict) and 'df' in data_csmar else data_csmar\ndf_csmar['code'] = df_csmar['Stkcd'].astype(str)\ndf_csmar['year'] = df_csmar['accper'].astype(str)\n\ndf_financial = merge_on_code_year(df_financial, df_basic, suffix='_basic')\ndf_final = merge_on_code_year(df_financial, df_csmar, suffix='_csmar')\n\n# 调整前两列顺序\ncols = list(df_final.columns)\nif 'code' in cols and 'year' in cols:\n    cols.remove('code')\n    cols.remove('year')\n    df_final = df_final[['code', 'year'] + cols]\n\n# 保存\nwith open(os.path.join(data_clean_folder, 'df_final.pkl'), 'wb') as f:\n    pickle.dump(df_final, f)\n\nprint(\"==== 合并后数据框 df_final ====\")\nprint(\"df_final shape:\", df_final.shape)\nprint(df_final.iloc[:3, :5])\nprint('-'*40)\n\n# 4. 合并数据字典\ndicts = []\n# 资产负债表\nwith open(os.path.join(data_clean_folder, '资产负债表-2011-2024.pkl'), 'rb') as f:\n    d = pickle.load(f)\n    if isinstance(d, dict) and 'varname_cn' in d:\n        dicts.append(d['varname_cn'])\n# 利润表\nfor fname in ['利润表-现金流量表-2000-2010.pkl', '利润表-现金流量表-2011-2024.pkl']:\n    pkl_path = os.path.join(data_clean_folder, fname)\n    if os.path.exists(pkl_path):\n        with open(pkl_path, 'rb') as f:\n            d = pickle.load(f)\n            if isinstance(d, dict) and 'varname_cn' in d:\n                dicts.append(d['varname_cn'])\n# 基本信息\nif isinstance(data_basic, dict) and 'varname_cn' in data_basic:\n    dicts.append(data_basic['varname_cn'])\n# CSMAR常用变量\nif isinstance(data_csmar, dict) and 'varname_cn' in data_csmar:\n    dicts.append(data_csmar['varname_cn'])\n\ndict_CSMAR = {}\nfor d in dicts:\n    dict_CSMAR.update(d)\n\nprint(\"==== 合并后数据字典 dict_CSMAR ====\")\nprint(f\"字典总变量数: {len(dict_CSMAR)}\")\nfor i, (k, v) in enumerate(dict_CSMAR.items()):\n    print(f\"{k}: {v}\")\n    if i &gt;= 200:\n        break\nprint('-'*40)\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[58], line 15\n     12     return merged\n     14 # 1. 资产负债表与利润表的合并\n---&gt; 15 with open(os.path.join(data_clean_folder, 'df_assets.pkl'), 'rb') as f:\n     16     df_assets = pickle.load(f)\n     17 with open(os.path.join(data_clean_folder, 'df_profit.pkl'), 'rb') as f:\n\nFile c:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324, in _modified_open(file, *args, **kwargs)\n    317 if file in {0, 1, 2}:\n    318     raise ValueError(\n    319         f\"IPython won't let you open fd={file} by default \"\n    320         \"as it is likely to crash IPython. If you know what you are doing, \"\n    321         \"you can use builtins' open.\"\n    322     )\n--&gt; 324 return io_open(file, *args, **kwargs)\n\nFileNotFoundError: [Errno 2] No such file or directory: 'D:\\\\Github\\\\ds_data\\\\data\\\\CSMAR\\\\data_clean\\\\df_assets.pkl'\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n# 只对数值型变量做统计\nnumeric_cols = df_final.select_dtypes(include=[np.number]).columns\n\n# 统计量\ndesc = df_final[numeric_cols].agg(['count', 'mean', 'std', 'min', 'max']).T\ndesc = desc.rename(columns={'count': 'N', 'mean': 'Mean', 'std': 'SD', 'min': 'Min', 'max': 'Max'})\n\n# 缺失值统计\nmissing = df_final.isnull().sum()\nmissing = missing[missing &gt; 0].sort_values(ascending=False)\n\nprint(\"==== 数值型变量基本统计量 (N, Mean, SD, Min, Max) ====\")\ndisplay(desc)\n\nprint(\"\\n==== 各变量缺失值数量 (只显示有缺失的变量) ====\")\ndisplay(missing)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[57], line 8\n      5 numeric_cols = df_final.select_dtypes(include=[np.number]).columns\n      7 # 统计量\n----&gt; 8 desc = df_final[numeric_cols].agg(['count', 'mean', 'std', 'min', 'max']).T\n      9 desc = desc.rename(columns={'count': 'N', 'mean': 'Mean', 'std': 'SD', 'min': 'Min', 'max': 'Max'})\n     11 # 缺失值统计\n\nFile c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:10149, in DataFrame.aggregate(self, func, axis, *args, **kwargs)\n  10146 axis = self._get_axis_number(axis)\n  10148 op = frame_apply(self, func=func, axis=axis, args=args, kwargs=kwargs)\n&gt; 10149 result = op.agg()\n  10150 result = reconstruct_and_relabel_result(result, func, **kwargs)\n  10151 return result\n\nFile c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:928, in FrameApply.agg(self)\n    926 result = None\n    927 try:\n--&gt; 928     result = super().agg()\n    929 finally:\n    930     self.obj = obj\n\nFile c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:193, in Apply.agg(self)\n    190     return self.agg_dict_like()\n    191 elif is_list_like(func):\n    192     # we require a list, but not a 'str'\n--&gt; 193     return self.agg_list_like()\n    195 if callable(func):\n    196     f = com.get_cython_func(func)\n\nFile c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:326, in Apply.agg_list_like(self)\n    318 def agg_list_like(self) -&gt; DataFrame | Series:\n    319     \"\"\"\n    320     Compute aggregation in the case of a list-like argument.\n    321 \n   (...)\n    324     Result of aggregation.\n    325     \"\"\"\n--&gt; 326     return self.agg_or_apply_list_like(op_name=\"agg\")\n\nFile c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:745, in NDFrameApply.agg_or_apply_list_like(self, op_name)\n    742     raise NotImplementedError(\"axis other than 0 is not supported\")\n    744 keys, results = self.compute_list_like(op_name, obj, kwargs)\n--&gt; 745 result = self.wrap_results_list_like(keys, results)\n    746 return result\n\nFile c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:402, in Apply.wrap_results_list_like(self, keys, results)\n    399 obj = self.obj\n    401 try:\n--&gt; 402     return concat(results, keys=keys, axis=1, sort=False)\n    403 except TypeError as err:\n    404     # we are concatting non-NDFrame objects,\n    405     # e.g. a list of scalars\n    406     from pandas import Series\n\nFile c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:382, in concat(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\n    379 elif copy and using_copy_on_write():\n    380     copy = False\n--&gt; 382 op = _Concatenator(\n    383     objs,\n    384     axis=axis,\n    385     ignore_index=ignore_index,\n    386     join=join,\n    387     keys=keys,\n    388     levels=levels,\n    389     names=names,\n    390     verify_integrity=verify_integrity,\n    391     copy=copy,\n    392     sort=sort,\n    393 )\n    395 return op.get_result()\n\nFile c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:445, in _Concatenator.__init__(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\n    442 self.verify_integrity = verify_integrity\n    443 self.copy = copy\n--&gt; 445 objs, keys = self._clean_keys_and_objs(objs, keys)\n    447 # figure out what our result ndim is going to be\n    448 ndims = self._get_ndims(objs)\n\nFile c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:507, in _Concatenator._clean_keys_and_objs(self, objs, keys)\n    504     objs_list = list(objs)\n    506 if len(objs_list) == 0:\n--&gt; 507     raise ValueError(\"No objects to concatenate\")\n    509 if keys is None:\n    510     objs_list = list(com.not_none(*objs_list))\n\nValueError: No objects to concatenate\n\n\n\n\ndf_final.summary()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n~\\AppData\\Local\\Temp\\ipykernel_53792\\2207892642.py in ?()\n----&gt; 1 df_final.summary()\n\nc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py in ?(self, name)\n   6295             and name not in self._accessors\n   6296             and self._info_axis._can_hold_identifiers_and_holds_name(name)\n   6297         ):\n   6298             return self[name]\n-&gt; 6299         return object.__getattribute__(self, name)\n\nAttributeError: 'DataFrame' object has no attribute 'summary'\n\n\n\n\n\n15.9.3 输出最终数据文件\n\n\n\n\n\n\n提示词\n\n\n\n== 输出 csv 和 txt 文件到 [data_final] 文件夹\n\n将 df_final 数据框保存为 CSMAR_final.csv 文件，存储在 [data_final] 文件夹中。\n将 dict_CSMAR 字典保存为 CSMAR_var_label.txt 文件，存储在 [data_final] 文件夹中。每行格式为：变量名: 中文简称。\n\n\n\n\nimport os\n\n# 创建 data_final 文件夹（如果不存在）\ndata_final_folder = os.path.join(path, 'data_final')\nif not os.path.exists(data_final_folder):\n    os.makedirs(data_final_folder)\n\n# 1. 保存 df_final 为 CSV\ncsv_path = os.path.join(data_final_folder, 'CSMAR_final.csv')\ndf_final.to_csv(csv_path, index=False, encoding='utf-8-sig')\nprint(f\"已保存数据文件: {csv_path}\")\n\n# 2. 保存 dict_CSMAR 为 txt\ntxt_path = os.path.join(data_final_folder, 'CSMAR_var_label.txt')\nwith open(txt_path, 'w', encoding='utf-8') as f:\n    for k, v in dict_CSMAR.items():\n        f.write(f\"{k}: {v}\\n\")\nprint(f\"已保存变量标签文件: {txt_path}\")\n\n已保存数据文件: d:\\Github\\ds_data\\data\\CSMAR\\data_final\\CSMAR_final.csv\n已保存变量标签文件: d:\\Github\\ds_data\\data\\CSMAR\\data_final\\CSMAR_var_label.txt\n\n\n\n\n15.9.4 收尾：删除无用文件和过程文件\n\nNote：这一步是可选的，主要是为了清理不必要的文件，以节省存储空间和提高数据处理效率。\n建议：确认所有数据处理和合并工作完成后，再执行此步骤。\n\n有些过程文件已经不需要了：\n\ndata_raw 文件夹中的 .xlsx 文件已经转换为 .csv 文件，因此可以删除。\ndata_clean 文件夹中的一些文件也不需要了，如 利润表-现金流量表-xxx.pkl 和 资产负债表-xxx.pkl，因为它们已经被合并为 df_financial.pkl。\n\n\n\n\n\n\n\n提示词\n\n\n\n\n删除 data_raw 文件夹中所有子文件夹中的 .xlsx 文件。\n删除 data_clean 文件夹中如下文件：\n\n以 利润表-现金流量表- 开头的文件；\n以 资产负债表- 开头的文件；\n\n\n\n\n\nimport os\n\n# 1. 删除 data_raw 文件夹中所有子文件夹中的 .xlsx 文件\nfor subfolder in os.listdir(extract_folder):\n    subfolder_path = os.path.join(extract_folder, subfolder)\n    if os.path.isdir(subfolder_path):\n        for fname in os.listdir(subfolder_path):\n            if fname.endswith('.xlsx'):\n                file_path = os.path.join(subfolder_path, fname)\n                os.remove(file_path)\n                print(f\"已删除: {file_path}\")\n\n# 2. 删除 data_clean 文件夹中指定前缀的文件\nprefixes = ['利润表-现金流量表-', '资产负债表-']\nfor fname in os.listdir(data_clean_folder):\n    if any(fname.startswith(prefix) for prefix in prefixes):\n        file_path = os.path.join(data_clean_folder, fname)\n        if os.path.isfile(file_path):\n            os.remove(file_path)\n            print(f\"已删除: {file_path}\")\n\n\n\n15.9.5 呈现项目文档树\n要点：\n\n代码执行后会自动将工作目录切换回项目根目录，确保后续操作路径一致。\n\n\n\n\n\n\n\n提示词\n\n\n\n\n列示项目文档树结构。\n只列示文件夹名称和文件名称，不需要显示文件内容。\n如果文件夹中有子文件夹，则显示子文件夹名称。\n处理完后，将工作目录切换到项目根目录。\n为此代码块添加标题和合适的注释。\n\n\n\n\nimport os\n\n# === 项目文档树结构展示 ===\n# 本代码用于递归列示当前项目的文件夹和文件结构，便于快速了解项目目录布局。\n# 最后将工作目录切换回项目根目录。\n\ndef print_project_tree(root, indent=\"\"):\n    for item in os.listdir(root):\n        item_path = os.path.join(root, item)\n        print(indent + \"|-- \" + item)\n        if os.path.isdir(item_path):\n            print_project_tree(item_path, indent + \"    \")\n\nprint(\"=== 项目文档树结构 ===\")\nprint_project_tree(path)\n\n# 切换工作目录到项目根目录\nos.chdir(path)\n\n=== 项目文档树结构 ===\n|-- 01_read_merge.ipynb\n|-- 02_data_clean.ipynb\n|-- CSMAR_API.md\n|-- CSMAR_firm_basic_infor.ipynb\n|-- data_clean\n    |-- CSMAR常用变量-2000-2024.pkl\n    |-- df_assets.pkl\n    |-- df_final.pkl\n    |-- df_profit.pkl\n    |-- 上市公司基本信息变更表2000-2024.pkl\n    |-- 上市公司基本信息年度表.pkl\n    |-- 利润表-现金流量表-2000-2010.pkl\n    |-- 利润表-现金流量表-2011-2024.pkl\n    |-- 资产负债表-2000-2010.pkl\n    |-- 资产负债表-2011-2024.pkl\n|-- data_final\n    |-- CSMAR_final.csv\n    |-- CSMAR_var_label.txt\n|-- data_raw\n    |-- CSMAR常用变量-2000-2024\n        |-- CSMAR常用变量-2000-2024.csv\n        |-- CSMAR常用变量-2000-2024.xlsx\n        |-- CSMAR常用变量-2000-2024_DES.txt\n        |-- 版权声明.pdf\n    |-- 上市公司基本信息变更表2000-2024\n        |-- STK_LISTEDCOINFOCHG.csv\n        |-- STK_LISTEDCOINFOCHG.xlsx\n        |-- STK_LISTEDCOINFOCHG[DES][xlsx].txt\n        |-- 版权声明.pdf\n    |-- 上市公司基本信息年度表\n        |-- STK_LISTEDCOINFOANL.csv\n        |-- STK_LISTEDCOINFOANL.xlsx\n        |-- STK_LISTEDCOINFOANL[DES][xlsx].txt\n        |-- 上市公司基本信息 数据库说明书.pdf\n    |-- 利润表-现金流量表-2000-2010\n        |-- 利润表-现金流量表-2000-2010.csv\n        |-- 利润表-现金流量表-2000-2010.xlsx\n        |-- 利润表-现金流量表-2000-2010_DES.txt\n        |-- 版权声明.pdf\n    |-- 利润表-现金流量表-2011-2024\n        |-- 利润表-现金流量表-2011-2024.csv\n        |-- 利润表-现金流量表-2011-2024.xlsx\n        |-- 利润表-现金流量表-2011-2024_DES.txt\n        |-- 版权声明.pdf\n    |-- 资产负债表-2000-2010\n        |-- 版权声明.pdf\n        |-- 资产负债表-2000-2010.csv\n        |-- 资产负债表-2000-2010.xlsx\n        |-- 资产负债表-2000-2010_DES.txt\n    |-- 资产负债表-2011-2024\n        |-- 版权声明.pdf\n        |-- 资产负债表-2011-2024.csv\n        |-- 资产负债表-2011-2024.xlsx\n        |-- 资产负债表-2011-2024_DES.txt\n|-- data_raw_zip\n    |-- CSMAR常用变量-2000-2024.zip\n    |-- 上市公司基本信息变更表2000-2024.zip\n    |-- 上市公司基本信息年度表.zip\n    |-- 利润表-现金流量表-2000-2010.zip\n    |-- 利润表-现金流量表-2011-2024.zip\n    |-- 资产负债表-2000-2010.zip\n    |-- 资产负债表-2011-2024.zip\n|-- functions\n    |-- CSMAR_var_label.py\n\n\n\n\n15.9.6 保留哪些文件夹？\n在本地运行完毕后，项目根目录下会包含如下文件夹：\n\n[1] data_raw_zip 文件夹：务必保留，里面存放了从 CSMAR 下载的原始数据压缩包。\n[2] data_raw 文件夹：可以删除，里面存放了解压后的 .xlsx 和 .txt 文档\n[3] data_clean 文件夹：酌情删除，里面存放了清洗后的 .pkl 文件。\n[4] data_final 文件夹：建议保留，里面存放了最终的 .csv 和 .txt 文件。\n\n可复现文档分享方案：\n一旦确定上述代码无误，在分享时，只需保留 01_read_merge.ipynb 文件和 [data_raw_zip] 文件夹即可。\n\n完整复现文档：\n\n01_read_merge.ipynb 文件：包含了从数据读取到合并的完整代码。\n[data_raw_zip] 文件夹：包含了从 CSMAR 下载的原始数据压缩包。\n\n直接使用最终文档 + 了解处理过程：\n\ndata_final 文件夹：包含了最终的 .csv 和 .txt 文件，便于直接使用。\n01_read_merge.ipynb 文件：包含了从数据读取到合并的完整代码。\n\n\n\n## 下一步？\n\n接下来，我们会编写：\n\n-  `02_data_clean.ipynb` 文件，对 `data_final` 文件夹中的数据做进一步处理，包括：缺失值、文字变量转换为数值变量、生成新的变量、处理离群值等。\n-  `03_data_analysis.ipynb` 文件，对 `data_final` 文件夹中的数据进行分析，包括：描述性统计、相关性分析、回归分析等。",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>CSMAR 数据处理</span>"
    ]
  },
  {
    "objectID": "examples/web_dangdang_book/case_Dangdang_book.html",
    "href": "examples/web_dangdang_book/case_Dangdang_book.html",
    "title": "16  案例：当当网 Python 类图书销售数据分析",
    "section": "",
    "text": "16.1 背景\n我想写一本有关 Python 应用的书。目前我有很多想法，拿不定主意。比如，\n在不了解这些信息的情况下，贸然画一年的时间写一本书是一个非常冒险的决定。为了降低风险，我决定先做一些市场调研，看看读者对 Python 书籍的需求。\n我先通过百度搜索和 Google 搜索，了解到当当网是中国最大的图书销售网站之一，拥有丰富的图书数据。我决定从当当网获取一些 Python 类图书的销售数据，进行分析，看看读者对 Python 书籍的需求。\n问题来了，我只有一些非常基础的 Python 知识 (大体上限于能对着课本或帮助文档提供的实例进行一些微小的修改，然后运行)。而且，我目前掌握的 Python 知识基本上进局限于回归分析。对于网络爬虫、数据分析、数据可视化等知识，我几乎一无所知。\n我听说 ChatGPT, DeepSeek 等 AI 工具可以帮助我完成一些复杂的任务，比如网络爬虫、数据分析、数据可视化等。我决定试试这些工具，看看它们能否帮助我完成我的市场调研。\n于是，我写一个提示词，发给了 ChatGPT：\n以下是 ChatGPT 的回复：",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>案例：当当网 Python 类图书销售数据分析</span>"
    ]
  },
  {
    "objectID": "examples/web_dangdang_book/case_Dangdang_book.html#背景",
    "href": "examples/web_dangdang_book/case_Dangdang_book.html#背景",
    "title": "16  案例：当当网 Python 类图书销售数据分析",
    "section": "17.1 一、分析目标",
    "text": "目标读者是谁？经管类博士生，还是中小学生？\n书的内容是什么？是 Python 基础知识，还是 Python 数据分析？亦或是机器学习？\n书的形式是什么？是电子书，还是纸质书？亦或是两者都有？\n书的价格是多少？是 50 元，还是 100 元？亦或是 200 元？\n找哪家出版社？\n\n\n\n\n\n\n\n与 ChatGPT 的完整对话\n\n\n\n\n\n\n\n提示词\n\n\n\n- 目的：从当当网获取Python相关书籍的销售量信息，最终输出一份市场调研报告。\n- 语言：Python\n- 网站：[https://search.dangdang.com/](https://search.dangdang.com/)\n- 关键词：'python'\n- 排序关键词：'销量'\n- 排序方式：从高到低\n- 信息条数：前 20 条\n- 目前搜索结果页面URL：[https://search.dangdang.com/?key=python\\&act=input\\&sort\\_type=sort\\_sale\\_amt\\_desc#J\\_tab](https://search.dangdang.com/?key=python&act=input&sort_type=sort_sale_amt_desc#J_tab)\n\n- 预期输出： \n  - 书名，作者，年份，出版社，评论数，原价、折后价\n\n以上是我目前想到的。\n请帮我做一个全面的规划，包括分析目标、收集那些变量，分析流程，以及最终的报告形式、核心内容等。\n\n\n\n\n\n\n\n\n\nAI 的回复\n\n\n\n好的，以下是一份较为系统的规划方案，适用于以 Python 编写程序从当当网采集“Python”相关书籍的销售量数据，并撰写一份结构完整的市场调研报告。\n\n\n17 Python 相关图书市场调研项目规划\n\n17.1 一、分析目标\n\n核心问题：当前在当当网上热销的 Python 图书有哪些？主要出版时间集中在哪些年份？哪些出版社主导了市场？\n业务用途：\n\n为出版商、培训机构、书店提供选书参考；\n了解当前市场偏好（入门/进阶、编程/数据分析/人工智能）；\n为课程教材选择提供数据支持。\n\n\n\n\n\n17.2 二、数据采集计划\n\n17.2.1 1. 数据来源\n\n网址：https://search.dangdang.com/?key=python&act=input&sort_type=sort_sale_amt_desc#J_tab\n请求类型：GET\n页数限制：目前仅采集排序后前 20 本书\n排序方式：按销量从高到低\n\n\n\n17.2.2 2. 待采集字段（建议）\n\n\n\n字段名\n说明\n\n\n\n\n书名\n完整书名\n\n\n作者\n主作者及其他合著者\n\n\n出版社\n出版机构\n\n\n出版时间\n年份或具体日期\n\n\n原价\n划线价\n\n\n折后价\n实际售价\n\n\n评论数\n用户评论数，反映活跃度\n\n\n星级评分\n若有星级则采集\n\n\n封面图链接\n便于后续可视化展示\n\n\n图书详情页链接\n为后续扩展字段或人工校验提供入口\n\n\n\n\n\n\n\n17.3 三、分析流程\n\n17.3.1 Step 1：数据采集（Python 编程）\n\n使用库：\n\nrequests 模拟访问\nBeautifulSoup 解析 HTML\nre 或 lxml 辅助提取字段\n\n反爬措施应对：\n\n合理设置 headers（如模拟浏览器 UA）\n增加 time.sleep() 随机延迟\n如遇 JavaScript 动态加载，可考虑 selenium\n\n\n\n\n17.3.2 Step 2：数据清洗与规范化\n\n作者、出版社等字段统一格式\n出版时间转为 datetime\n价格字段转换为 float\n评论数转换为整数（去除“条评论”字样）\n\n\n\n17.3.3 Step 3：数据分析\n\n17.3.3.1 3.1 描述性统计\n\n平均价格、评论数中位数\n出版年份分布（近几年为主）\n高频作者、出版社统计\n\n\n\n17.3.3.2 3.2 分类分析\n\n入门类 vs 进阶类\n数据分析 vs Web 编程 vs 机器学习\n是否为教材或考试参考书\n\n\n\n17.3.3.3 3.3 排行榜可视化\n\n前 10 图书销量/评论/价格对比柱状图\n出版时间折线图\n价格 vs 评论数散点图\n\n\n\n\n\n\n17.4 四、报告输出格式\n\n17.4.1 1. 技术报告形式（推荐）\n\n格式：Markdown / Quarto / PDF（可由 Jupyter Notebook 或 Quarto 生成）\n结构建议：\n\n# Python 图书销售市场调研报告（当当网）\n\n## 1. 项目背景与目标\n## 2. 数据采集与字段说明\n## 3. 市场总体概况\n   - 销售 Top20 图书一览\n   - 平均价格、评分、评论数等统计\n## 4. 出版特征分析\n   - 出版社分布、出版时间分布\n   - 作者贡献度分析\n## 5. 图书类型对比分析\n   - 入门、进阶分布\n   - 专业方向分布\n## 6. 结论与建议\n   - 当前趋势总结\n   - 出版或选书建议\n\n\n17.4.2 2. 附录部分\n\n原始数据表格（20 本书）\n数据采集代码（附说明）\n图形可视化汇总图（Matplotlib / Seaborn / Plotly）\n\n\n\n\n\n17.5 五、可扩展方向（可选）\n\n同步采集京东、淘宝图书频道以对比\n分析定价策略（原价 vs 折扣价规律）\n抽取关键词构建词云图\n构建图书推荐系统雏形\n\n\n如你同意，我可以继续为你编写：\n\nPython 抓取前 20 条当当网图书数据的爬虫脚本；\n可视化分析和 Markdown 报告生成模板；\n教学型讲义或课程材料用版本。\n\n是否现在开始写代码部分？是否使用 requests + BeautifulSoup，还是你希望使用 selenium 模拟浏览器？",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>案例：当当网 Python 类图书销售数据分析</span>"
    ]
  },
  {
    "objectID": "examples/web_dangdang_book/case_Dangdang_book.html#一分析目标",
    "href": "examples/web_dangdang_book/case_Dangdang_book.html#一分析目标",
    "title": "16  案例：当当网 Python 类图书销售数据分析",
    "section": "",
    "text": "核心问题：当前在当当网上热销的 Python 图书有哪些？主要出版时间集中在哪些年份？哪些出版社主导了市场？\n业务用途：\n\n为出版商、培训机构、书店提供选书参考；\n了解当前市场偏好（入门/进阶、编程/数据分析/人工智能）；\n为课程教材选择提供数据支持。",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>案例：当当网 Python 类图书销售数据分析</span>"
    ]
  },
  {
    "objectID": "examples/web_dangdang_book/case_Dangdang_book.html#二数据采集计划",
    "href": "examples/web_dangdang_book/case_Dangdang_book.html#二数据采集计划",
    "title": "16  案例：当当网 Python 类图书销售数据分析",
    "section": "17.2 二、数据采集计划",
    "text": "17.2 二、数据采集计划\n\n17.2.1 1. 数据来源\n\n网址：https://search.dangdang.com/?key=python&act=input&sort_type=sort_sale_amt_desc#J_tab\n请求类型：GET\n页数限制：目前仅采集排序后前 20 本书\n排序方式：按销量从高到低\n\n\n\n17.2.2 2. 待采集字段（建议）\n\n\n\n字段名\n说明\n\n\n\n\n书名\n完整书名\n\n\n作者\n主作者及其他合著者\n\n\n出版社\n出版机构\n\n\n出版时间\n年份或具体日期\n\n\n原价\n划线价\n\n\n折后价\n实际售价\n\n\n评论数\n用户评论数，反映活跃度\n\n\n星级评分\n若有星级则采集\n\n\n封面图链接\n便于后续可视化展示\n\n\n图书详情页链接\n为后续扩展字段或人工校验提供入口",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>案例：当当网 Python 类图书销售数据分析</span>"
    ]
  },
  {
    "objectID": "examples/web_dangdang_book/case_Dangdang_book.html#三分析流程",
    "href": "examples/web_dangdang_book/case_Dangdang_book.html#三分析流程",
    "title": "16  案例：当当网 Python 类图书销售数据分析",
    "section": "17.3 三、分析流程",
    "text": "17.3 三、分析流程\n\n17.3.1 Step 1：数据采集（Python 编程）\n\n使用库：\n\nrequests 模拟访问\nBeautifulSoup 解析 HTML\nre 或 lxml 辅助提取字段\n\n反爬措施应对：\n\n合理设置 headers（如模拟浏览器 UA）\n增加 time.sleep() 随机延迟\n如遇 JavaScript 动态加载，可考虑 selenium\n\n\n\n\n17.3.2 Step 2：数据清洗与规范化\n\n作者、出版社等字段统一格式\n出版时间转为 datetime\n价格字段转换为 float\n评论数转换为整数（去除“条评论”字样）\n\n\n\n17.3.3 Step 3：数据分析\n\n17.3.3.1 3.1 描述性统计\n\n平均价格、评论数中位数\n出版年份分布（近几年为主）\n高频作者、出版社统计\n\n\n\n17.3.3.2 3.2 分类分析\n\n入门类 vs 进阶类\n数据分析 vs Web 编程 vs 机器学习\n是否为教材或考试参考书\n\n\n\n17.3.3.3 3.3 排行榜可视化\n\n前 10 图书销量/评论/价格对比柱状图\n出版时间折线图\n价格 vs 评论数散点图",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>案例：当当网 Python 类图书销售数据分析</span>"
    ]
  },
  {
    "objectID": "examples/web_dangdang_book/case_Dangdang_book.html#四报告输出格式",
    "href": "examples/web_dangdang_book/case_Dangdang_book.html#四报告输出格式",
    "title": "16  案例：当当网 Python 类图书销售数据分析",
    "section": "17.4 四、报告输出格式",
    "text": "17.4 四、报告输出格式\n\n17.4.1 1. 技术报告形式（推荐）\n\n格式：Markdown / Quarto / PDF（可由 Jupyter Notebook 或 Quarto 生成）\n结构建议：\n\n# Python 图书销售市场调研报告（当当网）\n\n## 1. 项目背景与目标\n## 2. 数据采集与字段说明\n## 3. 市场总体概况\n   - 销售 Top20 图书一览\n   - 平均价格、评分、评论数等统计\n## 4. 出版特征分析\n   - 出版社分布、出版时间分布\n   - 作者贡献度分析\n## 5. 图书类型对比分析\n   - 入门、进阶分布\n   - 专业方向分布\n## 6. 结论与建议\n   - 当前趋势总结\n   - 出版或选书建议\n\n\n17.4.2 2. 附录部分\n\n原始数据表格（20 本书）\n数据采集代码（附说明）\n图形可视化汇总图（Matplotlib / Seaborn / Plotly）",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>案例：当当网 Python 类图书销售数据分析</span>"
    ]
  },
  {
    "objectID": "examples/web_dangdang_book/case_Dangdang_book.html#五可扩展方向可选",
    "href": "examples/web_dangdang_book/case_Dangdang_book.html#五可扩展方向可选",
    "title": "16  案例：当当网 Python 类图书销售数据分析",
    "section": "17.5 五、可扩展方向（可选）",
    "text": "17.5 五、可扩展方向（可选）\n\n同步采集京东、淘宝图书频道以对比\n分析定价策略（原价 vs 折扣价规律）\n抽取关键词构建词云图\n构建图书推荐系统雏形\n\n\n如你同意，我可以继续为你编写：\n\nPython 抓取前 20 条当当网图书数据的爬虫脚本；\n可视化分析和 Markdown 报告生成模板；\n教学型讲义或课程材料用版本。\n\n是否现在开始写代码部分？是否使用 requests + BeautifulSoup，还是你希望使用 selenium 模拟浏览器？",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>案例：当当网 Python 类图书销售数据分析</span>"
    ]
  },
  {
    "objectID": "examples/web_dangdang_book/case_Dangdang_book.html#爬取数据",
    "href": "examples/web_dangdang_book/case_Dangdang_book.html#爬取数据",
    "title": "16  案例：当当网 Python 类图书销售数据分析",
    "section": "17.6 爬取数据",
    "text": "17.6 爬取数据\n我们让 ChatGPT 执行第一步：\nStep 1：使用 Python 抓取当当网「Python」相关图书销量前 20 名信息 的完整采集代码。\n\n17.6.1 技术选择说明\n\n目标网址：https://search.dangdang.com/?key=python&sort_type=sort_sale_amt_desc\n方案：采用 requests + BeautifulSoup 进行静态页面解析（当当网图书搜索结果页大部分信息在 HTML 中，JS 渲染较少）\n爬取字段：\n\n书名\n作者\n出版社\n出版时间\n原价\n折后价\n评论数\n商品详情页链接\n\n\n\n\n17.6.2 Python 代码\n\n# 设置项目路径\n#   [./data]: 存储数据文件\n#   [./output]: 存储输出文件\n\n\n#---------------- 项目路径，酌情修改 -------------------\n#\npath = r'D:\\Github\\dslian\\cases\\web_dangdang_book' \n#\n#------------------------------------------------------\n\nimport os\nimport sys\npath_data = os.path.join(path, 'data')\npath_output = os.path.join(path, 'output')\n\nos.chdir(path)\nos.getcwd()\n\n'd:\\\\Github\\\\dslian\\\\cases\\\\web_dangdang_book'\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\nimport os\n\n# 设置目标搜索 URL（销量降序排序）\nurl = \"https://search.dangdang.com/?key=python&sort_type=sort_sale_amt_desc\"\n\n# 设置请求头，模拟浏览器访问\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \\\n                   (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n}\n\n# 发送请求并获取页面内容\nresponse = requests.get(url, headers=headers)\nresponse.encoding = \"gb2312\"\n\n# 使用 BeautifulSoup 解析 HTML 内容\nsoup = BeautifulSoup(response.text, \"html.parser\")\n\n# 所有图书的信息在 ul.bigimg li 中\nbook_list = soup.select(\"ul.bigimg &gt; li\")\n\n# 用于保存每本书的信息\nbooks_data = []\n\n# 遍历每本书的条目，提取信息\nfor item in book_list:\n    try:\n        title_tag = item.select_one(\"a[name='itemlist-title']\")\n        title = title_tag.get(\"title\").strip() if title_tag else \"\"\n        link = title_tag.get(\"href\") if title_tag else \"\"\n\n        author_tag = item.select_one(\"p.name a\")\n        author = author_tag.text.strip() if author_tag else \"\"\n\n        pub_info_tag = item.select_one(\"p.search_book_author\")\n        pub_info = pub_info_tag.text.strip() if pub_info_tag else \"\"\n        pub_info_parts = pub_info.split(\"\\xa0\\xa0\")\n\n        publisher = pub_info_parts[1].strip() if len(pub_info_parts) &gt; 1 else \"\"\n        pubdate = pub_info_parts[2].strip() if len(pub_info_parts) &gt; 2 else \"\"\n\n        comment_tag = item.select_one(\"p.search_star_line a\")\n        comment = comment_tag.text.strip() if comment_tag else \"\"\n\n        price_tag = item.select_one(\"p.price\")\n        orig_price_tag = price_tag.select_one(\"span.search_pre_price\") if price_tag else None\n        disc_price_tag = price_tag.select_one(\"span.search_now_price\") if price_tag else None\n        orig_price = orig_price_tag.text.strip(\"¥\") if orig_price_tag else \"\"\n        disc_price = disc_price_tag.text.strip(\"¥\") if disc_price_tag else \"\"\n\n        books_data.append({\n            \"书名\": title,\n            \"作者\": author,\n            \"出版社\": publisher,\n            \"出版时间\": pubdate,\n            \"原价\": orig_price,\n            \"折后价\": disc_price,\n            \"评论数\": comment,\n            \"详情页链接\": link\n        })\n\n        time.sleep(0.5)  # 加入短暂停顿防止访问过快\n\n    except Exception as e:\n        print(f\"解析失败：{e}\")\n        continue\n\n# 转换为 DataFrame\ndf_books = pd.DataFrame(books_data)\n\n# 确保保存路径存在\noutput_path = \"data\"\nos.makedirs(output_path, exist_ok=True)\noutput_file = os.path.join(output_path, \"dangdang_python_books_top20.csv\")\n\n# 保存为 CSV 文件\ndf_books.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n\n# 打印成功信息\nprint(f\"已成功下载 {len(df_books)} 条数据，\n        点击 [file://{os.path.abspath(output_file)}] \n        查看 dangdang_python_books_top20.csv 文件\")\n\n已成功下载 60 条数据，点击 [file://d:\\Github\\dslian\\cases\\web_dangdang_book\\data\\dangdang_python_books_top20.csv] 查看 dangdang_python_books_top20.csv 文件\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\nimport os\n\nurl = \"https://search.dangdang.com/?key=python&sort_type=sort_sale_amt_desc\"\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \\\n                   (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n}\n\nresponse = requests.get(url, headers=headers)\nresponse.encoding = \"gb2312\"\nsoup = BeautifulSoup(response.text, \"html.parser\")\nbook_list = soup.select(\"ul.bigimg &gt; li\")\n\nbooks_data = []\n\nfor item in book_list:\n    try:\n        # 1. 主标题和简介\n        title_tag = item.select_one(\"a[name='itemlist-title']\")\n        full_title = title_tag.get(\"title\").strip() if title_tag else \"\"\n        if \"：\" in full_title:\n            title, brief = full_title.split(\"：\", 1)\n        elif \"（\" in full_title:\n            title, brief = full_title.split(\"（\", 1)\n            brief = \"（\" + brief\n        else:\n            title, brief = full_title, \"\"\n\n        # 2. 详情页链接\n        link = title_tag.get(\"href\") if title_tag else \"\"\n\n        # 3. 作者 / 出版社 / 出版时间\n        pub_info_tag = item.select_one(\"p.search_book_author\")\n        pub_info_parts = pub_info_tag.text.strip().split(\"\\xa0\\xa0\") if pub_info_tag else []\n        author = pub_info_parts[0].replace(\"作者\", \"\").strip() if len(pub_info_parts) &gt; 0 else \"\"\n        publisher = pub_info_parts[1].strip() if len(pub_info_parts) &gt; 1 else \"\"\n        pubdate = pub_info_parts[2].strip() if len(pub_info_parts) &gt; 2 else \"\"\n\n        # 4. 评论数\n        comment_tag = item.select_one(\"p.search_star_line a\")\n        comment = comment_tag.text.strip() if comment_tag else \"\"\n\n        # 5. 价格\n        price_tag = item.select_one(\"p.price\")\n        orig_price_tag = price_tag.select_one(\"span.search_pre_price\") if price_tag else None\n        disc_price_tag = price_tag.select_one(\"span.search_now_price\") if price_tag else None\n        orig_price = orig_price_tag.text.strip(\"¥\") if orig_price_tag else \"\"\n        disc_price = disc_price_tag.text.strip(\"¥\") if disc_price_tag else \"\"\n\n        books_data.append({\n            \"书名\": title,\n            \"简介\": brief,\n            \"作者\": author,\n            \"出版社\": publisher,\n            \"出版时间\": pubdate,\n            \"原价\": orig_price,\n            \"折后价\": disc_price,\n            \"评论数\": comment,\n            \"详情页链接\": link\n        })\n\n        time.sleep(0.5)  # 加入短暂停顿防止访问过快\n\n    except Exception as e:\n        print(f\"解析失败：{e}\")\n        continue\n\ndf_books = pd.DataFrame(books_data)\n\noutput_path = \"data\"\nos.makedirs(output_path, exist_ok=True)\noutput_file = os.path.join(output_path, \"dangdang_python_books_cleaned.csv\")\ndf_books.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n\nprint(f\"已成功下载 {len(df_books)} 条数据，点击 [file://{os.path.abspath(output_file)}] 查看 dangdang_python_books_cleaned.csv 文件\")\n\n已成功下载 60 条数据，点击 [file://d:\\Github\\dslian\\cases\\web_dangdang_book\\data\\dangdang_python_books_cleaned.csv] 查看 dangdang_python_books_cleaned.csv 文件\n\n\n\n# 读入本地数据\ndf_books = pd.read_csv(r\"data\\dangdang_python_books_cleaned.csv\", encoding=\"utf-8-sig\")\n\n# 展示前几行结果\nprint(df_books.columns)\nprint(df_books.head(3))\nprint(df_books.info())\n\nIndex(['书名', '简介', '作者', '出版社', '出版时间', '原价', '折后价', '评论数', '详情页链接'], dtype='object')\n                                                  书名  \\\n0                                     Python编程三剑客第3版   \n1                                      小学生Python创意编程   \n2  Python编程从入门到实践 第3版 Python编程入门经典，自学利器，数据分析、网络爬虫...   \n\n                                                  简介  \\\n0          Python编程从入门到实践第3版+快速上手第2版+极客项目编程（当当套装共3册）   \n1  （视频教学版） 本书按照入门开发者的思维方式编写，非常适合孩子学习Python编程的基础知识...   \n2                                                NaN   \n\n                                                  作者  出版社  出版时间     原价   折后价  \\\n0                                            加入购物车收藏  NaN   NaN  267.8  21.6   \n1              刘凤飞 /2024-01-01  /清华大学出版社加入购物车购买电子书收藏  NaN   NaN   89.0  45.2   \n2  [美]埃里克・马瑟斯（Eric Matthes） /2023-05-01  /人民邮电出版社...  NaN   NaN  109.8  69.8   \n\n         评论数                                    详情页链接  \n0  124999条评论  //product.dangdang.com/12100010244.html  \n1    7157条评论     //product.dangdang.com/29666296.html  \n2   19970条评论     //product.dangdang.com/29564789.html  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 60 entries, 0 to 59\nData columns (total 9 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   书名      60 non-null     object \n 1   简介      29 non-null     object \n 2   作者      60 non-null     object \n 3   出版社     0 non-null      float64\n 4   出版时间    0 non-null      float64\n 5   原价      57 non-null     float64\n 6   折后价     60 non-null     float64\n 7   评论数     60 non-null     object \n 8   详情页链接   60 non-null     object \ndtypes: float64(4), object(5)\nmemory usage: 4.3+ KB\nNone",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>案例：当当网 Python 类图书销售数据分析</span>"
    ]
  },
  {
    "objectID": "examples/web_dangdang_book/case_Dangdang_book.html#后续",
    "href": "examples/web_dangdang_book/case_Dangdang_book.html#后续",
    "title": "16  案例：当当网 Python 类图书销售数据分析",
    "section": "17.7 后续",
    "text": "17.7 后续\n我花了 30 分钟做到目前的样子，基本上搞懂了使用 Copilot 编写 Python 代码实现爬虫的流程。\n刚好当时在教授「数据分析与 Python 应用」的课程，我索性把这个案例作为课堂作业，要求学生们完成。\n\n作业要求：T1-当当网Python类图书销售分析\n学生提交的 Jupyter Notebook：\n\n当当网-G7-01：爬虫和数据清洗\n当当网-G7-02：数据分析",
    "crumbs": [
      "**数据分析**",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>案例：当当网 Python 类图书销售数据分析</span>"
    ]
  },
  {
    "objectID": "examples/regenernate_figure_with_AI.html",
    "href": "examples/regenernate_figure_with_AI.html",
    "title": "17  Ansome Python + AI",
    "section": "",
    "text": "17.1 画一幅图\n有了 AI 的协助，我们可以在只掌握少量编程知识的情况下，用 Python 实现各种复杂的功能。本章例举一些实例，旨在改变大家对「编程」的认知。\n最终你会发现，Python 只是我们完成某些分析和研究任务的工具而已。AI 的作用是让我们使用「自然语言」来编程。此时的编程主要包括如下几个要点：\n我在 R for Data Science 书中看到了一幅不错的图形，想要绘制出来。这幅图不算复杂：\n于是，我把图片发给了 AI 工具，配上提示词：\n我把相同的问题同时发给了 ChatGPT，DeepSeek 和 豆包。最终，豆包给出的图形效果最好：\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.table import Table\nfrom matplotlib.patches import FancyArrowPatch\n\n# 原始数据\ndata1 = {\n    'id': ['A', 'B'],\n    'x_1': [1, 5],\n    'x_2': [2, 6],\n    'y_1': [3, 7],\n    'y_2': [4, 8]\n}\ndf1 = pd.DataFrame(data1)\n\n# 转换后数据\ndata2 = {\n    'id': ['A', 'A', 'B', 'B'],\n    'x': [1, 2, 5, 6],\n    'y': [3, 4, 7, 8],\n    'num': [1, 2, 1, 2]\n}\ndf2 = pd.DataFrame(data2)\n\n# 定义颜色映射\ncolor_mapping = {\n    'x_1': 'lightblue',\n    'x_2': 'plum',\n    'y_1': 'lightgreen',\n    'y_2': 'lightgreen',\n    'x': 'lightblue',\n    'y': 'lightgreen',\n    'num': ['orange', 'violet']\n}\n\n# 创建图形和子图\nfig, ax = plt.subplots(figsize=(10, 4))\nax.axis('off')\n\n# 绘制原始表格\n# 绘制原始表格\ntable1 = ax.table(\n    cellText=df1.values,\n    colLabels=df1.columns,\n    cellLoc='center',\n    loc='left',\n    bbox=[0, 0.3, 0.4, 0.6]\n)\nfor (row, col), cell in table1.get_celld().items():\n    if row == 0:\n        cell.set_text_props(weight='bold')\n    if row &gt; 0:\n        colname = df1.columns[col]\n        cell.set_facecolor(color_mapping.get(colname, 'white'))\n    else:\n        colname = df1.columns[col]\n        cell.set_facecolor(color_mapping.get(colname, 'white'))\n\n# 绘制箭头\n#ax.arrow(0.45, 0.6, 0.1, 0, head_width=0.03, head_length=0.05, fc='k', ec='k')\n\n# 绘制转换后表格\ntable2 = ax.table(\n    cellText=df2.values,\n    colLabels=df2.columns,\n    cellLoc='center',\n    loc='right',\n    bbox=[0.55, 0.3, 0.4, 0.6]\n)\nfor (row, col), cell in table2.get_celld().items():\n    if row == 0:\n        cell.set_text_props(weight='bold')\n    if row &gt; 0:\n        colname = df2.columns[col]\n        if colname == 'num':\n            cell.set_facecolor(color_mapping['num'][(row-1) % 2])\n        else:\n            cell.set_facecolor(color_mapping.get(colname, 'white'))\n    else:\n        colname = df2.columns[col]\n        if colname == 'num':\n            cell.set_facecolor(color_mapping['num'][0])\n        else:\n            cell.set_facecolor(color_mapping.get(colname, 'white'))\n            # 添加 FancyArrowPatch 箭头以美化转换效果\n\n            fancy_arrow = FancyArrowPatch(\n                (0.48, 0.6), (0.52, 0.6),\n                transform=fig.transFigure,\n                connectionstyle=\"arc3,rad=0\",\n                arrowstyle='-|&gt;',\n                linewidth=2,\n                color='black'\n            )\n            fig.patches.append(fancy_arrow)\nplt.show()",
    "crumbs": [
      "**其它应用**",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Ansome Python + AI</span>"
    ]
  },
  {
    "objectID": "examples/regenernate_figure_with_AI.html#画一幅图",
    "href": "examples/regenernate_figure_with_AI.html#画一幅图",
    "title": "17  Ansome Python + AI",
    "section": "",
    "text": "提示词\n\n\n\n写一段 Python 代码，制作类似的图形。\n请注意左右两侧图形的颜色块要一一对应，这是这幅图的关键。\n只保留中文注释，图中的文字用英文。\n不要显示 warning information",
    "crumbs": [
      "**其它应用**",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Ansome Python + AI</span>"
    ]
  },
  {
    "objectID": "examples/split_mdfile/split-topic-files.html",
    "href": "examples/split_mdfile/split-topic-files.html",
    "title": "18  将一个 md 文档切割成多个 md 文档",
    "section": "",
    "text": "18.1 任务描述\n在过去的三年中，除了重要节假日以外，连享会 基本能确保每天发布一篇新推文。这些推文的选题基本上都是我确定的。\n我每周会花一天的时间，阅读大量的文献和各种网站，收集和整理选题。这些选题会被我统一记录在一个 Markdown 文档中，形如 Topics-2025-07.md。经过修改后，这些选题会发布在 备选推文主题 页面，由助教们认领。\n我每周大概公布 10 个选题。其中最让我头疼的工作就是将这个 Markdown 文档切割成多个小文档 (由 图 1 转成 图 2)：我需要 新建一个 .md 文档 → 重命名 → 贴入内容。这个工作没什么难度，但很耗神，也容易出错。",
    "crumbs": [
      "**其它应用**",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>将一个 md 文档切割成多个 md 文档</span>"
    ]
  },
  {
    "objectID": "examples/split_mdfile/split-topic-files.html#任务描述",
    "href": "examples/split_mdfile/split-topic-files.html#任务描述",
    "title": "18  将一个 md 文档切割成多个 md 文档",
    "section": "",
    "text": "图 1：Topics-2025-07.md 文档中的选题笔记\n\n\n\n图 2：github 仓库中选题发布页面",
    "crumbs": [
      "**其它应用**",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>将一个 md 文档切割成多个 md 文档</span>"
    ]
  },
  {
    "objectID": "examples/split_mdfile/split-topic-files.html#处理思路",
    "href": "examples/split_mdfile/split-topic-files.html#处理思路",
    "title": "18  将一个 md 文档切割成多个 md 文档",
    "section": "18.2 处理思路",
    "text": "18.2 处理思路\n我此前尝试过用 Stata 写代码来处理这个任务，但效果不佳，可能是因为我对文档读写不太熟悉。\n前段时间我尝试用 AI 编写 Python 代码来完成这个任务。AI 给出的代码基本上没有 bugs。我运行代码后，便有了新的需求，比如：\n\n自动覆盖旧文件；\n可以指定从 .md 文档中选择一部分文本进行切割；\n指定存放输出结果的文件夹\n确定屏幕打印结果\n\n需要声明的是：在此过程中，我只能大概读懂 AI 编写的 Python 代码，我基本上无法自行编写 Python 代码。我的主要工作就是优化提示词 (本质上是弄清楚我自己到底想要什么样的结果)。因此，我从不修改 AI 编写的 Python 代码，我只修改提示词。\n下面是我经过 5-6 轮修改后形成的提示词。这些提示词的编排格式是我自己设计的，虽然花了一些时间，但我发现这个习惯在应对较为复杂的任务时至关重要。否则，如果你的提示词是大段的文字，逻辑不清晰，AI 帮你编写的代码也会很容易出错。\n\n\n\n\n\n\n提示词\n\n\n\n### input-output\n\n- Folder：当前路径\n- filename-input：包含关键词 'Topics-' 且后缀为 '.md' 的文件\n- range-转换范围: \n  - begin: # B860\n  -   end: # B851\n- Folder-output：'.\\topics'\n\n### 任务：\n\n- Input: 读取 {filename-input} 文件。\n- Actions：\n  - 在 {filename-input} 文档中，以 `^# B\\d{3,.}：title` 开头的行为分割线，将该行及以下到下一个 `^# B\\d{3,.}：title` 之间的文本写入一个新的 **.md** 文件中\n  - 仅读取 {begin} 到 {end} 范围内的选题\n- Output:\n  - 存入：{Folder-output} 文件夹\n    - 预先清空该文件夹 \n  - 文件名为 `B\\d{3,.}：title` \n    - 首字母为 'B'\n    - 文件后缀为 '.md'，而不是 '.md.md'\n  - 屏幕打印：\n    - 文件总数\n    - 文件名称列表\n\n\n下面是 AI 编写的 Python 代码：\n\nimport os\nimport re\nimport shutil\n\n# ================= 用户参数 =================\ninput_folder = r\".\"\noutput_folder = r\".\\topics\"\nbegin_tag = \"# B860\"\nend_tag = \"# B851\"\n\n# ================= 辅助函数 =================\ndef get_input_filename(folder):\n    for fname in os.listdir(folder):\n        if \"Topics-\" in fname and fname.endswith('.md'):\n            return os.path.join(folder, fname)\n    raise FileNotFoundError(\"未找到包含 'Topics-' 且后缀为 '.md' 的文件\")\n\ndef read_file(filepath):\n    with open(filepath, encoding=\"utf-8\") as f:\n        return f.read()\n\ndef split_by_title(text):\n    # 正则匹配 '# Bxxx：title'\n    blocks = list(re.finditer(r'^# B\\d{3,}：.*$', text, flags=re.MULTILINE))\n    result = []\n    for idx, match in enumerate(blocks):\n        start = match.start()\n        end = blocks[idx + 1].start() if idx + 1 &lt; len(blocks) else len(text)\n        title_line = text[match.start():match.end()]\n        block_text = text[start:end].strip()\n        result.append((title_line, block_text))\n    return result\n\ndef find_range(blocks, begin_tag, end_tag):\n    begin_idx, end_idx = None, None\n    for i, (title, _) in enumerate(blocks):\n        if title.strip().startswith(begin_tag):\n            begin_idx = i\n        if title.strip().startswith(end_tag):\n            end_idx = i\n    if begin_idx is not None and end_idx is not None and begin_idx &lt;= end_idx:\n        return blocks[begin_idx:end_idx+1]\n    else:\n        raise ValueError(\"未能定位 begin 或 end 位置，或顺序错误\")\n\ndef clean_folder(folder):\n    if os.path.exists(folder):\n        shutil.rmtree(folder)\n    os.makedirs(folder)\n\ndef save_blocks_to_folder(blocks, folder):\n    filenames = []\n    for title, content in blocks:\n        # 文件名采用 '# Bxxx：title' 的形式\n        file_base = re.match(r'^# (B\\d{3,}：.*)$', title.strip())\n        if not file_base:\n            continue\n        filename = file_base.group(1) + \".md\"\n        # 替换非法字符\n        filename = re.sub(r'[\\\\/:*?\"&lt;&gt;|]', '_', filename)\n        filepath = os.path.join(folder, filename)\n        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(content.strip() + \"\\n\")\n        filenames.append(filename)\n    return filenames\n\n# ================= 主程序 =================\nif __name__ == \"__main__\":\n    # 1. 获取输入文件\n    input_path = get_input_filename(input_folder)\n    # 2. 读取内容\n    text = read_file(input_path)\n    # 3. 按分割线分割\n    blocks = split_by_title(text)\n    # 4. 提取 begin 到 end 范围\n    blocks_in_range = find_range(blocks, begin_tag, end_tag)\n    # 5. 预清空输出文件夹\n    clean_folder(output_folder)\n    # 6. 写入输出\n    filelist = save_blocks_to_folder(blocks_in_range, output_folder)\n    # 7. 打印信息\n    print(f\"文件总数：{len(filelist)}\")\n    print(\"文件名称列表：\")\n    for fn in filelist:\n        print(fn)\n\n文件总数：9\n文件名称列表：\nB860：Stata 可视化：heatplot-热力图.md\nB859：cdist-如何估计反事实的分布特征？.md\nB858：数据量大时，啥啥都显著.md\nB857：介绍 Local-DID 模型.md\nB856：Stata 可视化：robbox-强大的的箱型图命令 - 快速呈现离群值.md\nB855：介绍 I4R 机构和网站.md\nB854：知乎热议：为什么国内博士毕业的 paper 普遍比国外博士多？.md\nB852：mmqreg_ quantile regressions via Method of Moments.md\nB851：翻译：异方差稳健性标准误：实操建议.md\n\n\n有了这段代码，我每次只需 1 秒即可完成任务。\n你可以在 B860：Stata 可视化：heatplot-热力图 查看选题 B860 的具体要求。再过 1 月左右，你就可以在 lianxh.cn 上看到这篇推文了。",
    "crumbs": [
      "**其它应用**",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>将一个 md 文档切割成多个 md 文档</span>"
    ]
  },
  {
    "objectID": "body/references.html",
    "href": "body/references.html",
    "title": "19  参考文献",
    "section": "",
    "text": "19.1 Papers",
    "crumbs": [
      "**参考文献和附录**",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>参考文献</span>"
    ]
  },
  {
    "objectID": "body/references.html#papers",
    "href": "body/references.html#papers",
    "title": "19  参考文献",
    "section": "",
    "text": "Bahoo, S., Goodell, J. W., Rhattat, R., & Shahid, S. (2025). Artificial Intelligence in Economics Research: What Have We Learned? What Do We Need to Learn? Journal of Economic Surveys. Portico. Link, PDF, Google.\nDong, M. M., Stratopoulos, T. C., & Wang, V. X. (2024). A scoping review of ChatGPT research in accounting and finance. International Journal of Accounting Information Systems, 55, 100715. Link, -PDF-, PDF2, Google.\nFang, H., Li, M., & Lu, G. (2025). Decoding China’s Industrial Policies. National Bureau of Economic Research. Link, PDF, Google.\n\nAppendix B Major Components of LLM Prompts: This section presents the summary of the definitions we provide to the LLMs for the major classification tasks.\n\nGiesecke, O. (2024). AI at the Frontier of Economic Research. SSRN Electronic Journal. Link, -PDF-, Google.\nHan, S. (2024). Mining Causality: AI-Assisted Search for Instrumental Variables (Version 3). arXiv. Link (rep), PDF, Google.\nKorinek, A. (2023). Generative AI for Economic Research: Use Cases and Implications for Economists. Journal of Economic Literature, 61(4), 1281–1317. Link (rep), PDF, -PDF2-, Appendix, Google. 作者主页-Tips-AI\n\n更新 1：2024 版全文更新\n更新 2：提供了很多 Prompts\n\nRenkema, M., & Tursunbayeva, A. (2024). The future of work of academics in the age of Artificial Intelligence: State-of-the-art and a research roadmap. Futures, 163, 103453. Link, PDF, Google.\nSahoo, S., & Dutta, K. (2024). Boardwalk Empire: How Generative AI is Revolutionizing Economic Paradigms (Version 2). arXiv. Link (rep), PDF, Google.\n\n介绍了各种使用 GenAI 处理文本、语音和影像资料的软件和平台。\n\nQin X, Huang M, Ding J. AITurk: Using ChatGPT for Social Science Research[J]. Available at SSRN 4922861, 2024. -PDF-. 中文推文, ATTurk\nWang, J. J., & Wang, V. X. (2024). Leveraging Large Language Models to Democratize Access to Costly Datasets for Academic Research (Version 2). arXiv. Link (rep), PDF, Google. Page 32-33 提供了作者用于收集数据的提示词清单。",
    "crumbs": [
      "**参考文献和附录**",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>参考文献</span>"
    ]
  },
  {
    "objectID": "body/references.html#相关推文",
    "href": "body/references.html#相关推文",
    "title": "19  参考文献",
    "section": "19.2 相关推文",
    "text": "19.2 相关推文\n\nNote：产生如下推文列表的 Stata 命令为：\n  lianxh AI ChatGPT 大语言 生成式, nocat md2 ex(知乎 xtnumfac AIC ebalance 高维线性 绘图模板 Sai2)\n安装最新版 lianxh 命令：\n  ssc install lianxh, replace\n\nAI 工具、平台与插件\n\n陈庭伟, 2025, 2025年学术研究中的15大最佳AI工具\n连小白, 2025, AI工具！AI工具分类大集合\n连小白, 2025, AI助手系列：napkin.ai-文字转换为图片和表格\n连小白, 2025, AI助手系列：借助AI工具复现高质量图形\n连小白, 2025, AI工具系列：英文学术论文语法检测与纠正\n陈云菲, 2025, PDF神器MinerU：结构重构、图表提取、LaTeX公式识别全搞定！\n王烨文, 2025, Marker：高效 PDF 文档解析与结构化提取工具\n连玉君, 2024, 从基础到 AI 助手：Python 用户最爱的 VScode 插件清单\n连玉君, 2024, VScode：实用 Markdown 插件推荐\n\n提示词与 Prompt 工程\n\n罗丹, 2025, 提示词！提示词！数据清洗、数据分析、可视化一网打尽\n连小白, 2025, 提示词来了！如何让 AI 翻译看起来像你写的\n董思源, 2025, 提示词！用 DeepSeek 快速生成更优代码\n赵文琦, 2025, LLM系列：ChatGPT提示词精选与实操指南\n袁明生, 2025, 给经济学研究者的Prompt Engineering指南：如何用AI高效构建研究假设？\n\n研究设计与因果分析类\n\n张琪琳, 2025, CClaRA-扒了四万篇论文：如何论证因果关系？\n吴欣洋, 2025, AI自动生成研究假设，靠谱吗？流程与挑战\n连玉君, 2025, 如何借助 AI 工具来伴读一篇理论类的论文？\n连小白, 2025, 自动化因果推断助手：Causal-Copilot 简介\n连享会, 2025, 生成式人工智能专题：社会科学研究的革新工具\n王烨文, 2025, 新书免费读：CausalMLBook-因果机器学习\n邱一崎, 2025, AITurk 平台论文复现\n\n编程助手：写代码\n\n于凡, 2024, AI可以编写Stata代码吗？\n余坚, 2023, Stata：ChatGPT你能帮我干点啥？\n罗银燕, 2023, 如何在 R 中安装并使用 chatgpt 包？\n连玉君, 2024, 借助ChatGPT4o学习排序算法：AI代码助手好酸爽！\n连享会, 2024, AI编程助手大盘点：不止ChatGPT和Copilot\n\n语言模型基础与通识介绍\n\n张弛, 2025, 大语言模型到底是个啥？通俗易懂教程\n张弛, 2025, 找不到IV？如何借助大语言模型寻找工具变量\n王烨文, 2025, LLM Agent：大语言模型的智能体图解\n吴小齐, 2024, 强大的Kimi：中国版ChatGPT平替\n颜国强, 2024, ChatGPT争议：学术研究的加速器or信任危机的催化剂？\n连玉君, 2023, 连玉君：我与ChatGPT聊了一个月\n\n其它\n\n连小白, 2025, 完美替代 MathPix：我用 ChatGPT、豆包、Kimi 识别数学公式\n连玉君, 2025, 老连买电脑：ChatGPT，DeepSeek，豆包来帮忙\n连小白, 2025, No Chinglish：学术写作中的中式英语陷阱",
    "crumbs": [
      "**参考文献和附录**",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>参考文献</span>"
    ]
  }
]